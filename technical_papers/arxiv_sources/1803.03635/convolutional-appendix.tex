\section{Hyperparameter Exploration for Convolutional Networks}
\label{app:conv}

This Appendix accompanies Sections \ref{sec:conv} of the main paper. It explores the space of optimization
algorithms and hyperparameters for the Conv-2, Conv-4, and Conv-6
architectures evaluated in Section \ref{sec:conv} with the same two purposes as Appendix \ref{app:mnist}: explaining the hyperparameters used
in the main body of the paper and evaluating the lottery ticket experiment on other choices of hyperparameters. 

\subsection{Experimental Methodology}

The Conv-2, Conv-4, and Conv-6 architectures are variants of the VGG~\citep{vgg} network architecture scaled down for the CIFAR10 \citep{cifar10} dataset.
Like VGG, the networks consist of a series of modules. Each module has two layers of 3x3 convolutional filters followed by a maxpool layer with stride 2.
After all of the modules are two fully-connected layers of size 256 followed by an output layer of size 10; in VGG, the fully-connected layers are of size 4096 and
the output layer is of size 1000. Like VGG, the first module has 64 convolutions in each layer, the second has 128, the third has 256, etc. The Conv-2, Conv-4, and Conv-6
architectures have 1, 2, and 3 modules, respectively.

The CIFAR10 dataset consists of 50,000 32x32 color (three-channel) training examples and 10,000 test examples. We randomly sampled a 5,000-example validation set from
the training set and used the remaining 45,000 training examples as our training set for the rest of the paper. The hyperparameter
 selection experiments throughout this Appendix are evaluated on the validation set, and the examples in the
main body of this paper (which make use of these hyperparameters) are evaluated on test set. The training set is presented to the network
in mini-batches of 60 examples; at each epoch, the entire training set is shuffled.

The Conv-2, Conv-4, and Conv-6 networks are initialized with Gaussian Glorot initialization \citep{xavier} and are trained for the number of iterations
specified in Figure \ref{fig:convnets}. The number of training iterations was selected such that heavily-pruned networks could still train in the
time provided. On dropout experiments, the number of training iterations is tripled to provide enough time for the dropout-regularized networks
to train. We optimize these networks with Adam, and select the learning rate for each network in this Appendix.

As with the MNIST experiments, validation and test performance is only considered retroactively and has no effect on the progression of the
lottery ticket experiments. We measure validation and test loss and accuracy every 100 training iterations.

Each line in each graph of this section represents the average of three separate experiments, with error bars indicating the minimum and maximum
value that any experiment took on at that point. (Experiments in the main body of the paper are conducted five times.)

We allow convolutional layers and fully-connected layers to be pruned at different rates; we select those rates for each network in this Appendix. The output
layer is pruned at half of the rate of the fully-connected layers for the reasons described in Appendix \ref{app:mnist}.

\subsection{Learning Rate}
\label{app:conv-learning-rate}

In this Subsection, we perform the lottery ticket experiment on the the Conv-2, Conv-4, and Conv-6 architectures as optimized with
Adam at various learning rates.



\begin{figure}
\centering
\includegraphics[width=.8\textwidth]{graphs/cifar10/conv/adam_rate_sweep1/legend}
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/adam_rate_sweep1/iteration}%
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/adam_rate_sweep1/accuracy}
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/adam_rate_sweep2/iteration}%
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/adam_rate_sweep2/accuracy}
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/adam_rate_sweep3/iteration}%
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/adam_rate_sweep3/accuracy}
\caption{The early-stopping iteration and validation accuracy at that iteration  of the iterative lottery ticket experiment on the Conv-2 (top), Conv-4 (middle),
and Conv-6 (bottom) architectures trained using
the Adam optimizer at various learning rates. Each line represents a different learning rate.}
\label{fig:appendix-conv-adam}
\end{figure}

Here, we select the learning rate that we use for Adam in the main body of the paper. 
Our criteria for selecting the learning rate are the same as in Appendix \ref{app:mnist}: minimizing training iterations and maximizing accuracy at
early-stopping, finding winning tickets containing as few parameters as possible, and remaining conservative enough to apply to a range of other experiments.

Figure \ref{fig:appendix-conv-adam} shows the results of performing the iterative lottery ticket experiment on the Conv-2 (top), Conv-4 (middle), and Conv-6 (bottom)
architectures.
Since we have not yet selected the pruning rates for each network, we temporarily pruned fully-connected layers at 20\% per iteration, convolutional
layers at 10\% per iteration, and the output layer at 10\% per iteration; we explore this part of the hyperparameter space in a later subsection.

For Conv-2, we select a learning rate of 0.0002, which has the highest initial validation accuracy, maintains both high validation accuracy and low early-stopping times for
the among the longest, and reaches the fastest early-stopping times. This learning rate also leads to a 3.3 percentage point improvement
in validation accuracy when the network is pruned to 3\% of its original size. Other learning rates, such
0.0004, have lower initial validation accuracy (65.2\% vs 67.6\%) but eventually
reach higher absolute levels of validation accuracy (71.7\%, a 6.5 percentage point increase, vs. 70.9\%, a 3.3 percentage point increase). 
However, learning rate 0.0002
shows the highest proportional decrease in early-stopping times: 4.8x (when pruned to 8.8\% of the original network size).

For Conv-4, we select learning rate 0.0003, which has among the highest initial validation accuracy, maintains high validation accuracy and fast early-stopping
times when pruned
by among the most, and balances improvements in validation accuracy (3.7 percentage point improvement to 78.6\% when 5.4\% of weights remain)
and improvements in early-stopping time (4.27x when 11.1\% of weights remain). Other learning rates reach higher validation accuracy
(0.0004---3.6 percentage point improvement to 79.1\% accuracy when 5.4\% of weights remain) or show better improvements in early-stopping
times (0.0002---5.1x faster when
9.2\% of weights remain) but not both. 

For Conv-6, we also select learning rate 0.0003 for similar reasons to those provided for Conv-4. Validation accuracy improves by 2.4 percentage points
to 81.5\% when 9.31\%
of weights remain and early-stopping times improve by 2.61x when pruned to 11.9\%. Learning rate 0.0004 reaches high final validation accuracy (81.9\%, an increase of 2.7
percentage points, when 15.2\% of weights remain) but with smaller improvements in early-stopping times,
and learning rate 0.0002 shows greater improvements in early-stopping times (6.26x when 19.7\% of weights
remain) but reaches lower overall validation accuracy.

We note that, across nearly all combinations of learning rates, the lottery ticket pattern---where early-stopping times were maintain or decreased and validation accuracy 
was maintained or increased during the course of the lottery ticket experiment---continues to hold. This pattern fails to hold at the very highest
learning rates: early-stopping times decreased only briefly (in the case of Conv-2 or Conv-4) or not at all (in the case of Conv-6), and accuracy increased only
briefly (in the case of all three networks). This pattern is similar to that which we observe in Section \ref{sec:larger}: at the highest learning rates, our
iterative pruning algorithm fails to find winning tickets.

\subsection{Other Optimization Algorithms}

\begin{figure}
\centering
\includegraphics[width=.8\textwidth]{graphs/cifar10/conv/sgd_rate_sweep1/legend}
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/sgd_rate_sweep1/iteration}%
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/sgd_rate_sweep1/accuracy}
\includegraphics[width=.6\textwidth]{graphs/cifar10/conv/sgd_rate_sweep2/legend}
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/sgd_rate_sweep2/iteration}%
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/sgd_rate_sweep2/accuracy}
\includegraphics[width=.8\textwidth]{graphs/cifar10/conv/sgd_rate_sweep3/legend}
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/sgd_rate_sweep3/iteration}%
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/sgd_rate_sweep3/accuracy}
\caption{The early-stopping iteration and validation accuracy at that iteration  of the iterative lottery ticket experiment on the Conv-2 (top), Conv-4 (middle),
and Conv-6 (bottom) architectures trained using
SGD at various learning rates. Each line represents a different learning rate. The legend for each pair of graphs is above the graphs.}
\label{fig:appendix-conv-sgd}
\end{figure}

\subsubsection{SGD}

Here, we explore the behavior of the lottery ticket experiment when the Conv-2, Conv-4, and Conv-6 networks are optimized with stochastic gradient descent (SGD)
at various learning rates. The results of doing so appear in Figure \ref{fig:appendix-conv-sgd}. In general, these networks---particularly Conv-2 and Conv-4---proved
challenging to train with SGD and Glorot initialization. As Figure \ref{fig:appendix-conv-sgd} reflects, we could not find SGD learning rates for which the unpruned
networks matched the validation accuracy of the same networks when trained with Adam; at best, the SGD-trained unpruned networks were typically 2-3 percentage points
less accurate. At higher learning rates than those in Figure \ref{fig:appendix-conv-adam},
gradients tended to explode when training the unpruned network; at lower learning rates, the networks often failed to learn at all.

At all of the learning rates depicted, we found winning tickets. In all cases, early-stopping times initially decreased with pruning before eventually
increasing again, just as in other lottery ticket experiments. The Conv-6 network also exhibited the same accuracy patterns as other experiments,
with validation accuracy initially increasing with pruning before eventually decreasing again.

However, the Conv-2 and Conv-4 architectures exhibited a different validation accuracy
pattern from other experiments in this paper. Accuracy initially declined with pruning before rising as the network was further pruned; it eventually
matched or surpassed the accuracy of the unpruned network. When they eventually did surpass the accuracy of the original network,
the pruned networks reached early-stopping in
about the same or fewer iterations than the original network, constituting a winning ticket by our definition. Interestingly, this pattern also appeared
for Conv-6 networks at slower SGD learning rates, suggesting that faster learning rates for Conv-2 and Conv-4 than those in Figure \ref{fig:appendix-conv-adam}
might cause the usual lottery ticket accuracy pattern to reemerge. Unfortunately, at these higher learning rates, gradients exploded on the unpruned networks,
preventing us from running these experiments.

\subsubsection{Momentum}

Here, we explore the behavior of the lottery ticket experiment when the network is optimized with SGD with momentum (0.9) at various learning rates.
The results of doing so appear in Figure \ref{fig:appendix-conv-momentum}. In general, the lottery ticket pattern continues to apply, with early-stopping times
decreasing and accuracy increasing as the networks are pruned. However, there were two exceptions to this pattern:
\begin{enumerate}
\item At the very lowest learning rates
(e.g., learning rate 0.001 for Conv-4 and all but the highest learning rate for Conv-2), accuracy initially decreased before increasing to higher levels than reached
by the unpruned network; this is the same pattern we observed when training these networks with SGD. 
\item At the very highest learning rates (e.g., learning rates 0.005 and 0.008 for Conv-2 and Conv-4), early-stopping times never decreased and instead remained
stable before increasing; this is the same pattern we observed for the highest learning rates when training with Adam.
\end{enumerate}


\begin{figure}
\centering
\includegraphics[width=.7\textwidth]{graphs/cifar10/conv/momentum_rate_sweep1/legend}
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/momentum_rate_sweep1/iteration}%
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/momentum_rate_sweep1/accuracy}
\includegraphics[width=.7\textwidth]{graphs/cifar10/conv/momentum_rate_sweep2/legend}
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/momentum_rate_sweep2/iteration}%
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/momentum_rate_sweep2/accuracy}
\includegraphics[width=.7\textwidth]{graphs/cifar10/conv/momentum_rate_sweep3/legend}
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/momentum_rate_sweep3/iteration}%
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/momentum_rate_sweep3/accuracy}
\caption{The early-stopping iteration and validation accuracy at that iteration  of the iterative lottery ticket experiment on the Conv-2 (top), Conv-4 (middle),
and Conv-6 (bottom) architectures trained using
SGD with momentum (0.9) at various learning rates. Each line represents a different learning rate. The legend for each pair of graphs is above the graphs.
Lines that are unstable and contain large error bars (large vertical lines) indicate that some experiments failed to
learn effectively, leading to very low accuracy and very high early-stopping times; these experiments reduce the averages that the lines trace and lead to
much wider error bars.}
\label{fig:appendix-conv-momentum}
\end{figure}

\subsection{Iterative Pruning Rate}
\label{app:conv-rate}

For the convolutional network architectures, we select different pruning rates for convolutional and fully-connected layers. In the Conv-2 and Conv-4 architectures,
convolutional parameters make up a relatively small portion of the overall number of parameters in the models. By pruning convolutions more slowly, we are likely
to be able to prune the model further while maintaining performance. In other words, we hypothesize that, if all layers were pruned evenly, convolutional layers
would become a bottleneck that would make it more difficult to find lower parameter-count models that are still able to learn. For Conv-6, the opposite
may be true: since nearly two thirds of its parameters are in convolutional layers, pruning fully-connected layers could become the bottleneck.

Our criterion for selecting hyperparameters in this section is to find a combination of pruning rates that allows networks to reach the lowest possible parameter-counts
while maintaining validation accuracy at or above the original accuracy and early-stopping times at or below that for the original network.

\begin{figure}
\centering
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/conv_rate_sweep1/legend}
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/conv_rate_sweep1/iteration}%
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/conv_rate_sweep1/accuracy}
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/conv_rate_sweep2/iteration}%
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/conv_rate_sweep2/accuracy}
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/conv_rate_sweep3/iteration}%
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/conv_rate_sweep3/accuracy}
\caption{The early-stopping iteration and validation accuracy at that iteration  of the iterative lottery ticket experiment on the Conv-2 (top), Conv-4 (middle),
and Conv-6 (bottom) architectures with an iterative pruning rate of 20\% for fully-connected layers. Each line represents a different iterative
pruning rate for convolutional layers.}
\label{fig:appendix-conv-rates}
\end{figure}

Figure \ref{fig:appendix-conv-rates} shows the results of performing the iterative lottery ticket experiment on Conv-2 (top), Conv-4 (middle), and Conv-6 (bottom)
with different combinations of pruning rates.

According to our criteria, we select an iterative convolutional pruning rate of 10\% for Conv-2, 10\% for Conv-4, and 15\% for Conv-6.
For each network, any rate between 10\% and 20\% seemed reasonable. Across
all convolutional pruning rates, the lottery ticket pattern continued to appear.

\subsection{Learning Rates (Dropout)}
\label{app:dropout}

In order to train the Conv-2, Conv-4, and Conv-6 architectures with dropout, we repeated the exercise from Section 
\ref{app:conv-learning-rate} to select appropriate learning rates.
Figure \ref{fig:appendix-conv-adam} shows the results of performing the iterative lottery ticket experiment on
Conv-2 (top), Conv-4 (middle), and Conv-6 (bottom) with dropout and Adam at various learning rates.
A network trained with dropout takes longer to learn, so we trained each
architecture for three times as many iterations as in the experiments without dropout: 60,000 iterations for Conv-2, 75,000 iterations for Conv-4, and 90,000
iterations for Conv-6. We iteratively pruned these networks at the rates determined in Section \ref{app:conv-rate}.

The Conv-2 network proved to be difficult to consistently train with dropout. The top right graph in Figure \ref{fig:appendix-conv-adam-dropout} contains wide error bars
and low average accuracy for many learning rates, especially early in the lottery ticket experiments. This indicates that some or all of the training runs failed to learn;
when they were averaged into the other results, they produced the aforementioned pattern in the graphs. At learning rate 0.0001, none of the three trials
learned productively until pruned to more than 26.5\%, at which point all three trials started learning. At learning rate 0.0002, some of the trials failed to learn
productively until several rounds of iterative pruning had passed. At learning rate 0.0003, all three networks learned productively at every pruning level. At learning
rate 0.0004, one network occasionally failed to learn. We selected learning rate 0.0003, which seemed to allow networks to learn productively most often while
achieving among the highest initial accuracy.

It is interesting to note that networks that were unable to learn at a particular learning rate (for example, 0.0001) eventually began learning after several
rounds of the lottery ticket experiment (that is, training, pruning, and resetting repeatedly). It is worth investigating whether this phenomenon was entirely due to
pruning (that is, removing any random collection of weights would put the network in a configuration more amenable to learning) or whether training the network
provided useful information for pruning, even if the network did not show improved accuracy.

For both the Conv-4 and Conv-6 architectures, a slightly slower learning rate (0.0002 as opposed to 0.0003) leads to the highest accuracy on the
unpruned networks in addition to the highest sustained accuracy and fastest sustained learning as the networks are pruned during the lottery ticket experiment.

With dropout, the unpruned Conv-4 architecture reaches an average validation accuracy of 77.6\%, a 2.7 percentage point improvement over the unpruned
Conv-4 network trained without dropout and one percentage point lower than the highest average validation accuracy attained by a winning ticket.
The dropout-trained winning tickets reach 82.6\% average validation accuracy when pruned to 7.6\%. Early-stopping times improve by up to 1.58x (when pruned to
7.6\%), a smaller improvement than then 4.27x achieved by a winning ticket obtained without dropout.

With dropout, the unpruned Conv-6 architecture reaches an average validation
accuracy of 81.3\%, an improvement of 2.2 percentage points over the accuracy without dropout;
this nearly matches the 81.5\% average accuracy obtained by Conv-6 trained without dropout and pruned to 9.31\%. The dropout-trained winning tickets
further improve upon these numbers, reaching 84.8\% average validation accuracy when pruned to 10.5\%.  Improvements in early-stopping times are less
dramatic than without dropout: a 1.5x average improvement when the network is pruned to 15.1\%.

At all learning rates we tested, the lottery ticket pattern generally holds for accuracy, with improvements as the networks are pruned. However,
not all learning rates show the decreases in early-stopping times. To the contrary, none of the learning rates for Conv-2 show clear improvements in early-stopping
times as seen in the other lottery ticket experiments. Likewise, the faster learning rates for Conv-4 and Conv-6 maintain the original early-stopping times until pruned
to about 40\%, at which point early-stopping times steadily increase.



\begin{figure}
\centering
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/dropout_adam_rate_sweep1/legend}
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/dropout_adam_rate_sweep1/iteration}%
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/dropout_adam_rate_sweep1/accuracy}
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/dropout_adam_rate_sweep2/iteration}%
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/dropout_adam_rate_sweep2/accuracy}
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/dropout_adam_rate_sweep3/iteration}%
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/dropout_adam_rate_sweep3/accuracy}
\caption{The early-stopping iteration and validation accuracy at that iteration  of the iterative lottery ticket experiment on the Conv-2 (top), Conv-4 (middle),
and Conv-6 (bottom) architectures trained using dropout and
the Adam optimizer at various learning rates. Each line represents a different learning rate.}
\label{fig:appendix-conv-adam-dropout}
\end{figure}

\subsection{Pruning Convolutions vs. Pruning Fully-Connected Layers}

\begin{figure}
\centering
\includegraphics[width=.6\textwidth]{graphs/cifar10/conv/conv_vs_fc1/legend}%
\vspace{-1em}
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/conv_vs_fc1/iteration}%
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/conv_vs_fc1/accuracy}
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/conv_vs_fc2/iteration}%
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/conv_vs_fc2/accuracy}
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/conv_vs_fc3/iteration}%
\includegraphics[width=.5\textwidth]{graphs/cifar10/conv/conv_vs_fc3/accuracy}
\vspace{-.5em}
\caption{Early-stopping iteration and accuracy of the Conv-2 (top), Conv-4 (middle), and Conv-6 (bottom) networks when only convolutions are pruned, only
fully-connected layers are pruned, and both are pruned. The x-axis measures the number of parameters remaining, making it possible to see the relative
contributions to the overall network made by pruning FC layers and convolutions individually.}
\label{fig:question-1b}
\end{figure}

Figure \ref{fig:question-1b} shows the effect of pruning convolutions alone (green), fully-connected layers alone (orange) and pruning
both (blue). The x-axis measures the number of parameters remaining to emphasize the relative contributions made by pruning convolutions and fully-connected
layers to the overall network. In all three cases, pruning convolutions alone leads to higher test accuracy and faster learning; pruning fully-connected
layers alone generally causes test accuracy to worsen and learning to slow.
However, pruning convolutions alone has limited ability to reduce the overall parameter-count of the network, since
fully-connected layers comprise 99\%, 89\%, and 35\% of the
parameters in Conv-2, Conv-4, and Conv-6.