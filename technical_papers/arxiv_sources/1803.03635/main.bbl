\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2018)Arora, Ge, Neyshabur, and Zhang]{arora}
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi~Zhang.
\newblock Stronger generalization bounds for deep nets via a compression
  approach.
\newblock \emph{ICML}, 2018.

\bibitem[Arpit et~al.(2017)Arpit, Jastrz{\k{e}}bski, Ballas, Krueger, Bengio,
  Kanwal, Maharaj, Fischer, Courville, Bengio, et~al.]{closer}
Devansh Arpit, Stanis{\l}aw Jastrz{\k{e}}bski, Nicolas Ballas, David Krueger,
  Emmanuel Bengio, Maxinder~S Kanwal, Tegan Maharaj, Asja Fischer, Aaron
  Courville, Yoshua Bengio, et~al.
\newblock A closer look at memorization in deep networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  233--242, 2017.

\bibitem[Ba \& Caruana(2014)Ba and Caruana]{do-deep}
Jimmy Ba and Rich Caruana.
\newblock Do deep nets really need to be deep?
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2654--2662, 2014.

\bibitem[Baldi \& Sadowski(2013)Baldi and Sadowski]{understanding-dropout}
Pierre Baldi and Peter~J Sadowski.
\newblock Understanding dropout.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2814--2822, 2013.

\bibitem[Bellec et~al.(2018)Bellec, Kappel, Maass, and Legenstein]{deepr}
Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein.
\newblock Deep rewiring: Training very sparse deep networks.
\newblock \emph{Proceedings of ICLR}, 2018.

\bibitem[Bengio et~al.(2006)Bengio, Roux, Vincent, Delalleau, and
  Marcotte]{convex}
Yoshua Bengio, Nicolas~L Roux, Pascal Vincent, Olivier Delalleau, and Patrice
  Marcotte.
\newblock Convex neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  123--130, 2006.

\bibitem[Cohen et~al.(2016)Cohen, Lo, and Ding]{randomout}
Joseph~Paul Cohen, Henry~Z Lo, and Wei Ding.
\newblock Randomout: Using a convolutional gradient norm to win the filter
  lottery.
\newblock \emph{ICLR Workshop}, 2016.

\bibitem[Cohen \& Shashua(2016)Cohen and Shashua]{inductive-bias}
Nadav Cohen and Amnon Shashua.
\newblock Inductive bias of deep convolutional networks through pooling
  geometry.
\newblock \emph{arXiv preprint arXiv:1605.06743}, 2016.

\bibitem[Denil et~al.(2013)Denil, Shakibi, Dinh, De~Freitas, et~al.]{denil}
Misha Denil, Babak Shakibi, Laurent Dinh, Nando De~Freitas, et~al.
\newblock Predicting parameters in deep learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2148--2156, 2013.

\bibitem[Dong et~al.(2017)Dong, Chen, and Pan]{obs2}
Xin Dong, Shangyu Chen, and Sinno Pan.
\newblock Learning to prune deep neural networks via layer-wise optimal brain
  surgeon.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4860--4874, 2017.

\bibitem[Du et~al.(2019)Du, Zhai, Poczos, and Singh]{provably}
Simon~S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=S1eK3i09YQ}.

\bibitem[Gal \& Ghahramani(2016)Gal and Ghahramani]{bayesian-dropout}
Yarin Gal and Zoubin Ghahramani.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In \emph{international conference on machine learning}, pp.\
  1050--1059, 2016.

\bibitem[Gal et~al.(2017)Gal, Hron, and Kendall]{concrete-dropout}
Yarin Gal, Jiri Hron, and Alex Kendall.
\newblock Concrete dropout.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3584--3593, 2017.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{xavier}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pp.\  249--256, 2010.

\bibitem[Guo et~al.(2016)Guo, Yao, and Chen]{network-surgery}
Yiwen Guo, Anbang Yao, and Yurong Chen.
\newblock Dynamic network surgery for efficient dnns.
\newblock In \emph{Advances In Neural Information Processing Systems}, pp.\
  1379--1387, 2016.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{han-pruning}
Song Han, Jeff Pool, John Tran, and William Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1135--1143, 2015.

\bibitem[Han et~al.(2017)Han, Pool, Narang, Mao, Tang, Elsen, Catanzaro, Tran,
  and Dally]{dsd}
Song Han, Jeff Pool, Sharan Narang, Huizi Mao, Shijian Tang, Erich Elsen, Bryan
  Catanzaro, John Tran, and William~J Dally.
\newblock Dsd: Regularizing deep neural networks with dense-sparse-dense
  training flow.
\newblock \emph{Proceedings of ICLR}, 2017.

\bibitem[Hassibi \& Stork(1993)Hassibi and Stork]{brain-surgeon}
Babak Hassibi and David~G Stork.
\newblock Second order derivatives for network pruning: Optimal brain surgeon.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  164--171, 1993.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[He et~al.(2017)He, Zhang, and Sun]{channel-pruning}
Yihui He, Xiangyu Zhang, and Jian Sun.
\newblock Channel pruning for accelerating very deep neural networks.
\newblock In \emph{International Conference on Computer Vision (ICCV)},
  volume~2, pp.\ ~6, 2017.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Hinton et~al.(2012)Hinton, Srivastava, Krizhevsky, Sutskever, and
  Salakhutdinov]{dropout-pre}
Geoffrey~E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and
  Ruslan~R Salakhutdinov.
\newblock Improving neural networks by preventing co-adaptation of feature
  detectors.
\newblock \emph{arXiv preprint arXiv:1207.0580}, 2012.

\bibitem[Howard et~al.(2017)Howard, Zhu, Chen, Kalenichenko, Wang, Weyand,
  Andreetto, and Adam]{mobilenets}
Andrew~G Howard, Menglong Zhu, Bo~Chen, Dmitry Kalenichenko, Weijun Wang,
  Tobias Weyand, Marco Andreetto, and Hartwig Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock \emph{arXiv preprint arXiv:1704.04861}, 2017.

\bibitem[Hu et~al.(2016)Hu, Peng, Tai, and Tang]{prune-activation}
Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang.
\newblock Network trimming: A data-driven neuron pruning approach towards
  efficient deep architectures.
\newblock \emph{arXiv preprint arXiv:1607.03250}, 2016.

\bibitem[Iandola et~al.(2016)Iandola, Han, Moskewicz, Ashraf, Dally, and
  Keutzer]{squeezenet}
Forrest~N Iandola, Song Han, Matthew~W Moskewicz, Khalid Ashraf, William~J
  Dally, and Kurt Keutzer.
\newblock Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5
  mb model size.
\newblock \emph{arXiv preprint arXiv:1602.07360}, 2016.

\bibitem[Jin et~al.(2016)Jin, Yuan, Feng, and Yan]{skinny-deep}
Xiaojie Jin, Xiaotong Yuan, Jiashi Feng, and Shuicheng Yan.
\newblock Training skinny deep neural networks with iterative hard thresholding
  methods.
\newblock \emph{arXiv preprint arXiv:1607.05423}, 2016.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma et~al.(2015)Kingma, Salimans, and Welling]{variational-dropout}
Diederik~P Kingma, Tim Salimans, and Max Welling.
\newblock Variational dropout and the local reparameterization trick.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2575--2583, 2015.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and Hinton]{cifar10}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[LeCun et~al.(1990)LeCun, Denker, and Solla]{brain-damage}
Yann LeCun, John~S Denker, and Sara~A Solla.
\newblock Optimal brain damage.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  598--605, 1990.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{lenet}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Li et~al.(2018)Li, Farkhoor, Liu, and Yosinski]{intrinsic}
Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski.
\newblock Measuring the intrinsic dimension of objective landscapes.
\newblock \emph{Proceedings of ICLR}, 2018.

\bibitem[Li et~al.(2016)Li, Kadav, Durdanovic, Samet, and
  Graf]{pruning-filters}
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans~Peter Graf.
\newblock Pruning filters for efficient convnets.
\newblock \emph{arXiv preprint arXiv:1608.08710}, 2016.

\bibitem[Liu et~al.(2019)Liu, Sun, Zhou, Huang, and
  Darrell]{rethinking-pruning}
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell.
\newblock Rethinking the value of network pruning.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=rJlnB3C5Ym}.

\bibitem[Louizos et~al.(2017)Louizos, Ullrich, and
  Welling]{bayesian-compression}
Christos Louizos, Karen Ullrich, and Max Welling.
\newblock Bayesian compression for deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3290--3300, 2017.

\bibitem[Louizos et~al.(2018)Louizos, Welling, and Kingma]{l0-reg}
Christos Louizos, Max Welling, and Diederik~P Kingma.
\newblock Learning sparse neural networks through $ l\_0 $ regularization.
\newblock \emph{Proceedings of ICLR}, 2018.

\bibitem[Luo et~al.(2017)Luo, Wu, and Lin]{thinet}
Jian-Hao Luo, Jianxin Wu, and Weiyao Lin.
\newblock Thinet: A filter level pruning method for deep neural network
  compression.
\newblock \emph{arXiv preprint arXiv:1707.06342}, 2017.

\bibitem[Mariet \& Sra(2016)Mariet and Sra]{diversity-nets}
Zelda Mariet and Suvrit Sra.
\newblock Diversity networks.
\newblock \emph{Proceedings of ICLR}, 2016.

\bibitem[Molchanov et~al.(2017)Molchanov, Ashukha, and
  Vetrov]{variational-sparsifies}
Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov.
\newblock Variational dropout sparsifies deep neural networks.
\newblock \emph{arXiv preprint arXiv:1701.05369}, 2017.

\bibitem[Molchanov et~al.(2016)Molchanov, Tyree, Karras, Aila, and
  Kautz]{pruning-resource-efficient}
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz.
\newblock Pruning convolutional neural networks for resource efficient transfer
  learning.
\newblock \emph{arXiv preprint arXiv:1611.06440}, 2016.

\bibitem[Narang et~al.(2017)Narang, Elsen, Diamos, and Sengupta]{exploring}
Sharan Narang, Erich Elsen, Gregory Diamos, and Shubho Sengupta.
\newblock Exploring sparsity in recurrent neural networks.
\newblock \emph{Proceedings of ICLR}, 2017.

\bibitem[Neklyudov et~al.(2017)Neklyudov, Molchanov, Ashukha, and
  Vetrov]{structured-bayesian-pruning}
Kirill Neklyudov, Dmitry Molchanov, Arsenii Ashukha, and Dmitry~P Vetrov.
\newblock Structured bayesian pruning via log-normal multiplicative noise.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6778--6787, 2017.

\bibitem[Neyshabur et~al.(2014)Neyshabur, Tomioka, and Srebro]{in-search}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock \emph{arXiv preprint arXiv:1412.6614}, 2014.

\bibitem[Rasmussen \& Ghahramani(2001)Rasmussen and Ghahramani]{occam}
Carl~Edward Rasmussen and Zoubin Ghahramani.
\newblock Occam's razor.
\newblock In T.~K. Leen, T.~G. Dietterich, and V.~Tresp (eds.), \emph{Advances
  in Neural Information Processing Systems 13}, pp.\  294--300. MIT Press,
  2001.
\newblock URL \url{http://papers.nips.cc/paper/1925-occams-razor.pdf}.

\bibitem[Rissanen(1986)]{mdl}
Jorma Rissanen.
\newblock Stochastic complexity and modeling.
\newblock \emph{The annals of statistics}, pp.\  1080--1100, 1986.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International Journal of Computer Vision}, 115\penalty0
  (3):\penalty0 211--252, 2015.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{vgg}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Srinivas \& Babu(2015{\natexlab{a}})Srinivas and
  Babu]{data-free-pruning}
Suraj Srinivas and R~Venkatesh Babu.
\newblock Data-free parameter pruning for deep neural networks.
\newblock \emph{arXiv preprint arXiv:1507.06149}, 2015{\natexlab{a}}.

\bibitem[Srinivas \& Babu(2015{\natexlab{b}})Srinivas and
  Babu]{learning-architectures}
Suraj Srinivas and R~Venkatesh Babu.
\newblock Learning neural network architectures using backpropagation.
\newblock \emph{arXiv preprint arXiv:1511.05497}, 2015{\natexlab{b}}.

\bibitem[Srinivas \& Babu(2016)Srinivas and Babu]{generalized-dropout}
Suraj Srinivas and R~Venkatesh Babu.
\newblock Generalized dropout.
\newblock \emph{arXiv preprint arXiv:1611.06791}, 2016.

\bibitem[Srinivas et~al.(2017)Srinivas, Subramanya, and
  Venkatesh~Babu]{sparse-neural-networks}
Suraj Srinivas, Akshayvarun Subramanya, and R~Venkatesh~Babu.
\newblock Training sparse neural networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition Workshops}, pp.\  138--145, 2017.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Wan et~al.(2013)Wan, Zeiler, Zhang, Le~Cun, and Fergus]{dropconnect}
Li~Wan, Matthew Zeiler, Sixin Zhang, Yann Le~Cun, and Rob Fergus.
\newblock Regularization of neural networks using dropconnect.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1058--1066, 2013.

\bibitem[Yang et~al.(2017)Yang, Chen, and Sze]{prune-energy}
Tien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze.
\newblock Designing energy-efficient convolutional neural networks using
  energy-aware pruning.
\newblock \emph{arXiv preprint}, 2017.

\bibitem[Zhang et~al.(2016)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{rethinking}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{arXiv preprint arXiv:1611.03530}, 2016.

\bibitem[Zhou et~al.(2018)Zhou, Veitch, Austern, Adams, and Orbanz]{comp}
Wenda Zhou, Victor Veitch, Morgane Austern, Ryan~P Adams, and Peter Orbanz.
\newblock Compressibility and generalization in large-scale deep learning.
\newblock \emph{arXiv preprint arXiv:1804.05862}, 2018.

\end{thebibliography}
