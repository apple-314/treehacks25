\section{Agents in Practice:  Harnessing AI for Good}\label{sec:Agents in Practice:  Harnessing AI for Good}
\input{figures/sec4_mindmap}

The LLM-based agent, as an emerging direction, has gained increasing attention from researchers. Many applications in specific domains and tasks have already been developed, showcasing the powerful and versatile capabilities of agents. We can state with great confidence that, the possibility of having a personal agent capable of assisting users with typical daily tasks is larger than ever before \cite{DBLP:journals/corr/abs-2306-05152}. As an LLM-based agent, its design objective should always be beneficial to humans, i.e., humans can \textit{harness AI for good}. Specifically, we expect the agent to achieve the following objectives:

\begin{enumerate}[leftmargin=*]
    \item Assist users in breaking free from daily tasks and repetitive labor, thereby Alleviating human work pressure and enhancing task-solving efficiency.
    \item No longer necessitates users to provide explicit low-level instructions. Instead, the agent can independently analyze, plan, and solve problems.
    \item After freeing users' hands, the agent also liberates their minds to engage in exploratory and  innovative work, realizing their full potential in cutting-edge scientific fields.
\end{enumerate}

In this section, we provide an in-depth overview of current applications of LLM-based agents, aiming to offer a broad perspective for the practical deployment scenarios (see Figure \ref{fig: sec4_framework}). First, we elucidate the diverse application scenarios of Single Agent, including task-oriented, innovation-oriented, and lifecycle-oriented scenarios (\S \ \ref{sec:General Ability of Single Agent}). Then, we present the significant coordinating potential of Multiple Agents. Whether through cooperative interaction for complementarity or adversarial interaction for advancement, both approaches can lead to higher task efficiency and response quality (\S \ \ref{sec:Collaborative Potential of Multi Agents}). Finally, we categorize the interactive collaboration between humans and agents into two paradigms and introduce the main forms and specific applications respectively (\S \ \ref{sec:Interactive Cooperation between Human-Agent}). The topological diagram for LLM-based agent applications is depicted in Figure \ref{fig:sec4_mindmap}.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.85 \textwidth]{figures/sec4_frame.pdf}
    \caption{Scenarios of LLM-based agent applications. We mainly introduce three scenarios: single-agent deployment, multi-agent interaction, and human-agent interaction. A \textbf{single agent} possesses diverse capabilities and can demonstrate outstanding task-solving performance in various application orientations. When \textbf{multiple agents} interact, they can achieve advancement through cooperative or adversarial interactions. Furthermore, in \textbf{human-agent} interactions, human feedback can enable agents to perform tasks more efficiently and safely, while agents can also provide better service to humans.}
    \label{fig: sec4_framework}
\end{figure} 
\subsection{General Ability of Single Agent}\label{sec:General Ability of Single Agent}
Currently, there is a vibrant development of application instances of LLM-based agents \cite{Chase-LangChain-2022, AgentGPT, gptengineer}. AutoGPT \cite{gravitasauto} is one of the ongoing popular open-source projects aiming to achieve a fully autonomous system. Apart from the basic functions of large language models like GPT-4, the AutoGPT framework also incorporates various practical external tools and long/short-term memory management. After users input their customized objectives, they can free their hands and wait for AutoGPT to automatically generate thoughts and perform specific tasks, all without requiring additional user prompts.



As shown in Figure \ref{fig: sec4_single_agent}, we introduce the astonishingly diverse capabilities that the agent exhibits in scenarios where only one single agent is present.

\subsubsection{Task-oriented Deployment}\label{sec:Task-Oriented Deployment}
The LLM-based agents, which can understand human natural language commands and perform everyday tasks \cite{DBLP:journals/corr/abs-2307-13854}, are currently among the most favored and practically valuable agents by users. This is because they have the potential to enhance task efficiency, alleviate user workload, and promote access for a broader user base. In \textbf{task-oriented deployment}, the agent follows high-level instructions from users, undertaking tasks such as goal decomposition \cite{DBLP:journals/corr/abs-2305-02412, DBLP:conf/icml/HuangAPM22, DBLP:journals/corr/abs-2307-12856, DBLP:journals/corr/abs-2306-07863}, sequence planning of sub-goals \cite{DBLP:journals/corr/abs-2305-02412, DBLP:journals/corr/abs-2308-01552}, interactive exploration of the environment \cite{DBLP:journals/corr/abs-2211-09935, DBLP:journals/corr/abs-2307-13854, DBLP:journals/corr/abs-2305-11854, DBLP:conf/nips/Yao0YN22}, until the final objective is achieved.

To explore whether agents can perform basic tasks, they are first deployed in text-based game scenarios. In this type of game, agents interact with the world purely using natural language \cite{DBLP:journals/corr/abs-2012-02757}. By reading textual descriptions of their surroundings and utilizing skills like memory, planning, and trial-and-error \cite{DBLP:journals/corr/abs-2305-02412}, they predict the next action. However, due to the limitation of foundation language models, agents often rely on reinforcement learning during actual execution \cite{DBLP:journals/corr/abs-2012-02757, DBLP:conf/atal/SinghSM22, DBLP:conf/naacl/AmmanabroluULSR21}.

With the gradual evolution of LLMs \cite{chatgpt2022}, agents equipped with stronger text understanding and generation abilities have demonstrated great potential to perform tasks through natural language. Due to their oversimplified nature, naive text-based scenarios have been inadequate as testing grounds for LLM-based agents \cite{DBLP:journals/corr/abs-2307-13854}. More realistic and complex simulated test environments have been constructed to meet the demand. Based on task types, we divide these simulated environments into \textbf{web scenarios} and \textbf{life scenarios}, and introduce the specific roles that agents play in them.

\paragraph{In web scenarios.}
Performing specific tasks on behalf of users in a web scenario is known as the web navigation problem \cite{DBLP:journals/corr/abs-2305-11854}. Agents interpret user instructions, break them down into multiple basic operations, and interact with computers. This often includes web tasks such as filling out forms, online shopping, and sending emails. Agents need to possess the ability to understand instructions within complex web scenarios, adapt to changes (such as noisy text and dynamic HTML web pages), and generalize successful operations \cite{DBLP:journals/corr/abs-2307-13854}. In this way, agents can achieve accessibility and automation when dealing with unseen tasks in the future \cite{DBLP:conf/naacl/XuMDCHLL21}, ultimately freeing humans from repeated interactions with computer UIs.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/sec4_single2.pdf}
    \caption{Practical applications of the single LLM-based agent in different scenarios. In \textbf{task-oriented deployment}, agents assist human users in solving daily tasks. They need to possess basic instruction comprehension and task decomposition abilities. In \textbf{innovation-oriented deployment}, agents demonstrate the potential for autonomous exploration in scientific domains. In \textbf{lifecycle-oriented deployment}, agents have the ability to continuously explore, learn, and utilize new skills to ensure long-term survival in an open world.}
    \label{fig: sec4_single_agent}
\end{figure} 

Agents trained through reinforcement learning can effectively mimic human behavior using predefined actions like typing, searching, navigating to the next page, etc. They perform well in basic tasks such as online shopping \cite{DBLP:conf/nips/Yao0YN22} and search engine retrieval \cite{DBLP:journals/corr/abs-2112-09332}, which have been widely explored. However, agents without LLM capabilities may struggle to adapt to the more realistic and complex scenarios in the real-world Internet. In dynamic, content-rich web pages such as online forums or online business management \cite{DBLP:journals/corr/abs-2307-13854}, agents often face challenges in performance.

In order to enable successful interactions between agents and more realistic web pages, some researchers \cite{DBLP:journals/corr/abs-2303-17491, DBLP:journals/corr/abs-2306-07863} have started to leverage the powerful HTML reading and understanding abilities of LLMs. By designing prompts, they attempt to make agents understand the entire HTML source code and predict more reasonable next action steps. Mind2Web \cite{DBLP:journals/corr/abs-2306-06070} combines multiple LLMs fine-tuned for HTML, allowing them to summarize verbose HTML code \cite{DBLP:journals/corr/abs-2307-12856} in real-world scenarios and extract valuable information. Furthermore, WebGum \cite{DBLP:journals/corr/abs-2305-11854} empowers agents with visual perception abilities by employing a multimodal corpus containing HTML screenshots. It simultaneously fine-tunes the LLM and a visual encoder, deepening the agent's comprehensive understanding of web pages.


\paragraph{In life scenarios.}
In many daily household tasks in life scenarios, it's essential for agents to understand implicit instructions and apply common-sense knowledge \cite{DBLP:conf/atal/SinghSM22}. For an LLM-based agent trained solely on massive amounts of text, tasks that humans take for granted might require multiple trial-and-error attempts \cite{DBLP:journals/corr/abs-2012-02757}. More realistic scenarios often lead to more obscure and subtle tasks. For example, the agent should proactively turn it on if it's dark and there's a light in the room. To successfully chop some vegetables in the kitchen, the agent needs to anticipate the possible location of a knife \cite{DBLP:journals/corr/abs-2305-02412}.

Can an agent apply the world knowledge embedded in its training data to real interaction scenarios? Huang et al. \cite{DBLP:conf/icml/HuangAPM22} lead the way in exploring this question. They demonstrate that sufficiently large LLMs, with appropriate prompts, can effectively break down high-level tasks into suitable sub-tasks without additional training. However, this static reasoning and planning ability has its potential drawbacks. Actions generated by agents often lack awareness of the dynamic environment around them. For instance, when a user gives the task ``clean the room'', the agent might convert it into unfeasible sub-tasks like ``call a cleaning service'' \cite{DBLP:journals/corr/abs-2210-04964}.

To provide agents with access to comprehensive scenario information during interactions, some approaches directly incorporate spatial data and item-location relationships as additional inputs to the model. This allows agents to gain a precise description of their surroundings \cite{DBLP:journals/corr/abs-2308-01552, DBLP:journals/corr/abs-2210-04964}. Wu et al. \cite{DBLP:journals/corr/abs-2305-02412} introduce the PET framework, which mitigates irrelevant objects and containers in environmental information through an early error correction method \cite{DBLP:journals/corr/abs-2211-09935}. PET encourages agents to explore the scenario and plan actions more efficiently, focusing on the current sub-task.

\subsubsection{Innovation-oriented Deployment}\label{sec:Innovation-Oriented Deployment}
The LLM-based agent has demonstrated strong capabilities in performing tasks and enhancing the efficiency of repetitive work. However, in a more intellectually demanding field, like cutting-edge science, the potential of agents has not been fully realized yet. This limitation mainly arises from two challenges \cite{DBLP:journals/corr/abs-2308-01423}: On one hand, the inherent complexity of science poses a significant barrier. Many domain-specific terms and multi-dimensional structures are difficult to represent using a single text. As a result, their complete attributes cannot be fully encapsulated. This greatly weakens the agent's cognitive level. On the other hand, there is a severe lack of suitable training data in scientific domains, making it difficult for agents to comprehend the entire domain knowledge \cite{DBLP:conf/emnlp/WangJCA22, DBLP:journals/corr/abs-2305-05091}. If the ability for autonomous exploration could be discovered within the agent, it would undoubtedly bring about beneficial innovation in human technology.

Currently, numerous efforts in various specialized domains aim to overcome this challenge \cite{DBLP:journals/corr/abs-2306-15626, lin2023evolutionary, DBLP:journals/mlst/IrwinDHB22}. Experts from the computer field make full use of the agent's powerful code comprehension and debugging abilities \cite{DBLP:journals/corr/abs-2306-05152, DBLP:journals/corr/abs-2308-00245}. In the fields of chemistry and materials, researchers equip agents with a large number of general or task-specific tools to better understand domain knowledge. Agents evolve into comprehensive scientific assistants, proficient in online research and document analysis to fill data gaps. They also employ robotic APIs for real-world interactions, enabling tasks like material synthesis and mechanism discovery \cite{DBLP:journals/corr/abs-2304-05332, bran2023chemcrow,  DBLP:journals/corr/abs-2308-01423}.

The potential of LLM-based agents in scientific innovation is evident, yet we do not expect their exploratory abilities to be utilized in applications that could threaten or harm humans. Boiko et al. \cite{DBLP:journals/corr/abs-2304-05332} study the hidden dangers of agents in synthesizing illegal drugs and chemical weapons, indicating that agents could be misled by malicious users in adversarial prompts. This serves as a warning for our future work.

\subsubsection{Lifecycle-oriented Deployment}\label{sec:Lifecycle-Oriented Deployment}
Building a universally capable agent that can continuously explore, develop new skills, and maintain a long-term life cycle in an open, unknown world is a colossal challenge. This accomplishment is regarded as a pivotal milestone in the field of AGI \cite{DBLP:journals/corr/abs-2302-01560}. Minecraft, as a typical and widely explored simulated survival environment, has become a unique playground for developing and testing the comprehensive ability of an agent. Players typically start by learning the basics, such as mining wood and making crafting tables, before moving on to more complex tasks like fighting against monsters and crafting diamond tools \cite{DBLP:journals/corr/abs-2305-16291}. Minecraft fundamentally reflects the real world, making it conducive for researchers to investigate an agent's potential to survive in the authentic world.

The survival algorithms of agents in Minecraft can generally be categorized into two types \cite{DBLP:journals/corr/abs-2305-16291}: \textbf{low-level control} and \textbf{high-level planning}. Early efforts mainly focused on reinforcement learning \cite{DBLP:journals/corr/abs-2305-16291, DBLP:journals/corr/abs-2211-00688} and imitation learning \cite{DBLP:journals/corr/abs-2007-02701}, enabling agents to craft some low-level items. With the emergence of LLMs, which demonstrated surprising reasoning and analytical capabilities, agents begin to utilize LLM as a high-level planner to guide simulated survival tasks \cite{DBLP:journals/corr/abs-2302-01560, DBLP:conf/icml/NottinghamAS0H023}. Some researchers use LLM to decompose high-level task instructions into a series of sub-goals \cite{DBLP:journals/corr/abs-2303-16563}, basic skill sequences \cite{DBLP:conf/icml/NottinghamAS0H023}, or fundamental keyboard/mouse operations \cite{DBLP:journals/corr/abs-2303-16563}, gradually assisting agents in exploring the open world.

Voyager\cite{DBLP:journals/corr/abs-2305-16291}, drawing inspiration from concepts similar to AutoGPT\cite{gravitasauto}, became the first LLM-based embodied lifelong learning agent in Minecraft, based on the long-term goal of ``discovering as many diverse things as possible''. It introduces a skill library for storing and retrieving complex action-executable code, along with an iterative prompt mechanism that incorporates environmental feedback and error correction. This enables the agent to autonomously explore and adapt to unknown environments without human intervention. An AI agent capable of autonomously learning and mastering the entire real-world techniques may not be as distant as once thought \cite{DBLP:journals/corr/abs-2303-16563}.


\subsection{Coordinating Potential of Multiple Agents} \label{sec:Collaborative Potential of Multi Agents}
\paragraph{Motivation and Background.} Although LLM-based agents possess commendable text understanding and generation capabilities, they operate as isolated entities in nature \cite{DBLP:journals/corr/abs-2306-03314}. They lack the ability to collaborate with other agents and acquire knowledge from social interactions. This inherent limitation restricts their potential to learn from multi-turn feedback from others to enhance their performance \cite{DBLP:journals/corr/abs-2305-16960}. Moreover, they cannot be effectively deployed in complex scenarios requiring collaboration and information sharing among multiple agents.

 As early as 1986, Marvin Minsky made a forward-looking prediction. In his book \textit{The Society of Mind} \cite{minsky1988society}, he introduced a novel theory of intelligence, suggesting that intelligence emerges from the interactions of many smaller agents with specific functions. For instance, certain agents might be responsible for pattern recognition, while others might handle decision-making or generate solutions. This idea has been put into concrete practice with the rise of distributed artificial intelligence \cite{balaji2010introduction}. Multi-agent systems(MAS) \cite{DBLP:journals/ker/WooldridgeJ95}, as one of the primary research domains, focus on how a group of agents can effectively coordinate and collaborate to solve problems. Some specialized communication languages, like KQML \cite{DBLP:conf/cikm/FininFMM94}, were designed early on to support message transmission and knowledge sharing among agents. However, their message formats were relatively fixed, and the semantic expression capacity was limited. In the 21st century, integrating reinforcement learning algorithms (such as Q-learning) with deep learning has become a prominent technique for developing MAS that operate in complex environments \cite{yang2020overview}. Nowadays, the construction approach based on LLMs is beginning to demonstrate remarkable potential. The natural language communication between agents has become more elegant and easily comprehensible to humans, resulting in a significant leap in interaction efficiency.

\paragraph{Potential advantages.}
Specifically, an LLM-based multi-agent system can offer several advantages. Just as Adam Smith clearly stated in \textit{The Wealth of Nations} \cite{smith1937wealth}, ``The greatest improvements in the productive powers of labor, and most of the skill, dexterity, and judgment with which it is directed or applied, seem to be results of the division of labor.'' Based on the principle of division of labor, a single agent equipped with specialized skills and domain knowledge can engage in specific tasks. On the one hand, agents' skills in handling specific tasks are increasingly refined through the division of labor. On the other hand, decomposing complex tasks into multiple subtasks can eliminate the time spent switching between different processes. In the end, efficient division of labor among multiple agents can accomplish a significantly greater workload than when there is no specialization, substantially improving the overall system's efficiency and output quality.

%\paragraph{Role-playing.}
%Before delving into specific multi-agent applications, let's first introduce the concept of role-playing. This technique has been widely adopted in LLM-based MAS. Through carefully crafted prompts, LLMs have the ability to assume various roles, be it a fifth-grade elementary student or a programming expert in the field of computer science. Surprisingly, when an LLM plays a designated role, it can trigger its internal, unique domain knowledge, yielding better answers compared to the responses without a specified role \cite{DBLP:journals/corr/abs-2305-14930}. Role-playing not only empowers LLM-based agents with individual advantages and specialized knowledge but also operates orthogonally to any existing patterns of collaboration (cooperation or negotiation). This significantly enhances the task-solving efficiency of MAS \cite{DBLP:journals/corr/abs-2308-12503}.

% 
In \S \  \ref{sec:General Ability of Single Agent}, we have provided a comprehensive introduction to the versatile abilities of LLM-based agents. Therefore, in this section, we focus on exploring the ways agents interact with each other in a multi-agent environment. Based on current research, these interactions can be broadly categorized as follows: \textbf{Cooperative Interaction for Complementarity} and \textbf{Adversarial Interaction for Advancement} (see Figure \ref{fig: sec4_multi_agent}).

\begin{figure}[t]
    \centering
    \includegraphics[width=1 \textwidth]{figures/sec4_multi.pdf}
    \caption{Interaction scenarios for multiple LLM-based agents. In \textbf{cooperative interaction}, agents collaborate in either a disordered or ordered manner to achieve shared objectives. In \textbf{adversarial interaction}, agents compete in a tit-for-tat fashion to enhance their respective performance.}
    \label{fig: sec4_multi_agent}
\end{figure} 

%\subsubsection{Cooperation for Synergistic Complementarity}
\subsubsection{Cooperative Interaction for Complementarity} \label{sec:Cooperative Engagement for Complementarity}
Cooperative multi-agent systems are the most widely deployed pattern in practical usage. Within such systems, individual agent assesses the needs and capabilities of other agents and actively seeks collaborative actions and information sharing with them \cite{DBLP:journals/corr/abs-2303-17760}. This approach brings forth numerous potential benefits, including enhanced task efficiency, collective decision improvement, and the resolution of complex real-world problems that one single agent cannot solve independently, ultimately achieving the goal of synergistic complementarity. In current LLM-based multi-agent systems, communication between agents predominantly employs natural language, which is considered the most natural and human-understandable form of interaction \cite{DBLP:journals/corr/abs-2303-17760}. We introduce and categorize existing cooperative multi-agent applications into two types: disordered cooperation and ordered cooperation.

\paragraph{Disordered cooperation.}
When three or more agents are present within a system, each agent is free to express their perspectives and opinions openly. They can provide feedback and suggestions for modifying responses related to the task at hand \cite{DBLP:journals/corr/abs-2307-04738}. This entire discussion process is uncontrolled, lacking any specific sequence, and without introducing a standardized collaborative workflow. We refer to this kind of multi-agent cooperation as \textbf{disordered cooperation}.

ChatLLM network \cite{DBLP:journals/corr/abs-2304-12998} is an exemplary representative of this concept. It emulates the forward and backward propagation process within a neural network, treating each agent as an individual node. Agents in the subsequent layer need to process inputs from all the preceding agents and propagate forward. One potential solution is introducing a dedicated coordinating agent in multi-agent systems, responsible for integrating and organizing responses from all agents, thus updating the final answer \cite{DBLP:journals/corr/abs-2307-05300}. However, consolidating a large amount of feedback data and extracting valuable insights poses a significant challenge for the coordinating agent.

Furthermore, \textbf{majority voting} can also serve as an effective approach to making appropriate decisions. However, there is limited research that integrates this module into multi-agent systems at present. Hamilton \cite{DBLP:journals/corr/abs-2301-05327} trains nine independent supreme justice agents to better predict judicial rulings in the U.S. Supreme Court, and decisions are made through a majority voting process.

\paragraph{Ordered cooperation.}
When agents in the system adhere to specific rules, for instance, expressing their opinions one by one in a sequential manner, downstream agents only need to focus on the outputs from upstream. This leads to a significant improvement in task completion efficiency, The entire discussion process is highly organized and ordered. We term this kind of multi-agent cooperation as \textbf{ordered cooperation}. It's worth noting that systems with only two agents, essentially engaging in a conversational manner through a back-and-forth interaction, also fall under the category of ordered cooperation.

CAMEL \cite{DBLP:journals/corr/abs-2303-17760} stands as a successful implementation of a dual-agent cooperative system. Within a role-playing communication framework, agents take on the roles of AI Users (giving instructions) and AI Assistants (fulfilling requests by providing specific solutions). Through multi-turn dialogues, these agents autonomously collaborate to fulfill user instructions \cite{DBLP:journals/corr/abs-2303-17071}. Some researchers have integrated the idea of dual-agent cooperation into a single agent's operation \cite{DBLP:journals/corr/abs-2305-17390}, alternating between rapid and deliberate thought processes to excel in their respective areas of expertise.

Talebirad et al. \cite{DBLP:journals/corr/abs-2306-03314} are among the first to systematically introduce a comprehensive LLM-based multi-agent collaboration framework.  This paradigm aims to harness the strengths of each individual agent and foster cooperative relationships among them. Many applications of multi-agent cooperation have successfully been built upon this foundation \cite{DBLP:journals/corr/abs-2305-16960,DBLP:journals/corr/abs-2308-08155,DBLP:journals/corr/abs-2308-11339,DBLP:journals/corr/abs-2305-13657}. Furthermore, AgentVerse \cite{DBLP:journals/corr/abs-2308-10848} constructs a versatile, multi-task-tested framework for group agents cooperation. It can assemble a team of agents that dynamically adapt according to the task's complexity. To promote more efficient collaboration, researchers hope that agents can learn from successful human cooperation examples \cite{DBLP:journals/corr/abs-2307-07924}. MetaGPT \cite{DBLP:journals/corr/abs-2308-00352} draws inspiration from the classic waterfall model in software development, standardizing agents' inputs/outputs as engineering documents. By encoding advanced human process management experience into agent prompts, collaboration among multiple agents becomes more structured.

However, during MetaGPT's practical exploration, a potential threat to multi-agent cooperation has been identified. Without setting corresponding rules, frequent interactions among multiple agents can amplify minor hallucinations indefinitely \cite{DBLP:journals/corr/abs-2308-00352}. For example, in software development, issues like incomplete functions, missing dependencies, and bugs that are imperceptible to the human eye may arise. Introducing techniques like cross-validation \cite{DBLP:journals/corr/abs-2307-07924} or timely external feedback could have a positive impact on the quality of agent outputs.

% Competition?Argumentation?
%\subsubsection{Argumentation for Respective Advancement}
\subsubsection{Adversarial Interaction for Advancement} \label{sec:Adversarial Interactions for Advancement}
Traditionally, cooperative methods have been extensively explored in multi-agent systems. However, researchers increasingly recognize that introducing concepts from game theory \cite{DBLP:books/daglib/0027240,DBLP:journals/sigact/Aziz10} into systems can lead to more robust and efficient behaviors. In competitive environments, agents can swiftly adjust strategies through dynamic interactions, striving to select the most advantageous or rational actions in response to changes caused by other agents. Successful applications in Non-LLM-based competitive domains already exist \cite{DBLP:journals/nature/SilverHMGSDSAPL16,Campbell2002DeepB}. AlphaGo Zero \cite{DBLP:journals/nature/SilverSSAHGHBLB17}, for instance, is an agent for Go that achieved significant breakthroughs through a process of self-play. Similarly, within LLM-based multi-agent systems, fostering change among agents can naturally occur through competition, argumentation, and debate \cite{DBLP:conf/emnlp/LewisYDPB17,DBLP:journals/corr/abs-1805-00899}. By abandoning rigid beliefs and engaging in thoughtful reflection, \textbf{adversarial interaction} enhances the quality of responses.

Researchers first delve into the fundamental debating abilities of LLM-based agents \cite{DBLP:journals/corr/abs-2305-10142,DBLP:journals/corr/abs-2305-11595}. Findings demonstrate that when multiple agents express their arguments in the state of  ``tit for tat'', one agent can receive substantial external feedback from other agents, thereby correcting its distorted thoughts \cite{DBLP:journals/corr/abs-2305-19118}. Consequently, multi-agent adversarial systems find broad applicability in scenarios requiring high-quality responses and accurate decision-making. In reasoning tasks, Du et al. \cite{DBLP:journals/corr/abs-2305-14325} introduce the concept of debate, endowing agents with responses from fellow peers. When these responses diverge from an agent's own judgments, a ``mental'' argumentation occurs, leading to refined solutions. ChatEval \cite{DBLP:journals/corr/abs-2308-07201} establishes a role-playing-based multi-agent referee team. Through self-initiated debates, agents evaluate the quality of text generated by LLMs, reaching a level of excellence comparable to human evaluators.

The performance of the multi-agent adversarial system has shown considerable promise. However, the system is essentially dependent on the strength of LLMs and faces several basic challenges:

\begin{itemize}[leftmargin=*]
    \item With prolonged debate, LLM's limited context cannot process the entire input. 
    \item In a multi-agent environment, computational overhead significantly increases. 
    \item Multi-agent negotiation may converge to an incorrect consensus, and all agents are firmly convinced of its accuracy \cite{DBLP:journals/corr/abs-2305-14325}. 
\end{itemize}

The development of multi-agent systems is still far from being mature and feasible. Introducing human guides when appropriate to compensate for agents' shortcomings is a good choice to promote the further advancements of agents.

\subsection{Interactive Engagement between Human and Agent}\label{sec:Interactive Cooperation between Human-Agent}
Human-agent interaction, as the name suggests, involves agents collaborating with humans to accomplish tasks. 
With the enhancement of agent capabilities, human involvement becomes progressively essential to effectively guide and oversee agents' actions, ensuring they align with human requirements and objectives \cite{DBLP:journals/corr/abs-2103-14659, DBLP:journals/corr/abs-2209-00626}. 
Throughout the interaction, humans play a pivotal role by offering guidance or by regulating the safety, legality, and ethical conduct of agents. 
% In the interaction process, humans either provide corresponding guidance or regulate and supervise agents' safety, legality, ethics, and so on. 
This is particularly crucial in specialized domains, such as medicine where data privacy concerns exist \cite{paul2023digitization}. In such cases, human involvement can serve as a valuable means to compensate for the lack of data, thereby facilitating smoother and more secure collaborative processes. 
% In certain specialized fields, such as medicine where data privacy concerns are taken into consideration \cite{paul2023digitization}, human involvement can also effectively compensate for the lack of data. 
Moreover, considering the anthropological aspect, language acquisition in humans predominantly occurs through communication and interaction \cite{bassiri2011interactional}, as opposed to merely consuming written content. 
As a result, agents shouldn't exclusively depend on models trained with pre-annotated datasets; instead, they should evolve through online interaction and engagement. 
The interaction between humans and agents can be classified into two paradigms (see Figure \ref{fig: human-agent}): (1) Unequal interaction (i.e., instructor-executor paradigm): humans serve as issuers of instructions, while agents act as executors, essentially participating as assistants to humans in collaboration. (2) Equal interaction (i.e., equal partnership paradigm): agents reach the level of humans, participating on an equal footing with humans in interaction.

\begin{figure}[t]
    \centering
    \includegraphics[width=1 \textwidth]{figures/sec4_human.pdf}
    \caption{Two paradigms of human-agent interaction. In the instructor-executor paradigm (left), humans provide instructions or feedback, while agents act as executors. In the equal partnership paradigm (right), agents are human-like, able to engage in empathetic conversation and participate in collaborative tasks with humans.}
    \label{fig: human-agent}
\end{figure} 

\subsubsection{Instructor-Executor Paradigm} \label{Instructor-Executor Paradigm}
The simplest approach involves human guidance throughout the process: humans provide clear and specific instructions directly, while the agents' role is to understand natural language commands from humans and translate them into corresponding actions \cite{DBLP:journals/aim/TellexKDWBTR11, DBLP:conf/iser/MatuszekHZF12, DBLP:conf/aaai/ChaplotSPRS18}. In \S \ref{sec:General Ability of Single Agent}, we have presented the scenario where agents solve single-step problems or receive high-level instructions from humans. Considering the interactive nature of language, in this section, we assume that the dialogue between humans and agents is also interactive. Thanks to LLMs, the agents are able to interact with humans in a conversational manner: the agent responds to each human instruction, refining its action through alternating iterations to ultimately meet human requirements \cite{DBLP:journals/corr/abs-2305-16291}. While this approach does achieve the goal of human-agent interaction, it places significant demands on humans. It requires a substantial amount of human effort and, in certain tasks, might even necessitate a high level of expertise. To alleviate this issue, the agent can be empowered to autonomously accomplish tasks, while humans only need to provide feedback in certain circumstances. Here, we roughly categorize feedback into two types: quantitative feedback and qualitative feedback.

\paragraph{Quantitative feedback.}
The forms of quantitative feedback mainly include absolute evaluations like binary scores and ratings, as well as relative scores. Binary feedback refers to the positive and negative evaluations provided by humans, which agents utilize to enhance their self-optimization \cite{DBLP:conf/iclr/LiMCRW17, DBLP:conf/acl/IyerKCKZ17, DBLP:conf/nips/Weston16, DBLP:journals/corr/abs-2208-03188, DBLP:journals/corr/abs-2204-03685}. Comprising only two categories, this type of user feedback is often easy to collect, but sometimes it may oversimplify user intent by neglecting potential intermediate scenarios. To showcase these intermediate scenarios, researchers attempt to expand from binary feedback to rating feedback, which involves categorizing into more fine-grained levels. However, the results of Kreutzer et al. \cite{DBLP:conf/naacl/KreutzerKMR18} suggest that there could be significant discrepancies between user and expert annotations for such multi-level artificial ratings, indicating that this labeling method might be inefficient or less reliable. Furthermore, agents can learn human preference from comparative scores like multiple choice \cite{DBLP:conf/acl/YavuzGSY18, DBLP:conf/emnlp/YaoSSY19}.

\paragraph{Qualitative feedback.}
Text feedback is usually offered in natural language, particularly for responses that may need improvement. The format of this feedback is quite flexible. Humans provide advice on how to modify outputs generated by agents, and the agents then incorporate these suggestions to refine their subsequent outputs \cite{DBLP:conf/naacl/MehtaG19, DBLP:conf/naacl/ElgoharyMRFRA21}. For agents without multimodal perception capabilities, humans can also act as critics, offering visual critiques \cite{DBLP:journals/corr/abs-2305-16291}, for instance. Additionally, agents can utilize a memory module to store feedback for future reuse \cite{DBLP:conf/naacl/TandonMCY22}. In \cite{DBLP:journals/corr/abs-2303-16755}, humans give feedback on the initial output generated by agents, prompting the agents to formulate various improvement proposals. The agents then discern and adopt the most suitable proposal, harmonizing with the human feedback. While this approach can better convey human intention compared to quantitative feedback, it might be more challenging for the agents to comprehend. Xu et al. \cite{DBLP:conf/acl/XuUKABW23} compare various types of feedback and observe that combining multiple types of feedback can yield better results. Re-training models based on feedback from multiple rounds of interaction (i.e., continual learning) can further enhance effectiveness. Of course, the collaborative nature of human-agent interaction also allows humans to directly improve the content generated by agents. This could involve modifying intermediate links \cite{DBLP:conf/chi/WuTC22, DBLP:journals/corr/abs-2306-07932} or adjusting the conversation content \cite{DBLP:journals/corr/abs-2307-07047}. In some studies, agents can autonomously judge whether the conversation is proceeding smoothly and seek feedback when errors are generated \cite{DBLP:conf/acl/HancockBMW19, DBLP:journals/corr/abs-2304-10750}. Humans can also choose to participate in feedback at any time, guiding the agent's learning in the right direction \cite{DBLP:conf/iclr/SchickYJPLIYNG023}.

Currently, in addition to tasks like writing \cite{DBLP:journals/corr/abs-2204-03685} and semantic parsing \cite{DBLP:conf/acl/IyerKCKZ17, DBLP:conf/naacl/ElgoharyMRFRA21}, the model of using agents as human assistants also holds tremendous potential in the field of education. For instance, Kalvakurth et al. \cite{DBLP:journals/corr/abs-2303-13548} propose the robot Dona, which supports multimodal interactions to assist students with registration. Gvirsman et al. \cite{DBLP:conf/hri/GvirsmanKNG20} focus on early childhood education, achieving multifaceted interactions between young children, parents, and agents. Agents can also aid in human understanding and utilization of mathematics \cite{DBLP:journals/corr/abs-2307-02502}. 
In the field of medicine, some medical agents have been proposed, showing enormous potential in terms of diagnosis assistance, consultations, and more \cite{DBLP:journals/corr/abs-2305-15075,DBLP:journals/corr/abs-2308-03549}. 
Especially in mental health, research has shown that agents can lead to increased accessibility due to benefits such as reduced cost, time efficiency, and anonymity compared to face-to-face treatment \cite{doi:10.1177/2055207617713827}. 
% In terms of mental health, research has shown that agents can lead to increased accessibility due to benefits such as reduced cost, time efficiency, and anonymity compared to face-to-face treatment \cite{doi:10.1177/2055207617713827}. 
Leveraging such advantages, agents have found widespread applications. Ali et al. \cite{DBLP:conf/iva/AliRLMKRSH20} design LISSA for online communication with adolescents on the autism spectrum, analyzing users' speech and facial expressions in real-time to engage them in multi-topic conversations and provide instant feedback regarding non-verbal cues. Hsu et al. \cite{hsu2023helping} build contextualized language generation approaches to provide tailored assistance for users who seek support on diverse topics ranging from relationship stress to anxiety. 
Furthermore, in other industries including business, a good agent possesses the capability to provide automated services or assist humans in completing tasks, thereby effectively reducing labor costs \cite{DBLP:conf/ijcnn/GaoGT23}. 
Amidst the pursuit of AGI, efforts are directed towards enhancing the multifaceted capabilities of general agents, creating agents that can function as universal assistants in real-life scenarios \cite{DBLP:journals/corr/abs-2306-08640}.

\subsubsection{Equal Partnership Paradigm} \label{Equal Partnership Paradigm}
\paragraph{Empathetic communicator.}
With the rapid development of AI, conversational agents have garnered extensive attention in research fields in various forms, such as personalized custom roles and virtual chatbots \cite{DBLP:series/synthesis/2020McTear}. It has found practical applications in everyday life, business, education, healthcare, and more \cite{DBLP:journals/corr/abs-2106-10901, DBLP:journals/ijmms/RappCB21, adamopoulou2020chatbots}. However, in the eyes of the public, agents are perceived as emotionless machines, and can never replace humans. Although it is intuitive that agents themselves do not possess emotions, can we enable them to exhibit emotions and thereby bridge the gap between agents and humans? Therefore, a plethora of research endeavors have embarked on delving into the empathetic capacities of agents. This endeavor seeks to infuse a human touch into these agents, enabling them to detect sentiments and emotions from human expressions, ultimately crafting emotionally resonant dialogues \cite{DBLP:conf/ijcai/Wang018, DBLP:conf/acl/WangZ18, lin2019caire, DBLP:journals/corr/abs-2110-03949, DBLP:conf/emnlp/LinMSXF19, DBLP:conf/emnlp/MajumderHPLGGMP20, DBLP:conf/aaai/SabourZH22, DBLP:conf/aaai/LiLRRC22}. Apart from generating emotionally charged language, agents can dynamically adjust their emotional states and display them through facial expressions and voice \cite{DBLP:journals/corr/abs-2308-03022}. These studies, viewing agents as empathetic communicators, not only enhance user satisfaction but also make significant progress in fields like healthcare \cite{ hsu2023helping,DBLP:conf/iva/AliRLMKRSH20, DBLP:journals/cbsn/LiuS18} and business marketing \cite{liu2022artificial}. Unlike simple rule-based conversation agents, agents with empathetic capacities can tailor their interactions to meet users' emotional needs \cite{DBLP:conf/amia/SuFJ0C20}.

\paragraph{Human-level participant.}
Furthermore, we hope that agents can be involved in the normal lives of humans, cooperating with humans to complete tasks from a human-level perspective. In the field of games, agents have already reached a high level. As early as the 1990s, IBM introduced the AI Deep Blue \cite{Campbell2002DeepB}, which defeated the reigning world champion in chess at that time. However, in pure competitive environments such as chess \cite{Campbell2002DeepB}, Go \cite{DBLP:journals/nature/SilverHMGSDSAPL16}, and poker \cite{DBLP:journals/corr/MoravcikSBLMBDW17}, the value of communication was not emphasized \cite{meta2022human}. In many gaming tasks, players need to collaborate with each other, devising unified cooperative strategies through effective negotiation \cite{DBLP:conf/iclr/Bakhtin0LGJFMB23,meta2022human,DBLP:conf/nips/CarrollSHGSAD19, bard2020hanabi}. In these scenarios, agents need to first understand the beliefs, goals, and intentions of others, formulate joint action plans for their objectives, and also provide relevant suggestions to facilitate the acceptance of cooperative actions by other agents or humans. In comparison to pure agent cooperation, we desire human involvement for two main reasons: first, to ensure interpretability, as interactions between pure agents could generate incomprehensible language \cite{DBLP:conf/nips/CarrollSHGSAD19}; second, to ensure controllability, as the pursuit of agents with complete ``free will'' might lead to unforeseen negative consequences, carrying the potential for disruption. Apart from gaming scenarios, agents also demonstrate human-level capabilities in other scenarios involving human interaction, showcasing skills in strategy formulation, negotiation, and more. Agents can collaborate with one or multiple humans, determining the shared knowledge among the cooperative partners, identifying which information is relevant to decision-making, posing questions, and engaging in reasoning to complete tasks such as allocation, planning, and scheduling \cite{DBLP:journals/corr/abs-2305-20076}. Furthermore, agents possess persuasive abilities \cite{DBLP:conf/acl/WangSKOYZY19}, dynamically influencing human viewpoints in various interactive scenarios \cite{DBLP:journals/corr/abs-2308-03313}.

The goal of the field of human-agent interaction is to learn and understand humans, develop technology and tools based on human needs, and ultimately enable comfortable, efficient, and secure interactions between humans and agents. Currently, significant breakthroughs have been achieved in terms of usability in this field. In the future, human-agent interaction will continue to focus on enhancing user experience, enabling agents to better assist humans in accomplishing more complex tasks in various domains. The ultimate aim is not to make agents more powerful but to better equip humans with agents. Considering practical applications in daily life, isolated interactions between humans and agents are not realistic. Robots will become colleagues, assistants, and even companions. Therefore, future agents will be integrated into a social network \cite{DBLP:journals/ijsr/AbramsP20}, embodying a certain level of social value.