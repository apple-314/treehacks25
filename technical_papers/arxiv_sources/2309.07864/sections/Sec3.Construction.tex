\section{The Birth of An Agent: Construction of LLM-based Agents}\label{sec:The Birth of An Agent: Construction of LLM-based Agents}


\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]
    {figures/sec3_construction_v2.pdf}
    \caption{Conceptual framework of LLM-based agent with three components: brain, perception, and action. Serving as the controller, the brain module undertakes basic tasks like memorizing, thinking, and decision-making. The perception module perceives and processes multimodal information from the external environment, and the action module carries out the execution using tools and influences the surroundings. Here we give an example to illustrate the workflow: When a human asks whether it will rain, the perception module converts the instruction into an understandable representation for LLMs. Then the brain module begins to reason according to the current weather and the weather reports on the internet. Finally, the action module responds and hands the umbrella to the human. By repeating the above process, an agent can continuously get feedback and interact with the environment.}
    \label{fig: agent_construction}
\end{figure} 

% Inspired by humans, we present a general framework of an LLM-based agent, which composed of three key parts: brain, perception and action. 
``Survival of the Fittest'' \cite{darwin1859origin} shows that if an individual wants to survive in the external environment, he must adapt to the surroundings efficiently. 
This requires him to be cognitive, able to perceive and respond to changes in the outside world, which is consistent with the definition of ``agent'' mentioned in \S \ref{sec:Origin of AI Agent}. 
Inspired by this, we present a general conceptual framework of an LLM-based agent composed of three key parts: brain, perception, and action (see Figure \ref{fig: agent_construction}). 
% \footnote{Many other works propose different frameworks \cite{DBLP:journals/corr/abs-2304-03442, weng2023prompt, DBLP:conf/nips/Yao0YN22, DBLP:journals/corr/abs-2307-02485}.}
% Inspired by the most advanced species in evolution, we believe that if intelligent agents want to make huge breakthroughs in the future, they must possess some key human-like factors: brain, perception and action.
We first describe the structure and working mechanism of the brain, which is primarily composed of a large language model (\S \ \ref{sec:Brain}). The brain is the core of an AI agent because it not only stores knowledge and memories but also undertakes indispensable functions like information processing and decision-making. It can present the process of reasoning and planning, and cope well with unseen tasks, exhibiting the intelligence of an agent. 
Next, we introduce the perception module (\S \ \ref{sec:Perception}). Its core purpose is to broaden the agent's perception space from a text-only domain to a multimodal sphere that includes textual, auditory, and visual modalities. This extension equips the agent to grasp and utilize information from its surroundings more effectively. 
Finally, we present the action module designed to expand the action space of an agent (\S \ \ref{sec:Action}). Specifically, we empower the agent with embodied action ability and tool-handling skills, enabling it to adeptly adapt to environmental changes, provide feedback, and even influence and mold the environment.

The framework can be tailored for different application scenarios, i.e. not every specific component will be used in all studies. 
In general, agents operate in the following workflow: First, the \textbf{perception} module, corresponding to human sensory systems such as the eyes and ears, perceives changes in the external environment and then converts multimodal information into an understandable representation for the agent. 
Subsequently, the \textbf{brain} module, serving as the control center, engages in information processing activities such as thinking, decision-making, and operations with storage including memory and knowledge. 
Finally, the \textbf{action} module, corresponding to human limbs, carries out the execution with the assistance of tools and leaves an impact on the surroundings. By repeating the above process, an agent can continuously get feedback and interact with the environment.
% The brain is responsible for thinking and decision-making, serving as the control center. Perception corresponds to human sensory systems such as the eyes and ears, providing the agent with multimodal capabilities. Action corresponds to human limbs, enabling the agent to perform tasks with the assistance of tools.


\subsection{Brain}\label{sec:Brain}
\input{figures/sec3_brain_mindmap}
The human brain is a sophisticated structure comprised of a vast number of interconnected neurons, capable of processing various information, generating diverse thoughts, controlling different behaviors, and even creating art and culture \cite{marshall2013discoveries}. 
Much like humans, the brain serves as the central nucleus of an AI agent, primarily composed of a large language model. 
\paragraph{Operating mechanism.}
To ensure effective communication, the ability to engage in natural language interaction (\S \ref{sec:Natural Language Interaction}) is paramount. After receiving the information processed by the perception module, the brain module first turns to storage, retrieving in knowledge (\S \ref{sec:knowledge}) and recalling from memory (\S \ref{sec:memory}). These outcomes aid the agent in devising plans, reasoning, and making informed decisions (\S \ref{sec:reasoning and planning}). Additionally, the brain module may memorize the agent's past observations, thoughts, and actions in the form of summaries, vectors, or other data structures. Meanwhile, it can also update the knowledge such as common sense and domain knowledge for future use. 
% When the required information is not readily stored, 
% The LLM-based agent also has the potential of transfer and generalization to achieve this. 
The LLM-based agent may also adapt to unfamiliar scenarios with its inherent generalization and transfer ability (\S \ref{sec:transferability and generalization}). 
% To ensure effective communication, the ability to engage in natural language interaction is paramount. After receiving the information processed by the perception module, the brain module first turns to storage, recalling from memory and retrieving in knowledge. These outcomes aid the agent in devising plans, reasoning, and making informed decisions. Additionally, the brain module may summarize the past memory and learn the new knowledge for future application. When the required information is not readily stored, the agent can also achieve this by transferring and generalizing. 
% When existing information is not readily available in storage, the agent can also accomplish this through transferring and generalizing.
% In order to engage in meaningful communication, the capability for natural language interaction is paramount, which requires an agent to store both world knowledge and crucial memories. 
% Furthermore, the brain, acting as the agent's controller, can make informed decisions by reasoning and planning, all while demonstrating a remarkable adaptability across various scenarios. 
In the subsequent sections, we delve into a detailed exploration of these extraordinary facets of the brain module as depicted in Figure \ref{fig:sec3_mindmap_brain}. 

\subsubsection{Natural Language Interaction}\label{sec:Natural Language Interaction}
As a medium for communication, language contains a wealth of information. In addition to the intuitively expressed content, there may also be the speaker's beliefs, desires, and intentions hidden behind it \cite{searle2007language}. Thanks to the powerful natural language understanding and generation capabilities inherent in LLMs \cite{DBLP:journals/corr/abs-2303-08774,DBLP:journals/corr/abs-2302-13971,  DBLP:journals/corr/abs-2211-05100, almazrouei2023falcon}, agents can proficiently engage in not only basic interactive conversations \cite{DBLP:journals/corr/SerbanLCP16, DBLP:journals/corr/VinyalsL15, DBLP:journals/corr/abs-2001-09977} in multiple languages \cite{DBLP:journals/corr/abs-2302-04023,DBLP:journals/corr/abs-2211-05100} but also exhibit in-depth comprehension abilities, which allow humans to easily understand and interact with agents \cite{DBLP:journals/corr/abs-2305-17066, DBLP:conf/eacl/RollerDGJWLXOSB21}. Besides, LLM-based agents that communicate in natural language can earn more trust and cooperate more effectively with humans \cite{DBLP:journals/corr/abs-2307-02485}.

\paragraph{Multi-turn interactive conversation.}
The capability of multi-turn conversation is the foundation of effective and consistent communication. As the core of the brain module, LLMs, such as GPT series \cite{radford2019language, DBLP:conf/nips/BrownMRSKDNSSAA20, DBLP:journals/corr/abs-2302-13971}, LLaMA series \cite{DBLP:journals/corr/abs-2302-13971, taori2023stanford} and T5 series \cite{DBLP:journals/corr/abs-2210-11416,raffel2020exploring}, can understand natural language and generate coherent and contextually relevant responses, which helps agents to comprehend better and handle various problems \cite{DBLP:journals/corr/abs-2304-04370}. However, even humans find it hard to communicate without confusion in one sitting, so multiple rounds of dialogue are necessary. Compared with traditional text-only reading comprehension tasks like SQuAD \cite{DBLP:conf/emnlp/RajpurkarZLL16}, multi-turn conversations (1) are interactive, involving multiple speakers, and lack continuity; (2) may involve multiple topics, and the information of the dialogue may also be redundant, making the text structure more complex \cite{DBLP:journals/corr/abs-2103-03125}. In general, the multi-turn conversation is mainly divided into three steps: (1) Understanding the history of natural language dialogue, (2) Deciding what action to take, and (3) Generating natural language responses. LLM-based agents are capable of continuously refining outputs using existing information to conduct multi-turn conversations and effectively achieve the ultimate goal \cite{DBLP:journals/corr/abs-2302-04023,DBLP:journals/corr/abs-2103-03125}.


\paragraph{High-quality natural language generation.}
Recent LLMs show exceptional natural language generation capabilities, consistently producing high-quality text in multiple languages \cite{DBLP:journals/corr/abs-2302-04023, DBLP:journals/corr/abs-2303-12528}. The coherency \cite{DBLP:conf/conll/SeePSYM19} and grammatical accuracy \cite{DBLP:journals/corr/abs-2304-01746} of LLM-generated content have shown steady enhancement, evolving progressively from GPT-3 \cite{DBLP:conf/nips/BrownMRSKDNSSAA20} to InstructGPT \cite{DBLP:conf/nips/Ouyang0JAWMZASR22}, and culminating in GPT-4 \cite{DBLP:journals/corr/abs-2303-08774}. See et al. \cite{DBLP:conf/conll/SeePSYM19} empirically affirm that these language models can ``adapt to the style and content of the conditioning text'' \cite{radford2019better}. And the results of Fang et al. \cite{DBLP:journals/corr/abs-2304-01746} suggest that ChatGPT excels in grammar error detection, underscoring its powerful language capabilities. In conversational contexts, LLMs also perform well in key metrics of dialogue quality, including content, relevance, and appropriateness \cite{DBLP:journals/corr/abs-2305-13711}. Importantly, they do not merely copy training data but display a certain degree of creativity, generating diverse texts that are equally novel or even more novel than the benchmarks crafted by humans \cite{DBLP:journals/corr/abs-2111-09509}. Meanwhile, human oversight remains effective through the use of controllable prompts, ensuring precise control over the content generated by these language models \cite{DBLP:conf/eacl/LuZZWY23}.

% \paragraph{Implication Understanding}

\paragraph{Intention and implication understanding.}
\label{intention and implication understanding}
% instruction LLMs
% implication  implied meaning, implicitly conveyed meaning
Although models trained on the large-scale corpus are already intelligent enough to understand instructions, most are still incapable of emulating human dialogues or fully leveraging the information conveyed in language \cite{DBLP:conf/aaai/TellexKDWBTR11}. 
Understanding the implied meanings is essential for effective communication and cooperation with other intelligent agents \cite{DBLP:journals/corr/abs-2109-01355}, and enables one to interpret others' feedback. 
The emergence of LLMs highlights the potential of foundation models to understand human intentions, but when it comes to vague instructions or other implications, it poses a significant challenge for agents \cite{DBLP:journals/corr/abs-2304-08354, DBLP:journals/corr/abs-2305-14763}. 
For humans, grasping the implied meanings from a conversation comes naturally, whereas for agents, they should formalize implied meanings into a reward function that allows them to choose the option in line with the speaker's preferences in unseen contexts \cite{DBLP:conf/acl/LinFKD22}. One of the main ways for reward modeling is inferring rewards based on feedback, which is primarily presented in the form of comparisons \cite{DBLP:conf/nips/ChristianoLBMLA17} (possibly supplemented with reasons \cite{DBLP:conf/hri/BasuSD18}) and unconstrained natural language \cite{DBLP:conf/aaai/SumersHHNG21}. Another way involves recovering rewards from descriptions, using the action space as a bridge \cite{DBLP:conf/acl/LinFKD22}. Jeon et al. \cite{DBLP:conf/nips/JeonMD20} suggests that human behavior can be mapped to a choice from an implicit set of options, which helps to interpret all the information in a single unifying formalism. By utilizing their understanding of context, agents can take highly personalized and accurate action, tailored to specific requirements.

\subsubsection{Knowledge} \label{sec:knowledge}

Due to the diversity of the real world, many NLP researchers attempt to utilize data that has a larger scale. This data usually is unstructured and unlabeled \cite{DBLP:conf/naacl/HillCK16,DBLP:journals/jmlr/CollobertWBKKK11}, yet it contains enormous knowledge that language models could learn. In theory, language models can learn more knowledge as they have more parameters \cite{DBLP:journals/corr/abs-2001-08361}, and it is possible for language models to learn and comprehend everything in natural language. Research \cite{DBLP:conf/emnlp/RobertsRS20} shows that language models trained on a large-scale dataset can encode a wide range of knowledge into their parameters and respond correctly to various types of queries. Furthermore, the knowledge can assist LLM-based agents in making informed decisions \cite{DBLP:journals/expert/McShane09}. All of this knowledge can be roughly categorized into the following types:

\begin{itemize}[leftmargin=*]
    \item \textbf{Linguistic knowledge.} Linguistic knowledge \cite{DBLP:conf/emnlp/VulicPLGK20, DBLP:conf/naacl/HewittM19,DBLP:journals/ipm/RauJZ89} is represented as a system of constraints, a grammar, which defines all and only the possible sentences of the language. It includes morphology, syntax, semantics \cite{DBLP:conf/dasfaa/YangCCHL16,DBLP:conf/emnlp/BeloucifB21}, and pragmatics. Only the agents that acquire linguistic knowledge can comprehend sentences and engage in multi-turn conversations \cite{DBLP:journals/corr/abs-2103-03125}. Moreover, these agents can acquire multilingual knowledge \cite{ DBLP:journals/corr/abs-2302-04023} by training on datasets that contain multiple languages, eliminating the need for extra translation models.
    \item \textbf{Commonsense knowledge.} 
    Commonsense knowledge \cite{DBLP:conf/emnlp/SafaviK21,DBLP:journals/tacl/JiangXAN20,DBLP:conf/emnlp/MadaanZ0YN22} refers to general world facts that are typically taught to most individuals at an early age. For example, people commonly know that medicine is used for curing diseases, and umbrellas are used to protect against rain. Such information is usually not explicitly mentioned in the context. Therefore, the models lacking the corresponding commonsense knowledge may fail to grasp or misinterpret the intended meaning \cite{DBLP:journals/sigmod/TandonVM17}. Similarly, agents without commonsense knowledge may make incorrect decisions, such as not bringing an umbrella when it rains heavily.
    \item \textbf{Professional domain knowledge.} 
    Professional domain knowledge refers to the knowledge associated with a specific domain like programming \cite{DBLP:conf/pldi/Xu0NH22,DBLP:conf/icml/Lai0WZZZYFWY23,DBLP:conf/emnlp/MadaanZ0YN22}, mathematics \cite{DBLP:journals/corr/abs-2110-14168}, medicine \cite{thirunavukarasu2023large}, etc. It is essential for models to effectively solve problems within a particular domain \cite{DBLP:conf/acl/GururanganMSLBD20}. For example, models designed to perform programming tasks need to possess programming knowledge, such as code format. Similarly, models intended for diagnostic purposes should possess medical knowledge like the names of specific diseases and prescription drugs.
\end{itemize}

Although LLMs demonstrate excellent performance in acquiring, storing, and utilizing knowledge \cite{DBLP:journals/corr/abs-2204-06031}, there remain potential issues and unresolved problems. For example, the knowledge acquired by models during training could become outdated or even be incorrect from the start. A simple way to address this is retraining. However, it requires advanced data, extensive time, and computing resources. Even worse, it can lead to catastrophic forgetting \cite{DBLP:conf/aaai/KemkerMAHK18}. Therefore, some researchers\cite{DBLP:conf/emnlp/CaoAT21,DBLP:journals/corr/abs-2305-13172,DBLP:conf/icml/MitchellLBMF22} try editing LLMs to locate and modify specific knowledge stored within the models. This involved unloading incorrect knowledge while simultaneously acquiring new knowledge. Their experiments show that this method can partially edit factual knowledge, but its underlying mechanism still requires further research. Besides, LLMs may generate content that conflicts with the source or factual information \cite{DBLP:conf/icml/ShiCMSDCSZ23}, a phenomenon often referred to as hallucinations \cite{DBLP:journals/corr/abs-2309-01219}. It is one of the critical reasons why LLMs can not be widely used in factually rigorous tasks. To tackle this issue, some researchers \cite{DBLP:journals/corr/abs-2303-08896} proposed a metric to measure the level of hallucinations and provide developers with an effective reference to evaluate the trustworthiness of LLM outputs. Moreover, some researchers\cite{DBLP:journals/corr/abs-2305-14623,DBLP:journals/corr/abs-2305-11738} enable LLMs to utilize external tools\cite{DBLP:journals/corr/abs-2304-08354,DBLP:journals/corr/abs-2302-07842,DBLP:journals/corr/abs-2307-11019} to avoid incorrect knowledge. Both of these methods can alleviate the impact of hallucinations, but further exploration of more effective approaches is still needed.

\subsubsection{Memory} \label{sec:memory}
In our framework, ``memory'' stores sequences of the agent's past observations, thoughts and actions, which is akin to the definition presented by Nuxoll et al. \cite{nuxoll2007extending}. Just as the human brain relies on memory systems to retrospectively harness prior experiences for strategy formulation and decision-making, agents necessitate specific memory mechanisms to ensure their proficient handling of a sequence of consecutive tasks \cite{squire1986mechanisms,schwabe2014reconsolidation,hutter2000theory}. When faced with complex problems, memory mechanisms help the agent to revisit and apply antecedent strategies effectively. Furthermore, these memory mechanisms enable individuals to adjust to unfamiliar environments by drawing on past experiences.

With the expansion of interaction cycles in LLM-based agents, two primary challenges arise. The first pertains to the sheer length of historical records. LLM-based agents process prior interactions in natural language format, appending historical records to each subsequent input. As these records expand, they might surpass the constraints of the Transformer architecture that most LLM-based agents rely on. When this occurs, the system might truncate some content. The second challenge is the difficulty in extracting relevant memories. As agents amass a vast array of historical observations and action sequences, they grapple with an escalating memory burden. This makes establishing connections between related topics increasingly challenging, potentially causing the agent to misalign its responses with the ongoing context.

\paragraph{Methods for better memory capability.} 
Here we introduce several methods to enhance the memory of LLM-based agents.

\begin{itemize}[leftmargin=*]
    \item \textbf{Raising the length limit of Transformers.} The first method tries to address or mitigate the inherent sequence length constraints. The Transformer architecture struggles with long sequences due to these intrinsic limits. As sequence length expands, computational demand grows exponentially due to the pairwise token calculations in the self-attention mechanism. Strategies to mitigate these length restrictions encompass text truncation \cite{DBLP:conf/acl/LewisLGGMLSZ20,DBLP:conf/acl/ParkVS22,DBLP:conf/acl/ZhangWZ19}, segmenting inputs \cite{DBLP:journals/corr/abs-2305-16300,DBLP:journals/corr/abs-2210-05529}, and emphasizing key portions of text \cite{ DBLP:conf/emnlp/NieH0M22,DBLP:journals/corr/abs-2305-01625,DBLP:conf/emnlp/ManakulG21}. Some other works modify the attention mechanism to reduce complexity, thereby accommodating longer sequences \cite{DBLP:conf/nips/ZaheerGDAAOPRWY20,DBLP:conf/naacl/GuoAUONSY22,DBLP:journals/corr/abs-2303-09752,DBLP:conf/acl/RuossDGGCBLV23}. 

    \item \textbf{Summarizing memory.} The second strategy for amplifying memory efficiency hinges on the concept of memory summarization. This ensures agents effortlessly extract pivotal details from historical interactions. Various techniques have been proposed for summarizing memory. Using prompts, some methods succinctly integrate memories \cite{DBLP:journals/corr/abs-2304-13343}, while others emphasize reflective processes to create condensed memory representations \cite{DBLP:journals/corr/abs-2304-03442, DBLP:journals/corr/abs-2308-10144}. Hierarchical methods streamline dialogues into both daily snapshots and overarching summaries \cite{DBLP:journals/corr/abs-2305-10250}. Notably, specific strategies translate environmental feedback into textual encapsulations, bolstering agents' contextual grasp for future engagements \cite{DBLP:journals/corr/abs-2303-11366}. Moreover, in multi-agent environments, vital elements of agent communication are captured and retained \cite{DBLP:journals/corr/abs-2308-07201}.

    \item \textbf{Compressing memories with vectors or data structures.} By employing suitable data structures, intelligent agents boost memory retrieval efficiency, facilitating prompt responses to interactions. Notably, several methodologies lean on embedding vectors for memory sections, plans, or dialogue histories \cite{DBLP:journals/corr/abs-2307-07924,DBLP:journals/corr/abs-2305-10250,DBLP:journals/corr/abs-2305-17144,DBLP:journals/corr/abs-2308-04026}. Another approach translates sentences into triplet configurations \cite{DBLP:journals/corr/abs-2305-14322}, while some perceive memory as a unique data object, fostering varied interactions \cite{DBLP:journals/corr/abs-2308-01542}. Furthermore, ChatDB \cite{DBLP:journals/corr/abs-2306-03901} and DB-GPT \cite{DBLP:journals/corr/abs-2308-05481} integrate the LLMrollers with SQL databases, enabling data manipulation through SQL commands.
\end{itemize}

\paragraph{Methods for memory retrieval.}
When an agent interacts with its environment or users, it is imperative to retrieve the most appropriate content from its memory. This ensures that the agent accesses relevant and accurate information to execute specific actions. 
An important question arises: How can an agent select the most suitable memory? Typically, agents retrieve memories in an automated manner \cite{DBLP:journals/corr/abs-2305-10250, DBLP:journals/corr/abs-2308-04026}. A significant approach in automated retrieval considers three metrics: Recency, Relevance, and Importance. The memory score is determined as a weighted combination of these metrics, with memories having the highest scores being prioritized in the model's context \cite{DBLP:journals/corr/abs-2304-03442}. 

Some research introduces the concept of interactive memory objects, which are representations of dialogue history that can be moved, edited, deleted, or combined through summarization. Users can view and manipulate these objects, influencing how the agent perceives the dialogue \cite{DBLP:journals/corr/abs-2308-01542}. Similarly, other studies allow for memory operations like deletion based on specific commands provided by users \cite{DBLP:journals/corr/abs-2306-03901}. Such methods ensure that the memory content aligns closely with user expectations.


\subsubsection{Reasoning and Planning} \label{sec:reasoning and planning}
\paragraph{Reasoning.}

Reasoning, underpinned by evidence and logic, is fundamental to human intellectual endeavors, serving as the cornerstone for problem-solving, decision-making, and critical analysis \cite{wason1968reasoning, wason1972psychology, galotti1989approaches}. Deductive, inductive, and abductive are the primary forms of reasoning commonly recognized in intellectual endeavor \cite{DBLP:conf/acl/0009C23}. For LLM-based 
agents, like humans, reasoning capacity is crucial for solving complex tasks \cite{DBLP:journals/corr/abs-2303-08774}.

Differing academic views exist regarding the reasoning capabilities of large language models. Some argue language models possess reasoning during pre-training or fine-tuning \cite{DBLP:conf/acl/0009C23}, while others believe it emerges after reaching a certain scale in size \cite{DBLP:journals/tmlr/WeiTBRZBYBZMCHVLDF22, DBLP:journals/corr/abs-2212-09196}. 
Specifically, the representative Chain-of-Thought (CoT) method \cite{DBLP:conf/nips/Wei0SBIXCLZ22,DBLP:conf/nips/KojimaGRMI22} has been demonstrated to elicit the reasoning capacities of large language models by guiding LLMs to generate rationales before outputting the answer.
Some other strategies have also been presented to enhance the performance of LLMs like self-consistency \cite{DBLP:conf/iclr/0002WSLCNCZ23}, self-polish \cite{DBLP:journals/corr/abs-2305-14497}, self-refine \cite{DBLP:journals/corr/abs-2303-17651} and selection-inference \cite{DBLP:conf/iclr/CreswellSH23}, among others.
Some studies suggest that the effectiveness of step-by-step reasoning can be attributed to the local statistical structure of training data, with locally structured dependencies between variables yielding higher data efficiency than training on all variables \cite{DBLP:journals/corr/abs-2305-15408}.

\paragraph{Planning.}

Planning is a key strategy humans employ when facing complex challenges. For humans, planning helps organize thoughts, set objectives, and determine the steps to achieve those objectives \cite{grafman2004planning,unterrainer2006planning,zula2007integrative}.
Just as with humans, the ability to plan is crucial for agents, and central to this planning module is the capacity for reasoning \cite{bratman1988plans,DBLP:books/lib/RussellN03,fainstein2015readings}.  
This offers a structured thought process for agents based on LLMs. Through reasoning, agents deconstruct complex tasks into more manageable sub-tasks, devising appropriate plans for each \cite{sebastia2006decomposition,crosby2013automated}. 
Moreover, as tasks progress, agents can employ introspection to modify their plans, ensuring they align better with real-world circumstances, leading to adaptive and successful task execution.

Typically, planning comprises two stages: plan formulation and plan reflection.

\begin{itemize}[leftmargin=*]
    \item \textbf{Plan formulation.} During the process of plan formulation, agents generally decompose an overarching task into numerous sub-tasks, and various approaches have been proposed in this phase. Notably, some works advocate for LLM-based agents to decompose problems comprehensively in one go, formulating a complete plan at once and then executing it sequentially \cite{DBLP:conf/iclr/ZhouSHWS0SCBLC23,DBLP:conf/corl/IchterBCFHHHIIJ22,DBLP:journals/corr/abs-2305-18323,DBLP:journals/corr/abs-2211-09935}. In contrast, other studies like the CoT-series employ an adaptive strategy, where they plan and address sub-tasks one at a time, allowing for more fluidity in handling intricate tasks in their entirety \cite{DBLP:conf/nips/Wei0SBIXCLZ22,DBLP:conf/nips/KojimaGRMI22,DBLP:journals/corr/abs-2301-13379}. Additionally, some methods emphasize hierarchical planning \cite{DBLP:journals/corr/abs-2305-02412,DBLP:journals/corr/abs-2305-17390}, while others underscore a strategy in which final plans are derived from reasoning steps structured in a tree-like format. The latter approach argues that agents should assess all possible paths before finalizing a plan \cite{DBLP:conf/iclr/0002WSLCNCZ23,DBLP:journals/corr/abs-2305-10601,DBLP:journals/corr/abs-2305-14992,DBLP:conf/icml/HuangAPM22,DBLP:journals/corr/abs-2305-14992}. While LLM-based agents demonstrate a broad scope of general knowledge, they can occasionally face challenges when tasked with situations that require expertise knowledge. Enhancing these agents by integrating them with planners of specific domains has shown to yield better performance \cite{DBLP:journals/corr/abs-2304-11477,DBLP:journals/corr/abs-2307-02485,DBLP:journals/corr/abs-2205-00445,DBLP:journals/corr/abs-2308-06391}.

    \item \textbf{Plan reflection.} Upon formulating a plan, it's imperative to reflect upon and evaluate its merits. LLM-based agents leverage internal feedback mechanisms, often drawing insights from pre-existing models, to hone and enhance their strategies and planning approaches \cite{DBLP:journals/corr/abs-2303-11366,DBLP:journals/corr/abs-2303-17651,DBLP:journals/corr/abs-2305-14323,DBLP:journals/corr/abs-2308-00436}. To better align with human values and preferences, agents actively engage with humans, allowing them to rectify some misunderstandings and assimilate this tailored feedback into their planning methodology \cite{DBLP:journals/corr/abs-2303-17760,DBLP:conf/chi/WuTC22,DBLP:journals/corr/abs-2305-16291}. Furthermore, they could draw feedback from tangible or virtual surroundings, such as cues from task accomplishments or post-action observations, aiding them in revising and refining their plans \cite{DBLP:conf/iclr/YaoZYDSN023,DBLP:journals/corr/abs-2212-04088,DBLP:conf/corl/HuangXXCLFZTMCS22,DBLP:journals/corr/abs-2303-08268,DBLP:journals/corr/abs-2307-06135}.
\end{itemize}


\subsubsection{Transferability and Generalization} \label{sec:transferability and generalization}
Intelligence shouldn't be limited to a specific domain or task, but rather encompass a broad range of cognitive skills and abilities \cite{DBLP:journals/corr/abs-2303-12712}. The remarkable nature of the human brain is largely attributed to its high degree of plasticity and adaptability. It can continuously adjust its structure and function in response to external stimuli and internal needs, thereby adapting to different environments and tasks. These years, plenty of research indicates that pre-trained models on large-scale corpora can learn universal language representations \cite{barandiaran2009defining, peters-etal-2018-deep, DBLP:conf/naacl/DevlinCLT19}. Leveraging the power of pre-trained models, with only a small amount of data for fine-tuning, LLMs can demonstrate excellent performance in downstream tasks \cite{solaiman2021process}. There is no need to train new models from scratch, which saves a lot of computation resources. However, through this task-specific fine-tuning, the models lack versatility and struggle to be generalized to other tasks. Instead of merely functioning as a static knowledge repository, LLM-based agents exhibit dynamic learning ability which enables them to adapt to novel tasks swiftly and robustly \cite{DBLP:conf/nips/Ouyang0JAWMZASR22, DBLP:conf/iclr/WeiBZGYLDDL22, DBLP:conf/iclr/SanhWRBSACSRDBX22}.

\paragraph{Unseen task generalization.} \label{unseen task generalization}
Studies show that instruction-tuned LLMs exhibit zero-shot generalization without the need for task-specific fine-tuning \cite{DBLP:conf/nips/Ouyang0JAWMZASR22, DBLP:journals/corr/abs-2303-08774, DBLP:conf/iclr/WeiBZGYLDDL22, DBLP:conf/iclr/SanhWRBSACSRDBX22, DBLP:journals/corr/abs-2210-11416}. With the expansion of model size and corpus size, LLMs gradually exhibit remarkable emergent abilities in unfamiliar tasks \cite{DBLP:journals/corr/abs-2302-04023}. Specifically, LLMs can complete new tasks they do not encounter in the training stage by following the instructions based on their own understanding. One of the implementations is multi-task learning, for example, FLAN \cite{DBLP:conf/iclr/WeiBZGYLDDL22} finetunes language models on a collection of tasks described via instructions, and T0 \cite{DBLP:conf/iclr/SanhWRBSACSRDBX22} introduces a unified framework that converts every language problem into a text-to-text format. Despite being purely a language model, GPT-4 \cite{DBLP:journals/corr/abs-2303-08774} demonstrates remarkable capabilities in a variety of domains and tasks, including abstraction, comprehension, vision, coding, mathematics, medicine, law, understanding of human motives and emotions, and others \cite{DBLP:journals/corr/abs-2303-12712}. It is noticed that the choices in prompting are critical for appropriate predictions, and training directly on the prompts can improve the models' robustness in generalizing to unseen tasks \cite{DBLP:conf/acl/BachSYWRNSKBFAD22}. Promisingly, such generalization capability can further be enhanced by scaling up both the model size and the quantity or diversity of training instructions \cite{DBLP:journals/corr/abs-2304-08354, DBLP:journals/corr/abs-2212-12017}.

\paragraph{In-context learning.}
Numerous studies indicate that LLMs can perform a variety of complex tasks through in-context learning (ICL), which refers to the models' ability to learn from a few examples in the context \cite{DBLP:journals/corr/abs-2301-00234}. Few-shot in-context learning enhances the predictive performance of language models by concatenating the original input with several complete examples as prompts to enrich the context \cite{DBLP:conf/nips/BrownMRSKDNSSAA20}. The key idea of ICL is learning from analogy, which is similar to the learning process of humans \cite{DBLP:journals/cacm/Winston80}. Furthermore, since the prompts are written in natural language, the interaction is interpretable and changeable, making it easier to incorporate human knowledge into LLMs \cite{DBLP:conf/nips/Wei0SBIXCLZ22, DBLP:conf/acl/LuBM0S22}. Unlike the supervised learning process, ICL doesn't involve fine-tuning or parameter updates, which could greatly reduce the computation costs for adapting the models to new tasks. Beyond text, researchers also explore the potential ICL capabilities in different multimodal tasks \cite{DBLP:conf/cvpr/WangWCS023,DBLP:journals/corr/abs-2301-02111, DBLP:conf/nips/TsimpoukelliMCE21, DBLP:conf/nips/BarGDGE22, DBLP:journals/corr/abs-2304-04675, DBLP:journals/corr/abs-2303-03926}, making it possible for agents to be applied to large-scale real-world tasks.

\paragraph{Continual learning.}
Recent studies \cite{DBLP:journals/corr/abs-2305-16291, zhang2023bootstrap} have highlighted the potential of LLMs' planning capabilities in facilitating continuous learning \cite{Ke2022ContinualLO, Wang2023ACS} for agents, which involves continuous acquisition and update of skills. 
A core challenge in continual learning is catastrophic forgetting \cite{McCloskey1989CatastrophicII}: as a model learns new tasks, it tends to lose knowledge from previous tasks. 
Numerous efforts have been devoted to addressing the above challenge, which can be broadly separated into three groups, introducing regularly used terms in reference to the previous model \cite{kirkpatrick2017overcoming, li2017learning, farajtabar2020orthogonal, smith2023continual}, approximating prior data distributions \cite{lopez2017gradient, de2019episodic, rolnick2019experience}, and designing architectures with task-adaptive parameters \cite{Serr2018OvercomingCF, razdaibiedina2023progressive}. 
LLM-based agents have emerged as a novel paradigm, leveraging the planning capabilities of LLMs to combine existing skills and address more intricate challenges. Voyager \cite{DBLP:journals/corr/abs-2305-16291} attempts to solve progressively harder tasks proposed by the automatic curriculum devised by GPT-4 \cite{DBLP:journals/corr/abs-2303-08774}. By synthesizing complex skills from simpler programs, the agent not only rapidly enhances its capabilities but also effectively counters catastrophic forgetting.



\subsection{Perception}\label{sec:Perception}
\input{figures/sec3_perception_mindmap}
% Humans and animals use sensory organs such as eyes and ears to gather perceptual information from their surroundings. These perception inputs are transformed into neural signals and subsequently transmitted to the relevant cortical areas, where the brain is responsible for integrating signals from various perception modalities. This mechanism grants humans the capacity to perceive their environment and engage with the surrounding world. Just as humans, we hold the same expectations for LLM-based agents. For this reason, it is crucial for agents to be able to receive information from diverse sources and multiple modalities. This expanded capability empowers the agent with a more comprehensive understanding of its working environment, enabling it to interact more effectively with the surroundings and make adaptive decisions. The initiative enriches the agent's perceptual domain and significantly enhances its versatility. In the end, this allows LLM-based agents to address a wider variety of tasks compared to LLMs alone, and becomes an inevitable and essential development direction.


Both humans and animals rely on sensory organs like eyes and ears to gather information from their surroundings. These perceptual inputs are converted into neural signals and sent to the brain for processing \cite{hubel1962receptive,logothetis1996visual}, allowing us to perceive and interact with the world. Similarly, it's crucial for LLM-based agents to receive information from various sources and modalities. This expanded perceptual space helps agents better understand their environment, make informed decisions, and excel in a broader range of tasks, making it an essential development direction. Agent handles this information to the Brain module for processing through the perception module.

In this section, we introduce how to enable LLM-based agents to acquire multimodal perception capabilities, encompassing textual (\S \ \ref{sec:Textual Input}), visual (\S \ \ref{sec:Visual Input}), and auditory inputs (\S \ \ref{sec:Auditory Input}). We also consider other potential input forms (\S \ \ref{sec:Others Input}) such as tactile feedback, gestures, and 3D maps to enrich the agent's perception domain and enhance its versatility.3). The typology diagram for the
LLM-based agent perception is depicted in Figure \ref{fig:sec3_mindmap_perception}.


\subsubsection{Textual Input} \label{sec:Textual Input}
Text is a way to carry data, information, and knowledge, making text communication one of the most important ways humans interact with the world. An LLM-based agent already has the fundamental ability to communicate with humans through textual input and output \cite{gravitasauto}. In a user's textual input, aside from the explicit content, there are also beliefs, desires, and intentions hidden behind it. Understanding implied meanings is crucial for the agent to grasp the potential and underlying intentions of human users, thereby enhancing its communication efficiency and quality with users. However, as discussed in \S \  \ref{intention and implication understanding}, understanding implied meanings within textual input remains challenging for the current LLM-based agent. For example, some works
\cite{DBLP:conf/acl/LinFKD22,DBLP:conf/nips/ChristianoLBMLA17,DBLP:conf/hri/BasuSD18,DBLP:conf/aaai/SumersHHNG21} employ reinforcement learning to perceive implied meanings and models feedback to derive rewards. This helps deduce the speaker's preferences, leading to more personalized and accurate responses from the agent. Additionally, as the agent is designed for use in complex real-world situations, it will inevitably encounter many entirely new tasks. Understanding text instructions for unknown tasks places higher demands on the agent's text perception abilities. As described in \S \ \ref{unseen task generalization}, an LLM that has undergone instruction tuning \cite{DBLP:conf/iclr/WeiBZGYLDDL22} can exhibit remarkable zero-shot instruction understanding and generalization abilities, eliminating the need for task-specific fine-tuning.

\subsubsection{Visual Input}\label{sec:Visual Input}
Although LLMs exhibit outstanding performance in language comprehension \cite{DBLP:journals/corr/abs-2303-08774,chatgpt2022} and multi-turn conversations \cite{DBLP:conf/sigir/LuR0LX20}, they inherently lack visual perception and can only understand discrete textual content. Visual input usually contains a wealth of information about the world, including properties of objects, spatial relationships, scene layouts, and more in the agent's surroundings. Therefore, integrating visual information with data from other modalities can offer the agent a broader context and a more precise understanding \cite{DBLP:conf/icml/DriessXSLCIWTVY23}, deepening the agent's perception of the environment.

To help the agent understand the information contained within images, a straightforward approach is to generate corresponding text descriptions for image inputs, known as image captioning \cite{DBLP:conf/iccv/HuangWCW19,DBLP:conf/cvpr/PanYLM20,DBLP:journals/corr/abs-1912-08226,DBLP:journals/corr/abs-2102-10407,DBLP:journals/corr/abs-2305-06355}. Captions can be directly linked with standard text instructions and fed into the agent. This approach is highly interpretable and doesn't require additional training for caption generation, which can save a significant number of computational resources. However, caption generation is a low-bandwidth method \cite{DBLP:conf/icml/DriessXSLCIWTVY23,DBLP:journals/corr/abs-2308-01399}, and it may lose a lot of potential information during the conversion process. Furthermore,  the agent's focus on images may introduce biases.

Inspired by the excellent performance of transformers \cite{DBLP:conf/nips/VaswaniSPUJGKP17} in natural language processing, researchers have extended their use to the field of computer vision. Representative works like ViT/VQVAE \cite{DBLP:conf/iclr/DosovitskiyB0WZ21,DBLP:conf/nips/OordVK17,DBLP:conf/iclr/MehtaR22,DBLP:conf/nips/TolstikhinHKBZU21,DBLP:conf/icml/TouvronCDMSJ21} have successfully encoded visual information using transformers. Researchers first divide an image into fixed-size patches and then treat these patches, after linear projection, as input tokens for Transformers \cite{DBLP:journals/corr/abs-2304-08485}. In the end, by calculating self-attention between tokens, they are able to integrate information across the entire image, resulting in a highly effective way to perceive visual content. Therefore, some works \cite{DBLP:conf/iclr/LuCZMK23} try to combine the image encoder and LLM directly to train the entire model in an end-to-end way. While the agent can achieve remarkable visual perception abilities, it comes at the cost of substantial computational resources.

Extensively pre-trained visual encoders and LLMs can greatly enhance the agent's visual perception and language expression abilities \cite{DBLP:journals/corr/abs-2302-14045,DBLP:journals/corr/abs-2306-14824}. Freezing one or both of them during training is a widely adopted paradigm that achieves a balance between training resources and model performance \cite{DBLP:conf/icml/0008LSH23}. 
However, LLMs cannot directly understand the output of a visual encoder, so it's necessary to convert the image encoding into embeddings that LLMs can comprehend. In other words, it involves aligning the visual encoder with the LLM. 
This usually requires adding an extra learnable interface layer between them. For example, BLIP-2 \cite{DBLP:conf/icml/0008LSH23} and InstructBLIP \cite{DBLP:journals/corr/abs-2305-06500} use the Querying Transformer(Q-Former) module as an intermediate layer between the visual encoder and the LLM \cite{DBLP:journals/corr/abs-2305-06500}. Q-Former is a transformer that employs learnable query vectors \cite{DBLP:journals/corr/abs-2305-04790}, giving it the capability to extract language-informative visual representations. 
It can provide the most valuable information to the LLM, reducing the agent's burden of learning visual-language alignment and thereby mitigating the issue of catastrophic forgetting. At the same time, some researchers adopt a computationally efficient method by using a single projection layer to achieve visual-text alignment, reducing the need for training additional parameters \cite{zhu2023minigpt, DBLP:journals/corr/abs-2305-16355,DBLP:journals/corr/abs-2306-14824}. Moreover, the projection layer can effectively integrate with the learnable interface to adapt the dimensions of its outputs, making them compatible with LLMs \cite{DBLP:journals/corr/abs-2305-04160,DBLP:journals/corr/abs-2306-02858,DBLP:journals/corr/abs-2306-09093,DBLP:journals/corr/abs-2306-05424}.

%
Video input consists of a series of continuous image frames. As a result, the methods used by agents to perceive images \cite{DBLP:conf/icml/0008LSH23} may be applicable to the realm of videos, allowing the agent to have good perception of video inputs as well. Compared to image information, video information adds a temporal dimension. Therefore, the agent's understanding of the relationships between different frames in time is crucial for perceiving video information. Some works like Flamingo \cite{DBLP:conf/nips/AlayracDLMBHLMM22,DBLP:journals/corr/abs-2304-03373} ensure temporal order when understanding videos using a mask mechanism. The mask mechanism restricts the agent's view to only access visual information from frames that occurred earlier in time when it perceives a specific frame in the video.

\subsubsection{Auditory Input}\label{sec:Auditory Input}
Undoubtedly, auditory information is a crucial component of world information. When an agent possesses auditory capabilities, it can improve its awareness of interactive content, the surrounding environment, and even potential dangers. Indeed, there are numerous well-established models and approaches  \cite{DBLP:journals/corr/abs-2304-12995,DBLP:conf/icml/RadfordKXBMS23,DBLP:conf/nips/RenRTQZZL19} for processing audio as a standalone modality. However, these models often excel at specific tasks. Given the excellent tool-using capabilities of LLMs (which will be discussed in detail in \S \ref{sec:Action}), a very intuitive idea is that the agent can use LLMs as control hubs, invoking existing toolsets or model repositories in a cascading manner to perceive audio information. For instance, AudioGPT \cite{DBLP:journals/corr/abs-2304-12995}, makes full use of the capabilities of models like FastSpeech \cite{DBLP:conf/nips/RenRTQZZL19}, GenerSpeech  \cite{DBLP:conf/icml/RadfordKXBMS23}, Whisper \cite{DBLP:conf/icml/RadfordKXBMS23}, and others \cite{DBLP:conf/ijcai/YeZ0022,DBLP:conf/icml/KimKS21,DBLP:journals/taslp/WangCCLKW23,DBLP:conf/aaai/Liu00CZ22,DBLP:conf/asru/InagumaDYW21} which have achieved excellent results in tasks such as Text-to-Speech, Style Transfer, and Speech Recognition.

An audio spectrogram provides an intuitive representation of the frequency spectrum of an audio signal as it changes over time \cite{flanagan2013speech}. For a segment of audio data over a period of time, it can be abstracted into a finite-length audio spectrogram. An audio spectrogram has a 2D representation, which can be visualized as a flat image. Hence, some research \cite{DBLP:conf/interspeech/GongCG21,DBLP:journals/taslp/HsuBTLSM21} efforts aim to migrate perceptual methods from the visual domain to audio. AST (Audio Spectrogram Transformer) \cite{DBLP:conf/interspeech/GongCG21} employs a Transformer architecture similar to ViT to process audio spectrogram images. By segmenting the audio spectrogram into patches, it achieves effective encoding of audio information. Moreover, some researchers \cite{DBLP:journals/corr/abs-2305-04160,DBLP:journals/corr/abs-2306-02858} have drawn inspiration from the idea of freezing encoders to reduce training time and computational costs. They align audio encoding with data encoding from other modalities by adding the same learnable interface layer.

\subsubsection{Other Input}\label{sec:Others Input}
As mentioned earlier, many studies have looked into perception units for text, visual, and audio. However, LLM-based agents might be equipped with richer perception modules. In the future, they could perceive and understand diverse modalities in the real world, much like humans. For example, agents could have unique touch and smell organs, allowing them to gather more detailed information when interacting with objects. At the same time, agents can also have a clear sense of the temperature, humidity, and brightness in their surroundings, enabling them to take environment-aware actions. Moreover, by efficiently integrating basic perceptual abilities like vision, text, and light sensitivity, agents can develop various user-friendly perception modules for humans. InternGPT \cite{DBLP:journals/corr/abs-2305-05662} introduces pointing instructions. Users can interact with specific, hard-to-describe portions of an image by using gestures or moving the cursor to select, drag, or draw. The addition of pointing instructions helps provide more precise specifications for individual text instructions. Building upon this, agents have the potential to perceive more complex user inputs. For example, technologies such as eye-tracking in AR/VR devices, body motion capture, and even brainwave signals in brain-computer interaction.

Finally, a human-like LLM-based agent should possess awareness of a broader overall environment.  At present, numerous mature and widely adopted hardware devices can assist agents in accomplishing this. Lidar \cite{schwarz2010mapping} can create 3D point cloud maps to help agents detect and identify objects in their surroundings. GPS \cite{parkinson1996progress} can provide accurate location coordinates and can be integrated with map data. Inertial Measurement Units (IMUs) can measure and record the three-dimensional motion of objects, offering details about an object's speed and direction. However, these sensory data are complex and cannot be directly understood by LLM-based agents. Exploring how agents can perceive more comprehensive input is a promising direction for the future.

%  Perception Conclusion
% Drawing from existing research and the previous discussion, here are some insights into how LLM-based agents can effectively process a wider range of input information. The simplest case is when agent receives perceptual information that can be directly used. An example is data obtained from robot sensors, which includes information like object position, size, color, and coordinates (x, y, z). These attributes can be explicitly described using numbers or text, making it possible for LLM-based agents to understand them directly. Naturally, sensory data isn't always simple and easy to comprehend. Some data require preprocessing computations, or removal of noise and outliers. For example, vibration signals from an accelerometer can't be directly understood by the agent. They need to undergo Fourier transformation to extract information about vibration frequency and amplitude.

% In addition, for information whose structure is more complex and encoding beyond the perception range of the agent, we can use approaches similar to those outlined in Section 3.2.2 to extract relevant features in it. Firstly, we can draw inspiration from caption generation methods to create textual descriptions based on data collected by sensors. For instance, we can transform sensor data into natural language descriptions like 'The blue square is accelerating' or 'The temperature is increasing' and feed these descriptions directly to the agent. Secondly, we can call well-performing, task-specific models to handle perception data, which agents may struggle with. InternGPT uses the very popular Segment Anything[] and OCR[] as the perception units to handle typical objects and scene text respectively. Finally, when possible, we can train an encoder specific to the data modality, such as ViT. For perceptual signals that require complex multi-step preprocessing, using a single-step encoder may simplify the conversion process and yield representations that the agent can understand.


\subsection{Action}\label{sec:Action}
\input{figures/sec3_action_mindmap}
After humans perceive their environment, their brains integrate, analyze, and reason with the perceived information and make decisions. 
Subsequently, they employ their nervous systems to control their bodies, enabling adaptive or creative actions in response to the environment, such as engaging in conversation, evading obstacles, or starting a fire. 
When an agent possesses a brain-like structure with capabilities of knowledge, memory, reasoning, planning, and generalization, as well as multimodal perception, it is also expected to possess a diverse range of actions akin to humans to respond to its surrounding environment. In the construction of the agent, the action module receives action sequences sent by the brain module and carries out actions to interact with the environment.
As Figure \ref{fig:sec3_mindmap_action} shows, this section begins with textual output (\S \ \ref{sec:Textual Output}), which is the inherent capability of LLM-based agents. 
Next we talk about the tool-using capability of LLM-based agents (\S \ \ref{sec:Tool Using}), which has proved effective in enhancing their versatility and expertise. 
Finally, we discuss equipping the LLM-based agent with embodied action to facilitate its grounding in the physical world (\S \ \ref{sec:Embodied Action}).


% Typically,  when agents possess a brain-like structure with capabilities of knowledge, memory, reasoning, planning, and generalization, as well as multimodal perception and an extensive action space, the potential for embodied action will emerge. 
% To achieve this, external tools can be used to expand the agent's interaction with the environment, enhancing its versatility and expertise. 
% Furthermore, when agents possess a brain-like structure with capabilities of knowledge, memory, reasoning, planning, and generalization, as well as multimodal perception and an extensive action space, the potential for embodied action will emerge. In other words, they may be able to respond to or change the environment after perceiving and understanding it, much like humans. 
% This section begins with traditional text output and then discusses the motivation, methods, and significance of tool usage and embodied action.

\subsubsection{Textual Output} \label{sec:Textual Output}

As discussed in \S \ \ref{sec:Natural Language Interaction}, the rise and development of Transformer-based generative large language models have endowed LLM-based agents with inherent language generation capabilities \cite{DBLP:journals/corr/abs-2302-04023, DBLP:journals/corr/abs-2303-12528}. The text quality they generate excels in various aspects such as fluency, relevance, diversity, controllability \cite{DBLP:journals/corr/abs-2305-13711, DBLP:conf/conll/SeePSYM19, DBLP:conf/eacl/LuZZWY23, DBLP:journals/corr/abs-2111-09509}. Consequently, LLM-based agents can be exceptionally strong language generators.

\subsubsection{Tool Using}\label{sec:Tool Using}
Tools are extensions of the capabilities of tool users. When faced with complex tasks, humans employ tools to simplify task-solving and enhance efficiency, freeing time and resources. Similarly, agents have the potential to accomplish complex tasks more efficiently and with higher quality if they also learn to use and utilize tools \cite{DBLP:journals/corr/abs-2304-08354}.

LLM-based agents have \textit{limitations} in some aspects, and the use of tools can \textit{strengthen the agents' capabilities}. 
First, although LLM-based agents have a strong knowledge base and expertise, they don't have the ability to memorize every piece of training data \cite{DBLP:journals/corr/abs-2301-13188,DBLP:conf/icail/SavelkaAGWX23}. They may also fail to steer to correct knowledge due to the influence of contextual prompts \cite{DBLP:journals/corr/abs-2302-07842}, or even generate hallucinate knowledge \cite{DBLP:conf/eacl/RollerDGJWLXOSB21}. Coupled with the lack of corpus, training data, and tuning for specific fields and scenarios, agents' expertise is also limited when specializing in specific domains \cite{ling2023domain}. Specialized tools enable LLMs to enhance their expertise, adapt domain knowledge, and be more suitable for domain-specific needs in a pluggable form. 
Furthermore, the decision-making process of LLM-based agents lacks transparency, making them less trustworthy in high-risk domains such as healthcare and finance \cite{DBLP:journals/entropy/LinardatosPK21}. Additionally, LLMs are susceptible to adversarial attacks \cite{DBLP:journals/corr/abs-2307-15043}, and their robustness against slight input modifications is inadequate. In contrast, agents that accomplish tasks with the assistance of tools exhibit stronger interpretability and robustness. The execution process of tools can reflect the agents' approach to addressing complex requirements and enhance the credibility of their decisions. Moreover, for the reason that tools are specifically designed for their respective usage scenarios, agents utilizing such tools are better equipped to handle slight input modifications and are more resilient against adversarial attacks \cite{DBLP:journals/corr/abs-2304-08354}.

LLM-based agents not only require the use of tools, but are also \textit{well-suited} for tool integration. Leveraging the rich world knowledge accumulated through the pre-training process and CoT prompting, LLMs have demonstrated remarkable reasoning and decision-making abilities in complex interactive environments \cite{DBLP:conf/iclr/0002WSLCNCZ23}, which help agents break down and address tasks specified by users in an appropriate way. What's more, LLMs show significant potential in intent understanding and other aspects \cite{ DBLP:journals/corr/abs-2303-08774,DBLP:journals/corr/abs-2302-13971, DBLP:journals/corr/abs-2211-05100, almazrouei2023falcon}. When agents are combined with tools, the threshold for tool utilization can be lowered, thereby fully unleashing the creative potential of human users \cite{DBLP:journals/corr/abs-2304-08354}.

\paragraph{Understanding tools.}
A prerequisite for an agent to use tools effectively is a comprehensive understanding of the tools' application scenarios and invocation methods. Without this understanding, the process of the agent using tools will become untrustworthy and fail to genuinely enhance the agent's capabilities. Leveraging the powerful zero-shot and few-shot learning abilities of LLMs \cite{radford2019language, DBLP:conf/nips/BrownMRSKDNSSAA20}, agents can acquire knowledge about tools by utilizing \textit{zero-shot prompts} that describe tool functionalities and parameters, or \textit{few-shot prompts} that provide demonstrations of specific tool usage scenarios and corresponding methods \cite{DBLP:journals/corr/abs-2302-04761, DBLP:journals/corr/abs-2205-12255}. These learning approaches parallel human methods of learning by consulting tool manuals or observing others using tools \cite{DBLP:journals/corr/abs-2304-08354}. A single tool is often insufficient when facing complex tasks. Therefore, the agents should first decompose the complex task into subtasks in an appropriate manner, and their understanding of tools play a significant role in task decomposition.

\paragraph{Learning to use tools.}
The methods for agents to learn to utilize tools primarily consist of \textit{learning from demonstrations} and \textit{learning from feedback}. This involves mimicking the behavior of human experts \cite{DBLP:journals/csur/HusseinGEJ17, DBLP:conf/icra/LiuGAL18, DBLP:conf/nips/BakerAZHTEHSC22}, as well as understanding the consequences of their actions and making adjustments based on feedback received from both the environment and humans \cite{ DBLP:conf/nips/Ouyang0JAWMZASR22,DBLP:journals/ijrr/LevinePKIQ18, DBLP:journals/corr/abs-2307-04964}. Environmental feedback encompasses result feedback on whether actions have successfully completed the task and intermediate feedback that captures changes in the environmental state caused by actions; human feedback comprises explicit evaluations and implicit behaviors, such as clicking on links \cite{DBLP:journals/corr/abs-2304-08354}.

If an agent rigidly applies tools without \textit{adaptability}, it cannot achieve acceptable performance in all scenarios. Agents need to generalize their tool usage skills learned in specific contexts to more general situations, such as transferring a model trained on Yahoo search to Google search. To accomplish this, it's necessary for agents to grasp the common principles or patterns in tool usage strategies, which can potentially be achieved through meta-tool learning \cite{Clarebout2013}. Enhancing the agent's understanding of relationships between simple and complex tools, such as how complex tools are built on simpler ones, can contribute to the agents' capacity to generalize tool usage. This allows agents to effectively discern nuances across various application scenarios and transfer previously learned knowledge to new tools \cite{DBLP:journals/corr/abs-2304-08354}. Curriculum learning \cite{10.1145/1553374.1553380}, which allows an agent to start from simple tools and progressively learn complex ones, aligns with the requirements. Moreover, benefiting from the understanding of user intent reasoning and planning abilities, agents can better design methods of tool utilization and collaboration and then provide higher-quality outcomes.

\paragraph{Making tools for self-sufficiency.}
Existing tools are often designed for human convenience, which might not be optimal for agents. To make agents use tools better, there's a need for tools specifically designed for agents. These tools should be more modular and have input-output formats that are more suitable for agents. 
If instructions and demonstrations are provided, LLM-based agents also possess the ability to create tools by generating executable programs, or integrating existing tools into more powerful ones \cite{DBLP:journals/corr/abs-2304-08354,DBLP:journals/corr/abs-2305-14318, chen2021evaluating}. and they can learn to perform self-debugging \cite{DBLP:journals/corr/abs-2304-05128}.
Moreover, if the agent that serves as a tool maker successfully creates a tool, it can produce packages containing the tool's code and demonstrations for other agents in a multi-agent system, in addition to using the tool itself \cite{DBLP:journals/corr/abs-2305-17126}.
Speculatively, in the future, agents might become self-sufficient and exhibit a high degree of autonomy in terms of tools. 

\paragraph{Tools can expand the action space of LLM-based agents.}
With the help of tools, agents can utilize various external \textit{resources} such as web applications and other LMs during the reasoning and planning phase \cite{DBLP:journals/corr/abs-2302-04761}. 
This process can provide information with high expertise, reliability, diversity, and quality for LLM-based agents, facilitating their decision-making and action. 
For example, search-based tools can improve the scope and quality of the knowledge accessible to the agents with the aid of external databases, knowledge graphs, and web pages, while domain-specific tools can enhance an agent's expertise in the corresponding field \cite{DBLP:journals/corr/abs-2304-04370,DBLP:journals/corr/abs-2306-08302}. Some researchers have already developed LLM-based controllers that generate SQL statements to query databases, or to convert user queries into search requests and use search engines to obtain the desired results \cite{DBLP:journals/corr/abs-2112-09332,DBLP:journals/corr/abs-2306-03901}. 
What's more, LLM-based agents can use scientific tools to execute tasks like organic synthesis in chemistry, or interface with Python interpreters to enhance their performance on intricate mathematical computation tasks \cite{bran2023chemcrow, DBLP:journals/corr/abs-2308-03427}. For multi-agent systems, communication tools (e.g., emails) may serve as a means for agents to interact with each other under strict security constraints, facilitating their \textit{collaboration}, and showing autonomy and flexibility \cite{DBLP:journals/corr/abs-2304-08354}.

% \paragraph{Tools can expand the action space of foundation models.}
Although the tools mentioned before enhance the capabilities of agents, the medium of interaction with the environment remains text-based. 
However, tools are designed to expand the functionality of language models, and their outputs are not limited to text. 
Tools for non-textual output can diversify the \textit{modalities} of agent actions, thereby expanding the application scenarios of LLM-based agents. For example, image processing and generation can be accomplished by an agent that draws on a visual model \cite{DBLP:journals/corr/abs-2303-04671}. In aerospace engineering, agents are being explored for modeling physics and solving complex differential equations \cite{ogundare2023industrial}; in the field of robotics, agents are required to plan physical operations and control the robot execution \cite{DBLP:conf/corl/IchterBCFHHHIIJ22}; and so on. Agents that are capable of dynamically interacting with the environment or the world through tools, or in a multimodal manner, can be referred to as digitally embodied \cite{DBLP:journals/corr/abs-2304-08354}. The \textit{embodiment} of agents has been a central focus of embodied learning research. We will make a deep discussion on agents' embodied action in \S\ref{sec:Embodied Action}.


\subsubsection{Embodied Action}\label{sec:Embodied Action}
% \paragraph{A Promising Pathway to AGI.}
In the pursuit of Artificial General Intelligence (AGI), the embodied agent is considered a pivotal paradigm while it strives to integrate model intelligence with the physical world. \textit{The Embodiment hypothesis} \cite{smith2005development} draws inspiration from the human intelligence development process, posing that an agent's intelligence arises from continuous interaction and feedback with the environment rather than relying solely on well-curated textbooks. Similarly, unlike traditional deep learning models that learn explicit capabilities from the internet datasets to solve domain problems, people anticipate that LLM-based agents' behaviors will no longer be limited to pure text output or calling exact tools to perform particular domain tasks \cite{DBLP:journals/tetci/DuanYTZT22}. 
Instead, they should be capable of actively perceiving, comprehending, and interacting with physical environments, making decisions, and generating specific behaviors to modify the environment based on LLM's extensive internal knowledge. We collectively term these as \textbf{embodied actions}, which enable agents' ability to interact with and comprehend the world in a manner closely resembling human behavior.

\paragraph{The potential of LLM-based agents for embodied actions.}
Before the widespread rise of LLMs, researchers tended to use methods like reinforcement learning to explore the embodied actions of agents. Despite the extensive success of RL-based embodiment \cite{DBLP:journals/corr/MnihKSGAWR13, DBLP:journals/nature/SilverHMGSDSAPL16, DBLP:journals/corr/abs-1806-10293}, it does have certain limitations in some aspects. In brief, RL algorithms face limitations in terms of data efficiency, generalization, and complex problem reasoning due to challenges in modeling the dynamic and often ambiguous real environment, or their heavy reliance on precise reward signal representations \cite{DBLP:conf/irc/NguyenL19}. 
Recent studies have indicated that leveraging the rich internal knowledge acquired during the pre-training of LLMs can effectively alleviate these issues \cite{DBLP:conf/icml/DriessXSLCIWTVY23, DBLP:conf/corl/HuangXXCLFZTMCS22, DBLP:conf/icml/HuangAPM22, DBLP:journals/corr/abs-2302-00763}. 

\begin{itemize}[leftmargin=*]
    \item \textbf{Cost efficiency.} 
    Some on-policy algorithms struggle with sample efficiency as they require fresh data for policy updates while gathering enough embodied data for high-performance training is costly and noisy. The constraint is also found in some end-to-end models \cite{DBLP:conf/cvpr/PuigRBLWF018, DBLP:journals/corr/abs-2011-13922, DBLP:journals/corr/abs-2108-04927}. By leveraging the intrinsic knowledge from LLMs, agents like PaLM-E \cite{DBLP:conf/icml/DriessXSLCIWTVY23} jointly train robotic data with general visual-language data to achieve significant transfer ability in embodied tasks while also showcasing that geometric input representations can improve training data efficiency.
    
    \item \textbf{Embodied action generalization.}
    As discussed in section \S \ref{sec:transferability and generalization}, an agent's competence should extend beyond specific tasks. When faced with intricate, uncharted real-world environments, it's imperative that the agent exhibits dynamic learning and generalization capabilities. However, the majority of RL algorithms are designed to train and evaluate relevant skills for specific tasks \cite{DBLP:journals/corr/abs-2212-04088, DBLP:journals/corr/abs-1911-05892, DBLP:journals/arc/TipaldiIM22, DBLP:conf/iccv/SavvaMPBKMZWJSL19}. In contrast, fine-tuned by diverse forms and rich task types, LLMs have showcased remarkable cross-task generalization capabilities \cite{longpre2023flan, wang2022self}. For instance, PaLM-E exhibits surprising zero-shot or one-shot generalization capabilities to new objects or novel combinations of existing objects \cite{DBLP:conf/icml/DriessXSLCIWTVY23}. Further, language proficiency represents a distinctive advantage of LLM-based agents, serving both as a means to interact with the environment and as a medium for transferring foundational skills to new tasks \cite{DBLP:conf/icra/LiangHXXHIFZ23}. SayCan \cite{DBLP:conf/corl/IchterBCFHHHIIJ22} decomposes task instructions presented in prompts using LLMs into corresponding skill commands, but in partially observable environments, limited prior skills often do not achieve satisfactory performance \cite{DBLP:journals/corr/abs-2212-04088}. To address this, Voyager \cite{DBLP:journals/corr/abs-2305-16291} introduces the skill library component to continuously collect novel self-verified skills, which allows for the agent's lifelong learning capabilities.
    
    \item \textbf{Embodied action planning.}
    Planning constitutes a pivotal strategy employed by humans in response to complex problems as well as LLM-based agents. Before LLMs exhibited remarkable reasoning abilities, researchers introduced Hierarchical Reinforcement Learning (HRL) methods while the high-level policy constraints sub-goals for the low-level policy and the low-level policy produces appropriate action signals \cite{DBLP:conf/corl/LiXMS19,DBLP:journals/corr/abs-2012-10147,DBLP:conf/nips/00070C22}. Similar to the role of high-level policies, LLMs with emerging reasoning abilities \cite{DBLP:journals/tmlr/WeiTBRZBYBZMCHVLDF22} can be seamlessly applied to complex tasks in a zero-shot or few-shot manner \cite{DBLP:conf/nips/Wei0SBIXCLZ22, DBLP:conf/iclr/0002WSLCNCZ23, DBLP:conf/iclr/ZhouSHWS0SCBLC23, DBLP:journals/corr/abs-2305-14497}. In addition, external feedback from the environment can further enhance LLM-based agents' planning performance. Based on the current environmental feedback, some work \cite{DBLP:journals/corr/abs-2212-04088, DBLP:conf/iclr/YaoZYDSN023, shinn2023reflexion, DBLP:journals/corr/abs-2306-03604} dynamically generate, maintain, and adjust high-level action plans in order to minimize dependency on prior knowledge in partially observable environments, thereby grounding the plan. Feedback can also come from models or humans, which can usually be referred to as the critics, assessing task completion based on the current state and task prompts \cite{DBLP:journals/corr/abs-2303-08774, DBLP:journals/corr/abs-2305-16291}.
\end{itemize}

\paragraph{Embodied actions for LLM-based agents.} 
Depending on the agents' level of autonomy in a task or the complexity of actions, there are several fundamental LLM-based embodied actions, primarily including observation, manipulation, and navigation. 

\begin{itemize}[leftmargin=*]
    \item \textbf{Observation.}
    Observation constitutes the primary ways by which the agent acquires environmental information and updates states, playing a crucial role in enhancing the efficiency of subsequent embodied actions. As mentioned in \S \ref{sec:Perception}, observation by embodied agents primarily occurs in environments with various inputs, which are ultimately converged into a multimodal signal. A common approach entails a pre-trained Vision Transformer (ViT) used as the alignment module for text and visual information and special tokens are marked to denote the positions of multimodal data \cite{DBLP:conf/icml/DriessXSLCIWTVY23, liu2022instruction, DBLP:journals/corr/abs-2305-15021}. 
    Soundspaces \cite{DBLP:conf/eccv/ChenJSGAIRG20} proposes the identification of physical spatial geometric elements guided by reverberant audio input, enhancing the agent's observations with a more comprehensive perspective \cite{DBLP:conf/nips/00070C22}. In recent times, even more research takes audio as a modality for embedded observation. Apart from the widely employed cascading paradigm \cite{DBLP:journals/corr/abs-2304-12995, DBLP:conf/nips/Huang0LCZ22, DBLP:conf/icml/RadfordKXBMS23}, audio information encoding similar to ViT further enhances the seamless integration of audio with other modalities of inputs \cite{DBLP:conf/interspeech/GongCG21}. 
    The agent's observation of the environment can also be derived from real-time linguistic instructions from humans, while human feedback helps the agent in acquiring detail information that may not be readily obtained or parsed \cite{DBLP:journals/corr/abs-2210-06407, DBLP:journals/corr/abs-2305-16291}.

    \item \textbf{Manipulation.}
    In general, manipulation tasks for embodied agents include object rearrangements, tabletop manipulation, and mobile manipulation \cite{DBLP:journals/corr/abs-2305-13246, DBLP:conf/icml/DriessXSLCIWTVY23}. The typical case entails the agent executing a sequence of tasks in the kitchen, which includes retrieving items from drawers and handing them to the user, as well as cleaning the tabletop \cite{DBLP:conf/corl/IchterBCFHHHIIJ22}. Besides precise observation, this involves combining a series of subgoals by leveraging LLM. Consequently, maintaining synchronization between the agent's state and the subgoals is of significance. DEPS \cite{DBLP:journals/corr/abs-2302-01560} utilizes an LLM-based interactive planning approach to maintain this consistency and help error correction from agent's feedback throughout the multi-step, long-haul reasoning process.
    In contrast to these, AlphaBlock \cite{DBLP:journals/corr/abs-2305-18898} focuses on more challenging manipulation tasks (e.g. making a smiley face using building blocks), which requires the agent to have a more grounded understanding of the instructions. Unlike the existing open-loop paradigm, AlphaBlock constructs a dataset comprising 35 complex high-level tasks, along with corresponding multi-step planning and observation pairs, and then fine-tunes a multimodal model to enhance its comprehension of high-level cognitive instructions.

    \item \textbf{Navigation.} 
    Navigation permits agents to dynamically alter their positions within the environment, which often involves multi-angle and multi-object observations, as well as long-horizon manipulations based on current exploration \cite{DBLP:journals/corr/abs-2305-13246}. Before navigation, it is essential for embodied agents to establish prior internal maps about the external environment, which are typically in the form of a topological map, semantic map or occupancy map \cite{DBLP:journals/tetci/DuanYTZT22}. For example, LM-Nav \cite{DBLP:conf/corl/ShahOIL22} utilizes the VNM \cite{DBLP:conf/icra/ShahEKRL21} to create an internal topological map. It further leverages the LLM and VLM for decomposing input commands and analyzing the environment to find the optimal path. Furthermore, some \cite{DBLP:conf/icra/HuangMZB23, DBLP:conf/cvpr/GeorgakisSWDMRD22} highlight the importance of spatial representation to achieve the precise localization of spatial targets rather than conventional point or object-centric navigation actions by leveraging the pre-trained VLM model to combine visual features from images with 3D reconstructions of the physical world \cite{DBLP:journals/tetci/DuanYTZT22}. Navigation is usually a long-horizon task, where the upcoming states of the agent are influenced by its past actions. A memory buffer and summary mechanism are needed to serve as a reference for historical information \cite{DBLP:journals/corr/abs-2305-16986}, which is also employed in Smallville and Voyager \cite{DBLP:journals/corr/abs-2304-03442, DBLP:journals/corr/abs-2305-16291, DBLP:journals/corr/abs-2303-03480, DBLP:conf/cvpr/LiZZYLZWYZHCG22}.
    Additionally, as mentioned in \S \ref{sec:Perception}, some works have proposed the audio input is also of great significance, but integrating audio information presents challenges in associating it with the visual environment. A basic framework includes a dynamic path planner that uses visual and auditory observations along with spatial memories to plan a series of actions for navigation \cite{DBLP:conf/nips/00070C22, DBLP:conf/icra/GanZ0GT20}. 
    
\end{itemize}

By integrating these, the agent can accomplish more complex tasks, such as embodied question answering, whose primary objective is autonomous exploration of the environment, and responding to pre-defined multimodal questions, such as \textit{Is the watermelon in the kitchen larger than the pot? Which one is harder?} To address these questions, the agent needs to navigate to the kitchen, observe the sizes of both objects and then answer the questions through comparison \cite{DBLP:journals/tetci/DuanYTZT22}. 

In terms of control strategies, as previously mentioned, LLM-based agents trained on particular embodied datasets typically generate high-level policy commands to control low-level policies for achieving specific sub-goals. The low-level policy can be a robotic transformer \cite{DBLP:conf/icml/DriessXSLCIWTVY23, DBLP:journals/corr/abs-2212-06817, DBLP:journals/corr/abs-2307-15818}, which takes images and instructions as inputs and produces control commands for the end effector as well as robotic arms in particular embodied tasks \cite{DBLP:conf/corl/IchterBCFHHHIIJ22}. Recently, in virtual embodied environments, the high-level strategies are utilized to control agents in gaming \cite{DBLP:journals/corr/abs-2305-17144,DBLP:journals/corr/abs-2302-01560, DBLP:journals/corr/abs-2305-16291, DBLP:conf/nips/FanWJMYZTHZA22} or simulated worlds \cite{DBLP:journals/corr/abs-2304-03442, DBLP:journals/corr/abs-2303-17760, DBLP:journals/corr/abs-2307-07924}. For instance, Voyager \cite{DBLP:journals/corr/abs-2305-16291} calls the Mineflayer \cite{Mineflayer-JavaScript-2013} API interface to continuously acquire various skills and explore the world.



\paragraph{Prospective future of the embodied action.} 
LLM-based embodied actions are seen as the bridge between virtual intelligence and the physical world, enabling agents to perceive and modify the environment much like humans. However, there remain several constraints such as high costs of physical-world robotic operators and the scarcity of embodied datasets, which foster a growing interest in investigating agents' embodied actions within simulated environments like Minecraft \cite{DBLP:journals/corr/abs-2302-01560, DBLP:journals/corr/abs-2106-14876, DBLP:conf/nips/FanWJMYZTHZA22, DBLP:journals/corr/abs-2305-16291, DBLP:conf/icml/NottinghamAS0H023}. By utilizing the Mineflayer \cite{Mineflayer-JavaScript-2013} API, these investigations enable cost-effective examination of a wide range of embodied agents' operations including exploration, planning, self-improvement, and even lifelong learning \cite{DBLP:journals/corr/abs-2305-16291}. Despite notable progress, achieving optimal embodied actions remains a challenge due to the significant disparity between simulated platforms and the physical world. To enable the effective deployment of embodied agents in real-world scenarios, an increasing demand exists for embodied task paradigms and evaluation criteria that closely mirror real-world conditions \cite{DBLP:journals/tetci/DuanYTZT22}. On the other hand, learning to ground language for agents is also an obstacle. For example, expressions like ``jump down like a cat'' primarily convey a sense of lightness and tranquility, but this linguistic metaphor requires adequate world knowledge \cite{DBLP:conf/emnlp/BiskHTABCLLMNPT20}. \cite{DBLP:conf/icml/SumersMAF023} endeavors to amalgamate text distillation with Hindsight Experience Replay (HER) to construct a dataset as the supervised signal for the training process. Nevertheless, additional investigation on grounding embodied datasets still remains necessary while embodied action plays an increasingly pivotal role across various domains in human life.



