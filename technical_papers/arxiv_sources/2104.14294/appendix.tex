%!TEX root = supp.tex
\renewcommand{\thesubsection}{\Alph{subsection}}
\newcommand\tab[1][5mm]{\hspace*{#1}}
\section*{Appendix}

\subsection{Additional Results}
\label{ap:more_results}
\paragraph{$k$-NN classification.}
\label{sec:knn}
In Tab.~\ref{tab:knn}, we evaluate the frozen representations given by ResNet-50 or ViT-small pre-trained with \OURS with two evaluation protocols: linear or $k$-NN.
For both evaluations, we extract representations from a pre-trained network without using any data augmentation.
Then, we perform classification either with weighted $k$-NN or with a linear regression learned with \texttt{cyanure} library~\cite{mairal2019cyanure}.
In Tab.~\ref{tab:knn} we see that ViT-S accuracies are better than accuracies obtained with RN50 both with a linear or a $k$-NN classifier.
However, the performance gap when using the $k$-NN evaluation is much more significant than when considering linear evaluation.
For example on ImageNet 1\%, ViT-S outperforms ResNet-50 by a large margin of $+14.1\%$ with $k$-NN evaluation.
This suggests that transformers architectures trained with \OURS might offer more model flexibility that benefits the $k$-NN evaluation.
$K$-NN classifiers have the great advantage of being fast and light to deploy, without requiring any domain adaptation.
Overall, ViT trained with \OURS provides features that combine particularly well with $k$-NN classifiers.

\begin{table}[h]
  \centering
\small
  \setlength{\tabcolsep}{5pt}
\caption{\textbf{$k$-NN and linear evaluation for ViT-S/16 and ResNet-50 pre-trained with \OURS.}
	We use ImageNet-1k~\cite{russakovsky2015imagenet} (``Inet''), Places205~\cite{zhou2014learning}, PASCAL VOC~\cite{everingham2010pascal} and Oxford-102 flowers (``FLOWERS'')~\cite{nilsback2008automated}.
ViT trained with \OURS provides features that are particularly $k$-NN friendly.}
  \begin{tabular}{@{}l ccg c ccg@{}}
    \toprule
    & \multicolumn{3}{c}{Logistic} && \multicolumn{3}{c}{$k$-NN} \\
    \cmidrule{2-4}
    \cmidrule{6-8}
	  & RN50 & ViT-S & $\Delta$ && RN50 & ViT-S & $\Delta$ \\
    \midrule
	  Inet   100\% & 72.1 & 75.7 & 3.6 && 67.5 & 74.5 & 7.0 \\
	  Inet   10\% & 67.8 & 72.2 & 4.4 && 59.3 & 69.1 & 9.8 \\
	  Inet    1\% & 55.1 & 64.5 & 9.4 && 47.2 & 61.3 & 14.1 \\
    Pl. 10\% & 53.4 & 52.1 & -1.3  && 46.9 & 48.6 & 1.7 \\
    Pl.  1\% & 46.5 & 46.3 & -0.2 && 39.2 & 41.3 & 2.1 \\
    VOC07     & 88.9 & 89.2 & 0.3 && 84.9 & 88.0 & 3.1 \\
    FLOWERS     & 95.6 & 96.4 & 0.8 && 87.9 & 89.1 & 1.2 \\
    %DOGS     & 75.8 & 84.0 & 8.2 && 62.1 & 79.2 & 17.1 \\
    %CUB     & 44.8 & 74.0 & 29.2 && 24.1 & 60.3 & 36.2 \\
    \midrule
    Average $\Delta$     & & & 2.4 && & & \bf 5.6 \\
    \bottomrule
  \end{tabular}
  \label{tab:knn}
\end{table}

\begin{table}[t]
  \centering
  \setlength{\tabcolsep}{.7em}
	\caption{
\textbf{ImageNet classification with different pretraining.}
Top-1 accuracy on ImageNet for supervised ViT-B/16 models using different pretrainings or using an additional pretrained convnet to guide the training.
The methods use different image resolution (``res.'') and training procedure (``tr. proc.''), i.e., data augmentation and optimization.
``MPP'' is \textit{Masked Patch Prediction}.
}
    \begin{tabular}{@{} lc c ccc @{}}
    \toprule
    \multicolumn{2}{c}{Pretraining} && & &  \\
    \cmidrule{1-2}
    method & data && res. & tr. proc. & Top-1\\
    \midrule
    \multicolumn{4}{@{}l}{\textit{Pretrain on additional data}}\\
	  MMP        &  JFT-300M &&384&\cite{dosovitskiy2020image}  & 79.9 \\
	  Supervised & JFT-300M  &&384&\cite{dosovitskiy2020image}  & 84.2 \\
    \midrule
    \multicolumn{4}{@{}l}{\textit{Train with additional model}}\\
	  Rand. init. & - &&224&\cite{touvron2020training}& 83.4 \\
    \midrule
    \multicolumn{4}{@{}l}{\textit{No additional data nor model}}\\
	  Rand. init. & -       &&224 &\cite{dosovitskiy2020image} & 77.9 \\
	  Rand. init. & -        &&224&\cite{touvron2020training}   & 81.8 \\
	  Supervised & ImNet &&224&\cite{touvron2020training}   & 81.9 \\
    \rowcolor{Light}
	  \OURS & ImNet      &&224&\cite{touvron2020training}   & 82.8 \\
	  %&&Ours & ViT-B/16 & \OURS & ImageNet & - \\
    \bottomrule
  \end{tabular}
  \label{tab:finetune}
\end{table}

\paragraph{Self-supervised ImageNet pretraining of ViT.}
In this experiment, we study the impact of pretraining a supervised ViT model with our method.
In Tab.~\ref{tab:finetune}, we compare the performance of supervised ViT models that are initialized with different pretraining or guided during training with an additional pretrained convnet.
The first set of models are pretrained with and without supervision on the large curated dataset composed of 300M images.
The second set of models are trained with hard knowledge distillation from a pretrained supervised RegNetY~\cite{radosavovic2020designing}.
The last set of models do not use any additional data nor models, and are initialized either randomly or after a pretraining with \OURS on ImageNet.
Compare to random initialization, pretraining with \OURS leads to a performance gain of +1\%.
This is not caused by a longer training since pretraining with supervision instead of \OURS does not improve performance.
Using self-supervised pretraining reduces the gap with models pretrained on extra data or distilled from a convnet.

\paragraph{Low-shot learning on ImageNet.}
We evaluate the features obtained with \OURS applied on ViT-S on low-shot learning.
In Tab.~\ref{tab:semisup}, we report the validation accuracy of a logistic regression trained on frozen features (\textsc{frozen}) with 1\% and 10\% labels.
The logistic regression is trained with the \texttt{cyanure} library~\cite{mairal2019cyanure}.
When comparing models with a similar number of parameters and image/sec, we observe that our features are on par with state-of-the-art semi-supervised models.
Interestingly, this performance is obtained by training a multi-class logistic regression on \emph{frozen features, without data augmentation nor finetuning}.
%When moving to larger models, we also observe that our features obtain competitive performance.

\begin{table}[t]
	\caption{
\textbf{Low-shot learning on ImageNet with frozen ViT features.}
We train a logistic regression on frozen features (\textsc{frozen}).
Note that this \textsc{frozen} evaluation is performed \emph{without any finetuning nor data augmentation}.
We report top-1 accuracy.
For reference, we show previously published results that uses finetuning and semi-supervised learning.}
    \centering
\small
    \begin{tabular}{@{}l l c c c@{}}
	    \toprule
        & && \multicolumn{2}{c}{Top 1}\\
        Method & Arch & Param. &  1\% &  10\% \\
\midrule
\multicolumn{5}{@{}l@{}}{\textit{Self-supervised pretraining with finetuning}}\\
        UDA~\cite{xie2020unsupervised} & RN50 & 23 & -- & 68.1 \\
        SimCLRv2~\cite{chen2020big} & RN50 & 23 & 57.9 & 68.4 \\
        BYOL~\cite{grill2020bootstrap} & RN50 & 23 & 53.2 & 68.8 \\
        SwAV~\cite{caron2020unsupervised} & RN50 & 23 & 53.9 & 70.2 \\
	SimCLRv2~\cite{chen2020exploring} & RN50w4 & 375 & 63.0 & 74.4\\
        BYOL~\cite{grill2020bootstrap} & RN200w2 & 250 & 71.2 & 77.7 \\
\midrule
\multicolumn{5}{@{}l@{}}{\textit{Semi-supervised methods}}\\
        SimCLRv2+KD~\cite{chen2020big} & RN50 & 23 & 60.0 & 70.5 \\
        SwAV+CT~\cite{assran2020recovering} & RN50 & 23 & -- & 70.8 \\
        FixMatch~\cite{sohn2020fixmatch} & RN50 & 23 & -- & 71.5 \\
        MPL~\cite{pham2020meta} & RN50 & 23 & -- & 73.9 \\
	SimCLRv2+KD~\cite{chen2020big} & RN152w3+SK & 794 & 76.6 & 80.9 \\
	\midrule
\multicolumn{5}{@{}l@{}}{\textit{Frozen self-supervised features}}\\
        \rowcolor{Light}
        \OURS\textsc{-frozen} & ViT-S/16 & 21 & 64.5 & 72.2 \\
	\bottomrule
    \end{tabular}
    \label{tab:semisup}
\end{table}

\begin{figure*}[t]
\centering
  \setlength{\tabcolsep}{3pt}
\begin{tabular}{ccc}
\includegraphics[width=0.35\linewidth]{678.pdf}&
%\includegraphics[width=0.4\linewidth]{4143.pdf}\\
\includegraphics[width=0.35\linewidth]{4833.pdf}&
\includegraphics[width=0.35\linewidth]{3296.pdf}\\
\end{tabular}
\caption{
	\textbf{Self-attention for a set of reference points.}
We visualize the self-attention module from the last block of a ViT-S/8 trained with \OURS.
The network is able to separate objects, though it has been trained with no supervision at all.
}
\label{fig:pointing}
\end{figure*}

\subsection{Methodology Comparison}
\label{ap:comp}
We compare the performance of different self-supervised frameworks, MoCo-v2~\cite{chen2020improved}, SwAV~\cite{caron2020unsupervised} and BYOL~\cite{grill2020bootstrap} when using convnet or ViT.
In Tab.~\ref{tab:vit_cnn}, we see that when trained with ResNet-50 (convnet), \OURS performs on par with SwAV and BYOL.
However, \OURS unravels its potential with ViT, outperforming MoCo-v2, SwAV and BYOL by large margins (+4.3\% with linear and +6.2\% with k-NN evaluations).
In the rest of this section, we perform ablations to better understand the performance of \OURS applied to ViT.
In particular, we provide a detailed comparison with methods that either use a momentum encoder, namely MoCo-v2 and BYOL, and methods that use multi-crop, namely SwAV.
\begin{table}[t]
\centering
  \caption{
	  \textbf{Methodology comparison for DEIT-small and ResNet-50.}
    We report ImageNet linear and $k$-NN evaluations validation accuracy after 300 epochs pre-training.
    All numbers are run by us and match or outperform published results.
  }
  \begin{tabular}{@{}ll cc c cc@{}}
    \toprule
  && \multicolumn{2}{c}{ResNet-50} && \multicolumn{2}{c}{ViT-small} \\
    \cmidrule{3-4}
    \cmidrule{6-7}
	  Method && Linear & $k$-NN && Linear & $k$-NN \\
    \midrule
	  MoCo-v2 && 71.1 & 62.9 && 71.6 & 62.0 \\
	  BYOL && 72.7 & 65.4 && 71.4 & 66.6 \\
	  SwAV && 74.1 & 65.4 && 71.8 & 64.7 \\
    \midrule
	  \OURS && \bf 74.5 & \bf 65.6  && \bf 76.1 & \bf 72.8 \\
    \bottomrule
  \end{tabular}
  \label{tab:vit_cnn}
\end{table}

\paragraph{Relation to MoCo-v2 and BYOL.}
In Tab.~\ref{tab:byol}, we present the impact of ablating components that differ between \OURS, MoCo-v2 and BYOL: the choice of loss, the predictor in the student head, the centering operation, the batch normalization in the projection heads, and finally, the multi-crop augmentation.
%BYOL shares many components with MoCo-v2, and hence we compare them jointly.
The loss in \OURS is a cross-entropy on sharpened softmax outputs (\texttt{CE}) while MoCo-v2 uses the InfoNCE contrastive loss (\texttt{INCE}) and BYOL a mean squared error on l2-normalized outputs (\texttt{MSE}).
No sharpening is applied with the \texttt{MSE} criterion.
%When using \texttt{MSE}, no sharpening is applied since we do not transform the output into probability distributions.
Though, \OURS surprisingly still works when changing the loss function to \texttt{MSE}, but this significantly alters the performance (see rows (\rownumber{1}, \rownumber{2}) and (\rownumber{4}, \rownumber{9})).
We also observe that adding a predictor has little impact (\rownumber{1}, \rownumber{3}).
However, in the case of BYOL, the predictor is critical to prevent collapse (\rownumber{7}, \rownumber{8}) which is consistent with previous studies~\cite{chen2020exploring,grill2020bootstrap}.
Interestingly, we observe that the teacher output centering avoids collapse without predictor nor batch normalizations in BYOL (\rownumber{7}, \rownumber{9}), though with a significant performance drop which can likely be explained by the fact that our centering operator is designed to work in combination with sharpening.
Finally, we observe that multi-crop works particularly well with \OURS and MoCo-v2, removing it hurts performance by $2-4\%$ (\rownumber{1} versus \rownumber{4} and, \rownumber{5} versus \rownumber{6}).
Adding multi-crop to BYOL does not work out-of-the-box (\rownumber{7}, \rownumber{10}) as detailed in Appendix~\ref{ap:mc} and further adaptation may be required.
%Our main conclusions are that MoCo-v2 works very well with multi-crop on ViT while any change made to BYOL results in a drop of performance.

\begin{table}[t]
  \centering
\small
  \setlength{\tabcolsep}{4.3pt}
	\caption{
    \textbf{Relation to MoCo-v2 and BYOL.}
	We ablate the components that differ between \OURS, MoCo-v2 and BYOL: the loss function (cross-entropy, \texttt{CE}, versus InfoNCE, \texttt{INCE}, versus mean-square error, \texttt{MSE}), the multi-crop training, the centering operator, the batch normalization in the projection heads and the student predictor. Models are run for 300 epochs with ViT-S/16. We report top-1 accuracy on ImageNet linear evaluation.
  }
  \begin{tabular}{@{}llcccccc@{}}
    \toprule
	  & Method & Loss & multi-crop & Center. & BN & Pred. & Top-1 \\
    \midrule
\rownumber{1}&	  \OURS & \texttt{CE} & \checkmark & \checkmark & & & 76.1 \\
\rownumber{2}&  -- & \texttt{MSE} & \checkmark & \checkmark & & & 62.4 \\
\rownumber{3}&  -- & \texttt{CE} & \checkmark & \checkmark & & \checkmark & 75.6 \\
\rownumber{4}&  -- & \texttt{CE} & & \checkmark & & & 72.5 \\
\arrayrulecolor{black!20}\midrule
\rownumber{5}&  MoCov2 & \texttt{INCE} & & & \checkmark &  & 71.4 \\
\rownumber{6}&   & \texttt{INCE} & \checkmark & & \checkmark &  & 73.4 \\
\arrayrulecolor{black!20}\midrule
\rownumber{7}&  BYOL & \texttt{MSE} & & & \checkmark & \checkmark & 71.4 \\
\rownumber{8}&  -- & \texttt{MSE} & & & \checkmark &  & \pzo0.1 \\
\rownumber{9}&  -- & \texttt{MSE} & & \checkmark &  &  & 52.6 \\
\rownumber{10}&  -- & \texttt{MSE} & \checkmark  & & \checkmark & \checkmark & 64.8 \\
   \arrayrulecolor{black} \bottomrule
  \end{tabular}
\label{tab:byol}
\end{table}


\begin{table}[t]
  \centering
  %\setlength{\tabcolsep}{2.5pt}
	\caption{\textbf{Relation to SwAV.}
We vary the operation on the teacher output between centering, a softmax applied over the batch dimension and the Sinkhorn-Knopp algorithm. 
We also ablate the Momentum encoder by replacing it with a hard copy of the student with a stop-gradient as in SwAV.
Models are run for 300 epochs with ViT-S/16. We report top-1 accuracy on ImageNet linear evaluation.
	}
  \begin{tabular}{@{}llccc@{}}
    \toprule
	 & Method & Momentum & Operation & Top-1 \\
    \midrule
\rownumber{1}&  \OURS & \checkmark & \texttt{Centering} & 76.1 \\
\rownumber{2}&  -- & \checkmark & \texttt{Softmax(batch)} & 75.8 \\
\rownumber{3}&  -- & \checkmark & \texttt{Sinkhorn-Knopp} & 76.0 \\
\rownumber{4}&  -- & & \texttt{Centering} & \pzo0.1 \\
\rownumber{5}&  -- & & \texttt{Softmax(batch)} & 72.2 \\
\rownumber{6}&  SwAV & & \texttt{Sinkhorn-Knopp} & 71.8 \\
    \bottomrule
  \end{tabular}
  \label{tab:swav}
\end{table}

\paragraph{Relation to SwAV.}
In Tab.~\ref{tab:swav}, we evaluate the differences between \OURS and SwAV: the presence of the momentum encoder and the operation on top of the teacher output.
In absence of the momentum, a copy of the student with a stop-gradient is used.
We consider three operations on the teacher output: \texttt{Centering}, \texttt{Sinkhorn-Knopp} or a \texttt{Softmax} along the batch axis.
The \texttt{Softmax} is similar to a single Sinkhorn-Knopp iteration as detailed in the next paragraph.
First, these ablations show that using a momentum encoder significantly improves the performance for ViT (\rownumber{3} versus \rownumber{6}, and \rownumber{2} versus \rownumber{5}).
Second, the momentum encoder also avoids collapse when using only centering (row \rownumber{1}).
In the absence of momentum, centering the outputs does not work (\rownumber{4}) and more advanced operations are required (\rownumber{5}, \rownumber{6}).
Overall, these ablations highlight the importance of the momentum encoder, not only for performance but also to stabilize training, removing the need for normalization beyond centering.

\paragraph{Details on the \texttt{Softmax(batch)} variant.}
The iterative Sinkhorn-Knopp algorithm~\cite{cuturi2013sinkhorn} used in SwAV~\cite{caron2020unsupervised} is implemented simply with the following PyTorch style code.
\begin{python}
# x is n-by-K
# tau is Sinkhorn regularization param
x = exp(x / tau)
for _ in range(num_iters): # 1 iter of Sinkhorn
	# total weight per dimension (or cluster)
	c = sum(x, dim=0, keepdim=True) 
	x /= c

	# total weight per sample
	n = sum(x, dim=1, keepdim=True) 
	# x sums to 1 for each sample (assignment)
	x /= n 
\end{python}
When performing a single Sinkhorn iteration (\texttt{num\_iters=1}) the implementation can be highly simplified into only two lines of code, which is our \texttt{softmax(batch)} variant:
\begin{python}
x = softmax(x / tau, dim=0)
x /= sum(x, dim=1, keepdim=True) 
\end{python}
We have seen in Tab.~\ref{tab:swav} that this highly simplified variant of SwAV works competitively with SwAV.
Intuitively, the \texttt{softmax} operation on the batch axis allows to select for each dimension (or ``cluster'') its best matches in the batch.


\paragraph{Validating our implementation.}
We observe in Tab.~\ref{tab:vit_cnn} that our reproduction of BYOL, MoCo-v2, SwAV matches or outperforms the corresponding published numbers with ResNet-50.
Indeed, we obtain $72.7\%$ for BYOL while \cite{grill2020bootstrap} report $72.5\%$ in this $300$-epochs setting.
We obtain $71.1\%$ for MoCo after $300$ epochs of training while \cite{chen2020improved} report $71.1\%$ after $800$ epochs of training.
Our improvement compared to the implementation of \cite{chen2020improved} can be explained by the use of a larger projection head (3-layer, use of batch-normalizations and projection dimension of $256$).
%We reproduce the published number for SwAV after $400$-epochs training: $74.6\%$.
%We obtain $74.1\%$ for SwAV after $300$ epochs, which is a setting not reported in \cite{caron2020unsupervised}.

\paragraph{Relation to other works.}
\OURS is also related to UIC~\cite{chen2020unsupervised} that use outputs from the previous epoch as hard pseudo-labels for ``unsupervised classification''.
However, we use centering to prevent collapse while UIC resorts to balance sampling techniques as in~\cite{caron2018deep}.
Our work can be interpreted as a soft UIC variant with momentum teacher.

The concurrent work CsMI~\cite{xu2021seed} also exhibits strong performance with simple k-NN classifiers on ImageNet, even with convnets.
As \OURS, CsMI combines a momentum network and multi-crop training, which we have seen are both crucial for good k-NN performance in our experiments with ViTs.
%Interestingly, unlike our work, CsMI reports excellent $k$-NN performance even with a ResNet-50 (72.4\%).
We believe studying this work would help us identifying more precisely the components important for good $k$-NN performance and leave this investigation for future work.

\subsection{Projection Head}
\label{ap:projhead}
Similarly to other self-supervised frameworks, using a projection head~\cite{chen2020simple} improves greatly the accuracy of our method.
The projection head starts with a $n$-layer multi-layer perceptron (MLP).
The hidden layers are 2048d and are with gaussian error linear units (GELU) activations.
The last layer of the MLP is without GELU.
Then we apply a $\ell_2$ normalization and a weight normalized fully connected layer~\cite{chen2020exploring,salimans2016weight} with $K$ dimensions.
This design is inspired from the projection head with a ``prototype layer'' used in SwAV~\cite{caron2020unsupervised}.
We do not apply batch normalizations.
%In the following, we study the effect of the l2-normalization bottleneck, the number of linear layers in the projection head, the output dimension $K$, the choice of activation unit and the impact of adding batch normalizations.

\paragraph{BN-free system.}
Unlike standard convnets, ViT architectures do not use batch normalizations (BN) by default.
\begin{table}[h!]
\vspace{-0.8em}
  \centering
  \begin{tabular}{@{}l c c@{}}
	  ViT-S, 100 epochs & heads w/o BN & heads w/ BN \\
    \midrule
	  $k$-NN top-1 & \colorbox{Light}{69.7} & 68.6 \\
  \end{tabular}
\vspace{-0.8em}
\end{table}
Therefore, when applying \OURS to ViT we do not use any BN also in the projection heads.
In this table we evaluate the impact of adding BN in the heads.
We observe that adding BN in the projection heads has little impact, showing that BN is not important in our framework.
\emph{Overall, when applying \OURS to ViT, we do not use any BN anywhere, making the system entirely BN-free.}
This is a great advantage of \OURS + ViT to work at state-of-the-art performance without requiring any BN.
Indeed, training with BN typically slows down trainings considerably, especially when these BN modules need to be synchronized across processes~\cite{he2020momentum,caron2020unsupervised,caron2019unsupervised,grill2020bootstrap}.
\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{head_design.pdf}
	\caption{
    \textbf{Projection head design w/ or w/o l2-norm bottleneck.}
  }
\label{fig:proj}
\end{figure}

\paragraph{L2-normalization bottleneck in projection head.}
We illustrate the design of the projection head with or without l2-normalization bottleneck in Fig.~\ref{fig:proj}.
\begin{table}[h!]
\vspace{-0.8em}
  \centering
  \begin{tabular}{@{}l c c c c@{}}
	  \# proj. head linear layers & $1$ & $2$ & $3$ & $4$\\
    \midrule
	  w/ l2-norm bottleneck & -- & 62.2 & 68.0 & \colorbox{Light}{69.3} \\
	  w/o l2-norm bottleneck & 61.6 & 62.9 & 0.1 & 0.1 \\
  \end{tabular}
\vspace{-0.8em}
\end{table}
We evaluate the accuracy of \OURS models trained with or without l2-normalization bottleneck and we vary the number of linear layers in the projection head.
With l2 bottleneck, the total number of linear layers is $n + 1$ ($n$ from the MLP and $1$ from the weight normalized layer) while without bottleneck the total number of linear layers is $n$ in the head.
In this table, we report ImageNet top-1 $k$-NN evaluation accuracy after 100 epochs pre-training with ViT-S/16.
The output dimensionality $K$ is set to $4096$ in this experiment.
We observe that \OURS training fails without the l2-normalization bottleneck when increasing the depth of the projection head.
L2-normalization bottleneck stabilizes the training of \OURS with deep projection head.
We observe that increasing the depth of the projection head improves accuracy.
Our default is to use a total of 4 linear layers: 3 are in the MLP and one is after the l2 bottleneck.

\paragraph{Output dimension.}
In this table, we evaluate the effect of varying the output dimensionality $K$.
\begin{table}[h!]
\vspace{-0.8em}
  \centering
  \begin{tabular}{@{}l c c c c c@{}}
	  $K$ & 1024 & 4096 & 16384 & 65536 & 262144 \\
    \midrule
	  $k$-NN top-1 & 67.8 & 69.3 & 69.2 & \colorbox{Light}{69.7} & 69.1 \\
  \end{tabular}
\vspace{-0.8em}
\end{table}
We observe that a large output dimensionality improves the performance.
We note that the use of l2-normalization bottleneck permits to use a large output dimension with a moderate increase in the total number of parameters.
Our default is to use $K$ equals to 65536 and $d=256$ for the bottleneck.

\paragraph{GELU activations.}
By default, the activations used in ViT are gaussian error linear units (GELU).
\begin{table}[h!]
\vspace{-0.8em}
  \centering
  \begin{tabular}{@{}l c c@{}}
	  ViT-S, 100 epochs & heads w/ GELU & heads w/ ReLU \\
    \midrule
	  $k$-NN top-1 & \colorbox{Light}{69.7} & 68.9 \\
  \end{tabular}
\vspace{-0.8em}
\end{table}
Therefore, for consistency within the architecture, we choose to use GELU also in the projection head.
We evaluate the effect of using ReLU instead of GELU in this table and observe that changing the activation unit to ReLU has relatively little impact.
%It is possible that re-tuning the hyperparameters would allow to recover the $0.8\%$ performance gap between our default and the heads with ReLU.


\subsection{Additional Ablations}
\label{ap:ablations}
We have detailed in the main paper that the combination of centering and sharpening is important to avoid collapse in \OURS.
We ablate the hyperparameters for these two operations in the following.
We also study the impact of training length and some design choices for the ViT networks.

\paragraph{Online centering.}
We study the impact of the smoothing parameters in the update rule for the center $c$ used in the output of the teacher network.
\begin{table}[h!]
\vspace{-0.8em}
  \centering
  \begin{tabular}{@{}l c c c c@{}}
	  $m$ & 0 & 0.9 & 0.99 & 0.999 \\
    \midrule
	  $k$-NN top-1 & 69.1 & \colorbox{Light}{69.7} & 69.4 & 0.1 \\
  \end{tabular}
\vspace{-0.8em}
\end{table}
The convergence is robust to a wide range of smoothing, and the model only collapses when the update is too slow, i.e., $m = 0.999$.

\paragraph{Sharpening.}
We enforce sharp targets by tuning the teacher softmax temperature parameter $\tau_t$.
In this table, we observe that a temperature lower than $0.06$ is required to avoid collapse.
\begin{table}[h!]
\vspace{-0.8em}
  \centering
  \setlength{\tabcolsep}{4pt}
  \begin{tabular}{@{}l c c c c c c@{}}
	  $\tau_t$ & $0$ & $0.02$ & $0.04$ & $0.06$ & $0.08$ & $0.04 \rightarrow 0.07$ \\
    \midrule
	  $k$-NN top-1 & 43.9 & 66.7 & 69.6 & 68.7 & 0.1 & \colorbox{Light}{69.7} \\
  \end{tabular}
\vspace{-0.8em}
\end{table}
When the temperature is higher than $0.06$, the training loss consistently converges to $ln(K)$.
However, we have observed that using higher temperature than $0.06$ does not collapse if we start the training from a smaller value and increase it during the first epochs.
%\OURS is particularly sensitive to collapse at the beginning of training.
%Our experiments suggest that we should seek for the maximum temperature that does not collapse.
In practice, we use a linear warm-up for $\tau_t$ from $0.04$ to $0.07$ during the first $30$ epochs of training.
Finally, note that $\tau \rightarrow 0$ (extreme sharpening) correspond to the \texttt{argmax} operation and leads to one-hot hard distributions.

\paragraph{Longer training.}
We observe in this table that longer training improves the performance of \OURS applied to ViT-Small.
\vspace{-0.8em}
\begin{table}[h!]
  \centering
\begin{tabular}{@{}l c c c@{}}
	\OURS ViT-S & 100-ep & 300-ep & 800-ep \\
\midrule
	$k$-NN top-1 & 70.9 & 72.8 & \colorbox{Light}{74.5} \\
\end{tabular}
\vspace{-0.8em}
\end{table}
This observation is consistent with self-supervised results obtained with convolutional architectures~\cite{chen2020simple}.
We note that in our experiments with BYOL on ViT-S, training longer than $300$ epochs has been leading to worse performance compare our $300$ epochs run.
For this reason we report BYOL for 300 epochs in Tab.~\ref{tab:sota} while SwAV, MoCo-v2 and DINO are trained for 800 epochs.
%Results in Tab. 1 of the main paper are with $800$ epochs training for ViT-S but $300$ epochs only for the other considered models (ViT-S/8, ViT-B/16, ViT-B/8) to save computational budget.
%We have not explored longer trainings for these models yet.

\paragraph{The teacher outperforms the student.}
We have shown in Fig.~\ref{fig:mom} that the momentum teacher outperforms the student with ViT and we show in this Figure that it is also the case with ResNet-50.
\begin{figure}[h!]
\vspace{-0.8em}
\centering
\includegraphics[width=0.48\linewidth]{figure_mom_rn50.pdf}
\vspace{-0.8em}
\end{figure}
The fact that the teacher continually outperforms the student further encourages the interpretation of \OURS as a form of Mean Teacher~\cite{tarvainen2017mean} self-distillation.
Indeed, as motivated in Tarvainen et al.~\cite{tarvainen2017mean}, weight averaging usually produces a better model than the individual models from each iteration~\cite{polyak1992acceleration}.
By aiming a target obtained with a teacher better than the student, the student's representations improve.
Consequently, the teacher also improves since it is built directly from the student weights.

\paragraph{Self-attention maps from supervised versus self-supervised learning.}
We evaluate the masks obtained by thresholding the self-attention maps to keep 80\% of the mass.
\begin{table}[h!]
\vspace{-0.8em}
  \centering
  \begin{tabular}{@{}l c@{}}
	  \toprule
	  ViT-S/16 weights & \\
	  \midrule
	  Random weights & 22.0 \\
	  Supervised & 27.3 \\
	  \midrule
	  \OURS & 45.9 \\
	  \OURS w/o multicrop & 45.1 \\
	  MoCo-v2 & 46.3 \\
	  BYOL & 47.8 \\
	  SwAV & 46.8 \\
	\bottomrule
  \end{tabular}
\vspace{-0.8em}
\end{table}
We compare the Jaccard similarity between the ground truth and these masks on the validation images of PASCAL VOC12 dataset for different ViT-S trained with different frameworks.
The properties that self-attention maps from ViT explicitly contain the scene layout and, in particular, object boundaries is observed across different self-supervised methods.

\paragraph{Impact of the number of heads in ViT-S.}
We study the impact of the number of heads in ViT-S on the accuracy and throughput (images processed per second at inference time on a singe V100 GPU).
\begin{table}[h!]
\vspace{-0.8em}
  \centering
  \begin{tabular}{@{}l c c c c c@{}}
	  \# heads & dim & dim/head & \# params & im/sec & $k$-NN \\
    \midrule
    \rowcolor{Light}
	  6 & 384 & 64 & 21 & 1007 & 72.8 \\
	  8 & 384 & 48 & 21 & 971 & 73.1 \\
	  12 & 384 & 32 & 21 & 927 & 73.7 \\
	  16 & 384 & 24 & 21 & 860 & 73.8 \\
  \end{tabular}
\vspace{-0.8em}
\end{table}
We find that increasing the number of heads improves the performance, at the cost of a slighlty worse throughput.
In our paper, all experiments are run with the default model DeiT-S~\cite{touvron2020training}, i.e. with $6$ heads only.

\subsection{Multi-crop}
\label{ap:mc}
In this Appendix, we study a core component of \OURS: multi-crop training~\cite{caron2020unsupervised}.


\paragraph{Range of scales in multi-crop.}
For generating the different views, we use the \texttt{RandomResizedCrop} method from \texttt{torchvision.transforms} module in PyTorch.
\begin{table}[h!]
\vspace{-0.8em}
\centering
  \begin{tabular}{@{}l c c c c c@{}}
	  (0.05, $s$), ($s$, 1), $s$: & 0.08 & 0.16 & 0.24 & 0.32 & 0.48 \\
    \midrule
	  $k$-NN top-1 & 65.6 & 68.0 & 69.7 & 69.8 & 69.5 \\
  \end{tabular}
\vspace{-0.8em}
\end{table}
We sample two global views with scale range $(s, 1)$ before resizing them to $224^2$ and $6$ local views with scale sampled in the range $(0.05, s)$ resized to $96^2$ pixels.
Note that we arbitrarily choose to have non-overlapping scaling range for the global and local views following the original design of SwAV.
However, the ranges could definitely be overlapping and experimenting with finer hyperparameters search could lead to a more optimal setting.
In this table, we vary the parameter $s$ that controls the range of scales used in multi-crop and find the optimum to be around $0.3$ in our experiments.
We note that this is higher than the parameter used in SwAV which is of $0.14$.

\paragraph{Multi-crop in different self-supervised frameworks.}
We compare different recent self-supervised learning frameworks, namely MoCo-v2~\cite{chen2020improved}, BYOL~\cite{grill2020bootstrap} and SwAV~\cite{caron2020unsupervised} with ViT-S/16 architecture.
\begin{table}[h!]
\vspace{-0.8em}
  \centering
\begin{tabular}{@{}l cc c cc@{}}
\toprule
crops	& \multicolumn{2}{c}{$2 \times 224^2$} && \multicolumn{2}{c}{$2 \times 224^2 + 6 \times 96^2$} \\
    \cmidrule{2-3}
    \cmidrule{5-6}
	eval & $k$-NN & linear && $k$-NN & linear  \\
\midrule
	BYOL & 66.6 & 71.4 && 59.8 & 64.8 \\
	SwAV & 60.5 & 68.5 && 64.7 & 71.8 \\
	MoCo-v2 & 62.0 & 71.6 && 65.4 & 73.4 \\
        \rowcolor{Light}
	\OURS & \bf 67.9 & \bf 72.5 && \bf 72.7 & \bf 75.9 \\
\bottomrule
\vspace{-0.8em}
\end{tabular}
\end{table}
For fair comparisons, all models are pretrained either with two $224^2$ crops or with multi-crop~\cite{caron2020unsupervised} training, i.e. two $224^2$ crops and six $96^2$ crops for each image.
We report $k$-NN and linear probing evaluations after 300 epochs of training.
Multi-crop does not benefit all frameworks equally, which has been ignored in benchmarks considering only the two crops setting~\cite{chen2020exploring}.
The effectiveness of multi-crop depends on the considered framework, which positions multi-crop as a core component of a model and not a simple ``add-ons'' that will boost any framework the same way.
Without multi-crop, \OURS has better accuracy than other frameworks, though by a moderate margin (1\%).
Remarkably, \OURS benefits the most from multi-crop training ($+3.4\%$ in linear eval).
Interestingly, we also observe that the ranking of the frameworks depends on the evaluation protocol considered.

\paragraph{Training BYOL with multi-crop.}
When applying multi-crop to BYOL with ViT-S, we observe the transfer performance is higher than the baseline without multi-crop for the first training epochs.
\begin{figure}[h]
\vspace{-0.8em}
\centering
\includegraphics[width=0.5\linewidth]{figure_byol.pdf}
\vspace{-0.8em}
\end{figure}
However, the transfer performance growth rate is slowing down and declines after a certain amount of training.
We have performed learning rate, weight decay, multi-crop parameters sweeps for this setting and systematically observe the same pattern.
More precisely, we experiment with \{$1e^{-5}$, $3e^{-5}$, $1e^{-4}$, $3e^{-4}$, $1e^{-3}$, $3e^{-3}$\} for learning rate base values, with \{$0.02$, $0.05$, $0.1$\} for weight decay and with different number of small crops: \{2, 4, 6\}.
All our runs are performed with synchronized batch normalizations in the heads.
When using a low learning rate, we did not observe the performance break point, i.e. the transfer performance was improving continually during training, but the overall accuracy was low.
We have tried a run with multi-crop training on ResNet-50 where we also observe the same behavior.
Since integrating multi-crop training to BYOL is not the focus of this study we did not push that direction further.
However, we believe this is worth investigating why multi-crop does not combine well with BYOL in our experiments and leave this for future work.

\subsection{Evaluation Protocols}
\subsubsection{$k$-NN classification}
Following the setting of Wu~\etal~\cite{wu2018unsupervised}, we evaluate the quality of features with a simple weighted $k$ Nearest Neighbor classifier.
We freeze the pretrained model to compute and store the features of the training data of the downstream task.
To classify a test image $x$, we compute its representation and compare it against all stored training features $T$.
The representation of an image is given by the output \texttt{[CLS]} token: it has dimensionality $d=384$ for ViT-S and $d=768$ for ViT-B.
The top $k$ NN (denoted $\mathcal{N}_k$) are used to make a prediction via weighted voting.
Specifically, the class $c$ gets a total weight of $\sum_{i \in \mathcal{N}_k} \alpha_i \mathbf{1}_{c_i = c}$, where $\alpha_i$ is a contribution weight.
We use $\alpha_i = \exp(T_i x / \tau)$ with $\tau$ equals to $0.07$ as in~\cite{wu2018unsupervised} which we do not tune.
We evaluate different values for $k$ and find that $k=20$ is consistently leading to the best accuracy across our runs.
This evaluation protocol does not require hyperparameter tuning, nor data augmentation and can be run with only one pass over the downstream dataset.

\subsubsection{Linear classification}
Following common practice in self-supervised learning, we evaluate the representation quality with a linear classifier.
The projection head is removed, and we train a supervised linear classifier on top of frozen features.
This linear classifier is trained with SGD and a batch size of $1024$ during $100$ epochs on ImageNet.
We do not apply weight decay.
For each model, we sweep the learning rate value.
During training, we apply only random resizes crops (with default parameters from PyTorch \texttt{RandomResizedCrop}) and horizontal flips as data augmentation.
We report central-crop top-1 accuracy.
When evaluating convnets, the common practice is to perform global average pooling on the final feature map before the linear classifier.
In the following, we describe how we adapt this design when evaluating ViTs.

\paragraph{ViT-S representations for linear eval.}
Following the \emph{feature-based} evaluations in BERT~\cite{devlin2018bert}, we concatenate the \texttt{[CLS]} tokens from the $l$ last layers.
\begin{table}[h!]
\vspace{-0.8em}
\small
  \centering
  \begin{tabular}{@{}l c c c c@{}}
	  concatenate $l$ last layers & $1$ & $2$ & $4$ & $6$\\
    \midrule
	  representation dim & 384 & 768 & 1536 & 2304 \\
	  ViT-S/16 linear eval & 76.1 & 76.6 & \colorbox{Light}{77.0} & 77.0 \\
  \end{tabular}
\vspace{-0.8em}
\end{table}
We experiment with the concatenation of a different number $l$ of layers and similarly to~\cite{devlin2018bert} we find $l=4$ to be optimal.
%We use the same setting when evaluating DeiT-S/8.

\paragraph{ViT-B representations for linear eval.}
With ViT-B we did not find that concatenating the representations from the last $l$ layers to provide any performance gain, and consider the final layer only ($l=1$).
\begin{table}[h!]
\small
\vspace{-0.8em}
  \centering
  \setlength{\tabcolsep}{4pt}
  \begin{tabular}{@{}l c c@{}}
	  pooling strategy & \texttt{[CLS]} tok. & concatenate \texttt{[CLS]} tok.\\
	  & only &  and avgpooled patch tok. \\
    \midrule
	  representation dim & 768 & 1536 \\
	  ViT-B/16 linear eval & 78.0 & \colorbox{Light}{78.2} \\
  \end{tabular}
\vspace{-0.8em}
\end{table}
In this setting, we adapt the pipeline used in convnets with global average pooling on the output patch tokens.
We concatenate these pooled features to the final \texttt{[CLS]} output token.
%We use the same setting when evaluating ViT-B/8.

\subsection{Self-Attention Visualizations}
\label{ap:visu}
We provide more self-attention visualizations in Fig.~\ref{fig:pointing} and in Fig.~\ref{fig:all}.
The images are randomly selected from COCO validation set, and are not used during training of \OURS.
In Fig.~\ref{fig:pointing}, we show the self-attention from the last layer of a \OURS ViT-S/8 for several reference points.

\subsection{Class Representation}
As a final visualization, we propose to look at the distribution of ImageNet concepts in the feature space from \OURS.
We represent each ImageNet class with the average feature vector for its validation images.
We reduce the dimension of these features to 30 with PCA, and run t-SNE with a perplexity of 20, a learning rate of 200 for 5000 iterations.
We present the resulting class embeddings in Fig.~\ref{fig:tsne}.
Our model recovers structures between classes: similar animal species are grouped together, forming coherent clusters of birds (top) or dogs, and especially terriers (far right).

\begin{figure*}
\centering
\setlength{\tabcolsep}{0.5pt}
\begin{tabular}{c cc ccc cc ccc cc c cc ccc cc ccc}
	&&& \multicolumn{3}{c}{\OURS} &&& \multicolumn{3}{c}{Supervised} &&& &&& \multicolumn{3}{c}{\OURS} &&& \multicolumn{3}{c}{Supervised} \\
    \cmidrule{4-6}
    \cmidrule{9-11}
    \cmidrule{17-19}
    \cmidrule{22-24}
\includegraphics[width=0.07\linewidth]{3478.png} &&&
\includegraphics[width=0.07\linewidth]{3478dino-depth12-head2.png} &
\includegraphics[width=0.07\linewidth]{3478dino-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{3478dino-depth12-head5.png} &&&
\includegraphics[width=0.07\linewidth]{3478sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{3478sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{3478sup-depth12-head2.png}
&&&
\includegraphics[width=0.07\linewidth]{944.png} &&&
\includegraphics[width=0.07\linewidth]{944dino-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{944dino-depth12-head2.png} &
\includegraphics[width=0.07\linewidth]{944dino-depth12-head3.png} &&&
\includegraphics[width=0.07\linewidth]{944sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{944sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{944sup-depth12-head2.png}
\\
\includegraphics[width=0.07\linewidth]{4323.png} &&&
\includegraphics[width=0.07\linewidth]{4323dino-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{4323dino-depth12-head2.png} &
\includegraphics[width=0.07\linewidth]{4323dino-depth12-head5.png} &&&
\includegraphics[width=0.07\linewidth]{4323sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{4323sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{4323sup-depth12-head2.png}
%\\
&&&
\includegraphics[width=0.07\linewidth]{4389.png} &&&
\includegraphics[width=0.07\linewidth]{4389dino-depth12-head2.png} &
\includegraphics[width=0.07\linewidth]{4389dino-depth12-head4.png} &
\includegraphics[width=0.07\linewidth]{4389dino-depth12-head5.png} &&&
\includegraphics[width=0.07\linewidth]{4389sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{4389sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{4389sup-depth12-head2.png}
\\
\includegraphics[width=0.07\linewidth]{102.png} &&&
\includegraphics[width=0.07\linewidth]{102dino-depth12-head2.png} &
\includegraphics[width=0.07\linewidth]{102dino-depth12-head4.png} &
\includegraphics[width=0.07\linewidth]{102dino-depth12-head5.png} &&&
\includegraphics[width=0.07\linewidth]{102sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{102sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{102sup-depth12-head2.png}
%\\
&&&
\includegraphics[width=0.07\linewidth]{4870.png} &&&
\includegraphics[width=0.07\linewidth]{4870dino-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{4870dino-depth12-head3.png} &
\includegraphics[width=0.07\linewidth]{4870dino-depth12-head2.png} &&&
\includegraphics[width=0.07\linewidth]{4870sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{4870sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{4870sup-depth12-head2.png}
\\
\includegraphics[width=0.07\linewidth]{135.png} &&&
\includegraphics[width=0.07\linewidth]{135dino-depth12-head3.png} &
\includegraphics[width=0.07\linewidth]{135dino-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{135dino-depth12-head4.png} &&&
\includegraphics[width=0.07\linewidth]{135sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{135sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{135sup-depth12-head2.png}
%\\
&&&
\includegraphics[width=0.07\linewidth]{2559.png} &&&
\includegraphics[width=0.07\linewidth]{2559dino-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{2559dino-depth12-head2.png} &
\includegraphics[width=0.07\linewidth]{2559dino-depth12-head1.png} &&&
\includegraphics[width=0.07\linewidth]{2559sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{2559sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{2559sup-depth12-head2.png}
\\
\includegraphics[width=0.07\linewidth]{4466.png} &&&
\includegraphics[width=0.07\linewidth]{4466dino-depth12-head2.png} &
\includegraphics[width=0.07\linewidth]{4466dino-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{4466dino-depth12-head5.png} &&&
\includegraphics[width=0.07\linewidth]{4466sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{4466sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{4466sup-depth12-head2.png}
%\\
&&&
\includegraphics[width=0.07\linewidth]{3689.png} &&&
\includegraphics[width=0.07\linewidth]{3689dino-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{3689dino-depth12-head4.png} &
\includegraphics[width=0.07\linewidth]{3689dino-depth12-head2.png} &&&
\includegraphics[width=0.07\linewidth]{3689sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{3689sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{3689sup-depth12-head2.png}
\\
\includegraphics[width=0.07\linewidth]{2076.png} &&&
\includegraphics[width=0.07\linewidth]{2076dino-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{2076dino-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{2076dino-depth12-head4.png} &&&
\includegraphics[width=0.07\linewidth]{2076sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{2076sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{2076sup-depth12-head2.png}
%\\
&&&
\includegraphics[width=0.07\linewidth]{1508.png} &&&
\includegraphics[width=0.07\linewidth]{1508dino-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{1508dino-depth12-head4.png} &
\includegraphics[width=0.07\linewidth]{1508dino-depth12-head5.png} &&&
\includegraphics[width=0.07\linewidth]{1508sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{1508sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{1508sup-depth12-head2.png}
\\
\includegraphics[width=0.07\linewidth]{3791.png} &&&
\includegraphics[width=0.07\linewidth]{3791dino-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{3791dino-depth12-head3.png} &
\includegraphics[width=0.07\linewidth]{3791dino-depth12-head2.png} &&&
\includegraphics[width=0.07\linewidth]{3791sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{3791sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{3791sup-depth12-head2.png}
%\\
&&&
\includegraphics[width=0.07\linewidth]{644.png} &&&
\includegraphics[width=0.07\linewidth]{644dino-depth12-head4.png} &
\includegraphics[width=0.07\linewidth]{644dino-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{644dino-depth12-head0.png} &&&
\includegraphics[width=0.07\linewidth]{644sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{644sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{644sup-depth12-head2.png}
\\
\includegraphics[width=0.07\linewidth]{2221.png} &&&
\includegraphics[width=0.07\linewidth]{2221dino-depth12-head5.png} &
\includegraphics[width=0.07\linewidth]{2221dino-depth12-head2.png} &
\includegraphics[width=0.07\linewidth]{2221dino-depth12-head0.png} &&&
\includegraphics[width=0.07\linewidth]{2221sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{2221sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{2221sup-depth12-head2.png}
%\\
&&&
\includegraphics[width=0.07\linewidth]{1323.png} &&&
\includegraphics[width=0.07\linewidth]{1323dino-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{1323dino-depth12-head5.png} &
\includegraphics[width=0.07\linewidth]{1323dino-depth12-head2.png} &&&
\includegraphics[width=0.07\linewidth]{1323sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{1323sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{1323sup-depth12-head2.png}
\\
\includegraphics[width=0.07\linewidth]{1178.png} &&&
\includegraphics[width=0.07\linewidth]{1178dino-depth12-head2.png} &
\includegraphics[width=0.07\linewidth]{1178dino-depth12-head4.png} &
\includegraphics[width=0.07\linewidth]{1178dino-depth12-head3.png} &&&
\includegraphics[width=0.07\linewidth]{1178sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{1178sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{1178sup-depth12-head2.png}
%\\
&&&
\includegraphics[width=0.07\linewidth]{578.png} &&&
\includegraphics[width=0.07\linewidth]{578dino-depth12-head5.png} &
\includegraphics[width=0.07\linewidth]{578dino-depth12-head3.png} &
\includegraphics[width=0.07\linewidth]{578dino-depth12-head1.png} &&&
\includegraphics[width=0.07\linewidth]{578sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{578sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{578sup-depth12-head2.png}
\\
\includegraphics[width=0.07\linewidth]{2281.png} &&&
\includegraphics[width=0.07\linewidth]{2281dino-depth12-head5.png} &
\includegraphics[width=0.07\linewidth]{2281dino-depth12-head4.png} &
\includegraphics[width=0.07\linewidth]{2281dino-depth12-head0.png} &&&
\includegraphics[width=0.07\linewidth]{2281sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{2281sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{2281sup-depth12-head2.png}
%\\
&&&
\includegraphics[width=0.07\linewidth]{2313.png} &&&
\includegraphics[width=0.07\linewidth]{2313dino-depth12-head2.png} &
\includegraphics[width=0.07\linewidth]{2313dino-depth12-head4.png} &
\includegraphics[width=0.07\linewidth]{2313dino-depth12-head0.png} &&&
\includegraphics[width=0.07\linewidth]{2313sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{2313sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{2313sup-depth12-head2.png}
\\
\includegraphics[width=0.07\linewidth]{3295.png} &&&
\includegraphics[width=0.07\linewidth]{3295dino-depth12-head2.png} &
\includegraphics[width=0.07\linewidth]{3295dino-depth12-head4.png} &
\includegraphics[width=0.07\linewidth]{3295dino-depth12-head5.png} &&&
\includegraphics[width=0.07\linewidth]{3295sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{3295sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{3295sup-depth12-head2.png}
%\\
&&&
\includegraphics[width=0.07\linewidth]{3377.png} &&&
\includegraphics[width=0.07\linewidth]{3377dino-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{3377dino-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{3377dino-depth12-head4.png} &&&
\includegraphics[width=0.07\linewidth]{3377sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{3377sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{3377sup-depth12-head2.png}
\\
\includegraphics[width=0.07\linewidth]{4229.png} &&&
\includegraphics[width=0.07\linewidth]{4229dino-depth12-head2.png} &
\includegraphics[width=0.07\linewidth]{4229dino-depth12-head4.png} &
\includegraphics[width=0.07\linewidth]{4229dino-depth12-head0.png} &&&
\includegraphics[width=0.07\linewidth]{4229sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{4229sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{4229sup-depth12-head2.png}
%\\
&&&
\includegraphics[width=0.07\linewidth]{581.png} &&&
\includegraphics[width=0.07\linewidth]{581dino-depth12-head5.png} &
\includegraphics[width=0.07\linewidth]{581dino-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{581dino-depth12-head3.png} &&&
\includegraphics[width=0.07\linewidth]{581sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{581sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{581sup-depth12-head2.png}
\\
\includegraphics[width=0.07\linewidth]{3059.png} &&&
\includegraphics[width=0.07\linewidth]{3059dino-depth12-head2.png} &
\includegraphics[width=0.07\linewidth]{3059dino-depth12-head4.png} &
\includegraphics[width=0.07\linewidth]{3059dino-depth12-head5.png} &&&
\includegraphics[width=0.07\linewidth]{3059sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{3059sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{3059sup-depth12-head2.png}
%\\
&&&
\includegraphics[width=0.07\linewidth]{4462.png} &&&
\includegraphics[width=0.07\linewidth]{4462dino-depth12-head2.png} &
\includegraphics[width=0.07\linewidth]{4462dino-depth12-head5.png} &
\includegraphics[width=0.07\linewidth]{4462dino-depth12-head3.png} &&&
\includegraphics[width=0.07\linewidth]{4462sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{4462sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{4462sup-depth12-head2.png}
\\
\includegraphics[width=0.07\linewidth]{1825.png} &&&
\includegraphics[width=0.07\linewidth]{1825dino-depth12-head2.png} &
\includegraphics[width=0.07\linewidth]{1825dino-depth12-head4.png} &
\includegraphics[width=0.07\linewidth]{1825dino-depth12-head5.png} &&&
\includegraphics[width=0.07\linewidth]{1825sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{1825sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{1825sup-depth12-head2.png}
%\\
&&&
\includegraphics[width=0.07\linewidth]{1971.png} &&&
\includegraphics[width=0.07\linewidth]{1971dino-depth12-head2.png} &
\includegraphics[width=0.07\linewidth]{1971dino-depth12-head4.png} &
\includegraphics[width=0.07\linewidth]{1971dino-depth12-head1.png} &&&
\includegraphics[width=0.07\linewidth]{1971sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{1971sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{1971sup-depth12-head2.png}
\\
\includegraphics[width=0.07\linewidth]{3224.png} &&&
\includegraphics[width=0.07\linewidth]{3224dino-depth12-head2.png} &
\includegraphics[width=0.07\linewidth]{3224dino-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{3224dino-depth12-head5.png} &&&
\includegraphics[width=0.07\linewidth]{3224sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{3224sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{3224sup-depth12-head2.png}
&&&
\includegraphics[width=0.07\linewidth]{393.png} &&&
\includegraphics[width=0.07\linewidth]{393dino-depth12-head5.png} &
\includegraphics[width=0.07\linewidth]{393dino-depth12-head2.png} &
\includegraphics[width=0.07\linewidth]{393dino-depth12-head3.png} &&&
\includegraphics[width=0.07\linewidth]{393sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{393sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{393sup-depth12-head2.png}
\\
\includegraphics[width=0.07\linewidth]{1525.png} &&&
\includegraphics[width=0.07\linewidth]{1525dino-depth12-head5.png} &
\includegraphics[width=0.07\linewidth]{1525dino-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{1525dino-depth12-head1.png} &&&
\includegraphics[width=0.07\linewidth]{1525sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{1525sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{1525sup-depth12-head2.png}
&&&
\includegraphics[width=0.07\linewidth]{4859.png} &&&
\includegraphics[width=0.07\linewidth]{4859dino-depth12-head3.png} &
\includegraphics[width=0.07\linewidth]{4859dino-depth12-head5.png} &
\includegraphics[width=0.07\linewidth]{4859dino-depth12-head0.png} &&&
\includegraphics[width=0.07\linewidth]{4859sup-depth12-head0.png} &
\includegraphics[width=0.07\linewidth]{4859sup-depth12-head1.png} &
\includegraphics[width=0.07\linewidth]{4859sup-depth12-head2.png}
\end{tabular}
	\caption{\textbf{Self-attention heads from the last layer.} We look at the attention map when using the \texttt{[CLS]} token as a query for the different heads in the last layer. Note that the \texttt{[CLS]} token is not attached to any label or supervision.}
\label{fig:all}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{tsne-classes.pdf}
  \caption{
    t-SNE visualization of ImageNet classes as represented using \OURS.
    For each class, we obtain the embedding by taking the average feature for all images of that class in the validation set.
  }
  \label{fig:tsne}
\end{figure*}
