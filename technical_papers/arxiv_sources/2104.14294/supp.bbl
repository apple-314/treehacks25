\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{assran2020recovering}
Mahmoud Assran, Nicolas Ballas, Lluis Castrejon, and Michael Rabbat.
\newblock Recovering petaflops in contrastive semi-supervised learning of
  visual representations.
\newblock {\em arXiv preprint arXiv:2006.10803}, 2020.

\bibitem{caron2019unsupervised}
Mathilde Caron, Piotr Bojanowski, Julien Mairal, and Armand Joulin.
\newblock Unsupervised pre-training of image features on non-curated data.
\newblock In {\em Proceedings of the International Conference on Computer
  Vision (ICCV)}, 2019.

\bibitem{caron2020unsupervised}
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and
  Armand Joulin.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock In {\em Proceedings of Advances in Neural Information Processing
  Systems (NeurIPS)}, 2020.

\bibitem{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock {\em arXiv preprint arXiv:2002.05709}, 2020.

\bibitem{chen2020big}
Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey
  Hinton.
\newblock Big self-supervised models are strong semi-supervised learners.
\newblock In {\em Proceedings of Advances in Neural Information Processing
  Systems (NeurIPS)}, 2020.

\bibitem{chen2020improved}
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.
\newblock Improved baselines with momentum contrastive learning.
\newblock {\em arXiv preprint arXiv:2003.04297}, 2020.

\bibitem{chen2020exploring}
Xinlei Chen and Kaiming He.
\newblock Exploring simple siamese representation learning.
\newblock {\em arXiv preprint arXiv:2011.10566}, 2020.

\bibitem{cuturi2013sinkhorn}
Marco Cuturi.
\newblock Sinkhorn distances: Lightspeed computation of optimal transport.
\newblock In {\em Proceedings of Advances in Neural Information Processing
  Systems (NeurIPS)}, 2013.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{everingham2010pascal}
Mark Everingham, Luc Van~Gool, Christopher~KI Williams, John Winn, and Andrew
  Zisserman.
\newblock The pascal visual object classes (voc) challenge.
\newblock {\em International Journal of Computer Vision (IJCV)}, 2010.

\bibitem{grill2020bootstrap}
Jean-Bastien Grill, Florian Strub, Florent Altch{\'e}, Corentin Tallec,
  Pierre~H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo~Avila Pires,
  Zhaohan~Daniel Guo, Mohammad~Gheshlaghi Azar, et~al.
\newblock Bootstrap your own latent: A new approach to self-supervised
  learning.
\newblock In {\em Proceedings of Advances in Neural Information Processing
  Systems (NeurIPS)}, 2020.

\bibitem{he2020momentum}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In {\em Proceedings of the Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 2020.

\bibitem{jabri2020space}
Allan Jabri, Andrew Owens, and Alexei~A Efros.
\newblock Space-time correspondence as a contrastive random walk.
\newblock 2020.

\bibitem{lai2020mast}
Zihang Lai, Erika Lu, and Weidi Xie.
\newblock Mast: A memory-augmented self-supervised tracker.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 6479--6488, 2020.

\bibitem{mairal2019cyanure}
Julien Mairal.
\newblock Cyanure: An open-source toolbox for empirical risk minimization for
  python, c++, and soon more.
\newblock {\em arXiv preprint arXiv:1912.08165}, 2019.

\bibitem{nilsback2008automated}
Maria-Elena Nilsback and Andrew Zisserman.
\newblock Automated flower classification over a large number of classes.
\newblock In {\em 2008 Sixth Indian Conference on Computer Vision, Graphics \&
  Image Processing}, 2008.

\bibitem{oh2019video}
Seoung~Wug Oh, Joon-Young Lee, Ning Xu, and Seon~Joo Kim.
\newblock Video object segmentation using space-time memory networks.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 9226--9235, 2019.

\bibitem{pham2020meta}
Hieu Pham, Qizhe Xie, Zihang Dai, and Quoc~V Le.
\newblock Meta pseudo labels.
\newblock {\em arXiv preprint arXiv:2003.10580}, 2020.

\bibitem{polyak1992acceleration}
Boris~T Polyak and Anatoli~B Juditsky.
\newblock Acceleration of stochastic approximation by averaging.
\newblock {\em SIAM journal on control and optimization}, 30(4):838--855, 1992.

\bibitem{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander~C
  Berg, and Li Fei-Fei.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em International Journal of Computer Vision (IJCV)}, 2015.

\bibitem{salimans2016weight}
Tim Salimans and Diederik~P Kingma.
\newblock Weight normalization: A simple reparameterization to accelerate
  training of deep neural networks.
\newblock {\em Proceedings of Advances in Neural Information Processing Systems
  (NeurIPS)}, 2016.

\bibitem{sohn2020fixmatch}
Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini,
  Ekin~D Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel.
\newblock Fixmatch: Simplifying semi-supervised learning with consistency and
  confidence.
\newblock In {\em Proceedings of Advances in Neural Information Processing
  Systems (NeurIPS)}, 2020.

\bibitem{tarvainen2017mean}
Antti Tarvainen and Harri Valpola.
\newblock Mean teachers are better role models: Weight-averaged consistency
  targets improve semi-supervised deep learning results.
\newblock {\em arXiv preprint arXiv:1703.01780}, 2017.

\bibitem{touvron2020training}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
  Sablayrolles, and Herv{\'e} J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock {\em arXiv preprint arXiv:2012.12877}, 2020.

\bibitem{wang2019learning}
Xiaolong Wang, Allan Jabri, and Alexei~A Efros.
\newblock Learning correspondence from the cycle-consistency of time.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 2566--2576, 2019.

\bibitem{wu2018unsupervised}
Zhirong Wu, Yuanjun Xiong, Stella~X Yu, and Dahua Lin.
\newblock Unsupervised feature learning via non-parametric instance
  discrimination.
\newblock In {\em Proceedings of the Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 2018.

\bibitem{xie2020unsupervised}
Qizhe Xie, Zihang~Dai Dai, Eduard Hovy, Minh-Thang Luong, and Quoc~V. Le.
\newblock Unsupervised data augmentation for consistency training.
\newblock {\em arXiv preprint arXiv:1904.12848}, 2020.

\bibitem{zhou2014learning}
Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva.
\newblock Learning deep features for scene recognition using places database.
\newblock In {\em Proceedings of Advances in Neural Information Processing
  Systems (NeurIPS)}, 2014.

\end{thebibliography}
