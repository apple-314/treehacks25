% !TEX root = main.tex

\section{Approach}
\subsection{SSL with Knowledge Distillation}
\label{sec:method}

The framework used for this work, \OURS, shares the same overall structure as recent self-supervised approaches~\cite{caron2020unsupervised,chen2020exploring,chen2020simple,grill2020bootstrap,he2020momentum}.
However, our method shares also similarities with knowledge distillation~\cite{hinton2015distilling} and we present it under this angle.
We illustrate \OURS in Figure~\ref{fig:model} and propose a pseudo-code implementation in Algorithm~\ref{algo:DINO}.

Knowledge distillation is a learning paradigm where we train a student network $g_{\theta_s}$ to match the output of a given teacher network $g_{\theta_t}$, parameterized by $\theta_s$ and $\theta_t$ respectively.
Given an input image $x$, both networks output probability distributions over $K$ dimensions denoted by $P_s$ and $P_t$.
The probability $P$ is obtained by normalizing the output of the network $g$ with a softmax function. More precisely,
\begin{equation}
  P_s(x)^{(i)} = \frac{\exp(g_{\theta_s}(x)^{(i)} / \tau_s)}{\sum_{k=1}^K \exp(g_{\theta_s}(x)^{(k)} / \tau_s)},
\end{equation}                                                                                                                                                                                        
with $\tau_s>0$ a temperature parameter that controls the sharpness of the output distribution, and a similar formula holds for~$P_t$ with temperature $\tau_t$.
Given a fixed teacher network~$g_{\theta_t}$, we learn to match these distributions by minimizing the cross-entropy loss w.r.t. the parameters of the student network $\theta_s$:
\begin{equation}
        \min_{\theta_s} H(P_t(x), P_s(x)),
  \label{eq:kd}                                                                                                                                                                                       
\end{equation}
where $H(a, b) = - a \log b$.

In the following, we detail how we adapt the problem in Eq.~(\ref{eq:kd}) to self-supervised learning.
First, we construct different distorted views, or crops, of an image with multi-crop strategy~\cite{caron2020unsupervised}.
More precisely, from a given image, we generate a set $V$ of different views.
This set contains two \emph{global} views, $x^{g}_1$ and $x^{g}_2$ and several \emph{local} views of smaller resolution.
All crops are passed through the student while only the \emph{global} views are passed through the teacher, therefore encouraging ``local-to-global'' correspondences.
We minimize the loss:
\begin{equation}
	\min_{\theta_s} \sum_{x \in \{x^{g}_1, x^{g}_2\}} \quad \sum_{\substack{x' \in V\\x'\neq \, x}} \quad H(P_t(x), P_s(x')).
\label{eq:loss}
\end{equation}


\begin{algorithm}[tb]
   \caption{\OURS PyTorch pseudocode w/o multi-crop.}
   \label{algo:DINO}
    \definecolor{codeblue}{rgb}{0.25,0.5,0.5}
    \lstset{
      basicstyle=\fontsize{7.2pt}{7.2pt}\ttfamily\bfseries,
      commentstyle=\fontsize{7.2pt}{7.2pt}\color{codeblue},
      keywordstyle=\fontsize{7.2pt}{7.2pt},
    }
\begin{lstlisting}[language=python]
# gs, gt: student and teacher networks
# C: center (K)
# tps, tpt: student and teacher temperatures
# l, m: network and center momentum rates
gt.params = gs.params
for x in loader: # load a minibatch x with n samples
    x1, x2 = augment(x), augment(x) # random views

    s1, s2 = gs(x1), gs(x2) # student output n-by-K
    t1, t2 = gt(x1), gt(x2) # teacher output n-by-K

    loss = H(t1, s2)/2 + H(t2, s1)/2
    loss.backward() # back-propagate

    # student, teacher and center updates
    update(gs) # SGD
    gt.params = l*gt.params + (1-l)*gs.params
    C = m*C + (1-m)*cat([t1, t2]).mean(dim=0)

def H(t, s):
    t = t.detach() # stop gradient
    s = softmax(s / tps, dim=1)
    t = softmax((t - C) / tpt, dim=1) # center + sharpen
    return - (t * log(s)).sum(dim=1).mean()


\end{lstlisting}
\end{algorithm}
This loss is general and can be used on any number of views, even only $2$.
However, we follow the standard setting for multi-crop by using 2 global views at resolution $224^2$ covering a large (for example greater than $50\%$) area of the original image, and several local views of resolution $96^2$ covering only small areas (for example less than $50\%$) of the original image.
We refer to this setting as the basic parametrization of \OURS, unless mentioned otherwise.
%This loss is general and can be used on any number of views, even with only the $2$ global crops, however the benefit of multi-crop relies on the combination of ``local-to-global'' matching and an increase in the number of view comparisons.
%However, the standard setting for the  multi-crop augmentation is to use $2$ views at the standard resolution of $224\times 224$, and several others at a small resolution, typically $96\times 96$.
%This allows to compare a large number of augmentations with limited overhead, and we refer to this setting in the basic parametrization of our framework, unless mentioned otherwise. 
%However, the standard setting for multi-crop is to use local views in addition to the at a small resolution and we refer to this setting in the basic parametrization of our framework, unless mentioned otherwise.
%Particularly, from a given instance, we generate two \emph{global} views, $x^{g}_1$ and $x^{g}_2$, at the standard resolution $224^2$ and a set of \emph{local} views of smaller resolution, typically $96^2$.
%The global views cover a relatively large, for example greater than $50\%$, area of the original image while the local views consists in zooms, covering only a small area, for example less than $50\%$ of the original image.

Both networks share the same architecture $g$ with different sets of parameters $\theta_s$ and $\theta_t$.
We learn the parameters $\theta_s$ by minimizing Eq.~(\ref{eq:loss}) with stochastic gradient descent.

\paragraph{Teacher network.}
Unlike knowledge distillation, we do not have a teacher~$g_{\theta_t}$ given \emph{a priori} and hence, we build it from past iterations of the student network.
We study different update rules for the teacher in Section~\ref{sec:teacher} and show that freezing the teacher network over an epoch works surprisingly well in our framework, while copying the student weight for the teacher fails to converge.
Of particular interest, using an exponential moving average (EMA) on the student weights, i.e., a momentum encoder~\cite{he2020momentum}, is particularly well suited for our framework. 
The update rule is $ \theta_t \leftarrow \lambda \theta_t + (1-\lambda) \theta_s,$ with $\lambda$ following a cosine schedule from $0.996$ to $1$ during training~\cite{grill2020bootstrap}.
Originally the momentum encoder has been introduced as a substitute for a queue in contrastive learning~\cite{he2020momentum}.
However, in our framework, its role differs since we do not have a queue nor a contrastive loss, and may be closer to the role of the mean teacher used in self-training~\cite{tarvainen2017mean}.
Indeed, we observe that this teacher performs a form of model ensembling similar to Polyak-Ruppert averaging with an exponential decay~\cite{polyak1992acceleration, ruppert1988efficient}.
Using Polyak-Ruppert averaging for model ensembling is a standard practice to improve the performance of a model~\cite{jean2014using}.
We observe that this teacher has better performance than the student throughout the training, and hence, guides the training of the student by providing target features of higher quality.
This dynamic was not observed in previous works~\cite{grill2020bootstrap, richemond2020byol}.
\begin{table}[t]
\centering
\caption{\textbf{Networks configuration.}
``Blocks'' is the number of Transformer blocks, ``dim'' is channel dimension and ``heads'' is the number of heads in multi-head attention.
``\# tokens'' is the length of the token sequence when considering $224^2$ resolution inputs, ``\# params'' is the total number of parameters (without counting the projection head) and ``im/s'' is the inference time on a NVIDIA V100 GPU with 128 samples per forward.
}
	\label{tab:archs}
  \setlength{\tabcolsep}{2pt}
  \begin{tabular}{@{}l c c c c c c@{}}
\toprule
	  model & blocks & dim & heads & \#tokens & \#params & im/s \\
    \midrule
	  ResNet-50 & -- & 2048 & -- & -- & 23M & 1237 \\
	  ViT-S/16 & 12 & 384 & 6 & 197 & 21M & 1007 \\
	  ViT-S/8 & 12 & 384 & 6 & 785 & 21M & 180 \\
	  ViT-B/16 & 12 & 768 & 12 & 197 & 85M & 312 \\
	  ViT-B/8 & 12 & 768 & 12 & 785 & 85M & 63 \\
\bottomrule
  \end{tabular}
\end{table}


\paragraph{Network architecture.}
The neural network $g$ is composed of a backbone $f$ (ViT~\cite{dosovitskiy2020image} or ResNet~\cite{he2016deep}), and of a projection head $h$: $g = h \circ f$.
The features used in downstream tasks are the backbone $f$ output.
The projection head consists of a 3-layer multi-layer perceptron (MLP) with hidden dimension $2048$ followed by $\ell_2$ normalization and a weight normalized fully connected layer~\cite{salimans2016weight} with $K$ dimensions, which is similar to the design from SwAV~\cite{caron2020unsupervised}.
We have tested other projection heads and this particular design appears to work best for \OURS (Appendix~\ref{ap:projhead}).
We do not use a predictor~\cite{grill2020bootstrap, chen2020exploring}, resulting in the exact same architecture in both student and teacher networks.
Of particular interest, we note that unlike standard convnets, ViT architectures do not use batch normalizations (BN) by default.
Therefore, when applying \OURS to ViT we do not use any BN also in the projection heads, making the system \emph{entirely BN-free}.

\paragraph{Avoiding collapse.} 
Several self-supervised methods differ by the operation used to avoid collapse, either through contrastive loss~\cite{wu2018unsupervised}, clustering constraints~\cite{caron2018deep,caron2020unsupervised}, predictor~\cite{grill2020bootstrap} or batch normalizations~\cite{grill2020bootstrap,richemond2020byol}.
While our framework can be stabilized with multiple normalizations~\cite{caron2020unsupervised}, it can also work with only a centering and sharpening of the momentum teacher outputs to avoid model collapse.
As shown experimentally in Section~\ref{sec:collapse}, centering prevents one dimension to dominate but encourages collapse to the uniform distribution, while the sharpening has the opposite effect.
Applying both operations balances their effects which is sufficient to avoid collapse in presence of a momentum teacher.
Choosing this method to avoid collapse trades stability for less dependence over the batch:
the centering operation only depends on first-order batch statistics and can be interpreted as adding a bias term $c$ to the teacher: $g_{t}(x) \leftarrow g_{t}(x) + c$.
The center $c$ is updated with an exponential moving average, which allows the approach to work well across different batch sizes as shown in Section~\ref{sec:smallbs}:
\begin{equation}
c \leftarrow m c + (1-m) \frac{1}{B} \sum_{i=1}^B g_{\theta_t}(x_i),
\label{eq:center_update}
\end{equation}
where $m>0$ is a rate parameter and $B$ is the batch size.
Output sharpening is obtained by using a low value for the temperature $\tau_t$ in the teacher softmax normalization.
%Sharpening the teacher distribution with a low $\tau_t$ is important to prevent collapse to uniform output distribution.


%\begin{table}[h]
%\setlength{\tabcolsep}{.3em}
%\centering
%\begin{tabular}{l  c c c c c}
%\toprule
%& MC  & SK & Predictor & Contrastive & Momentum. \\
%\midrule
%SimCLR &&  & & \checkmark &  \\
%\rowcolor{Gray}
%MoCo &&   & & \checkmark & \checkmark \\
%SwAV &\checkmark & \checkmark & & & \\
%\rowcolor{Gray}
%SimSiam & & & \checkmark & & \\
%BYOL & & & \checkmark & & \checkmark \\
%\rowcolor{Gray}
%\OURS &\checkmark& & & & \checkmark \\
%\bottomrule
%\end{tabular}
%\end{table}


%\paragraph{Relation to existing SSL methods.}
%Our approach belongs to the family of siamese representation learning methods~\cite{caron2020unsupervised,chen2020simple,he2020momentum,grill2020bootstrap,chen2020exploring}.
%Similarly to these works, we learn representations of images by matching their views.
%These methods differs from each others by the components used to avoid collapsing or improve performance.
%For instance, our method shares the symmetric structure of SimCLR~\cite{chen2020simple} and MoCo~\cite{he2020momentum} but does not use a contrastive loss to avoid collapse.
%Similarly, our approach differs from BYOL~\cite{grill2020bootstrap} or SimSiam~\cite{chen2020exploring} by the absence of predictor head but the presence of the multi-crop augmentation.
%While we use the same head and multi-crop component of SwAV where the target representation takes the form of a cluster assignment obtained with optimal transport.
%With \OURS, we also directly optimize the similarity between the representations but our centering and sharpening schemes allow us to work without predictor nor normalizations.
%%In a sense, our approach completes the interpretation initiated by BYOL of seeing self-supervised learning as a form of Mean Teacher co-distillation.
%Finally, our approach uses a momentum network like MoCo and BYOL and also takes components from SwAV, in particular the design of the projection head and multi-crop augmentation. 
%We propose a detailed empirical ablation study between \OURS, BYOL and SwAV in Sec.~\ref{sec:comp}.% and with MoCo in supplementary.



%The advantage of ViTs over 
% not sure it is absolutely an advantage, best to state pros and cons as already done below
%In contrast to convolution layer, the self-attention layer does not impose locality constraints.
%These unbounded dependencies potentially make ViT models more expressive but also more challenging to train and prone to overfitting.
%As a consequence, they achieve comparable performance with convnets only when pretrained on large curated datasets~\cite{dosovitskiy2020image} or trained using distillation from a pretrained state-of-the-art convnet~\cite{touvron2020training}.
%Pretraining these models with self-supervised learning on ImageNet is an alternative to avoid overfitting without using extra data nor additional pretrained networks.

\subsection{Implementation and evaluation protocols}
In this section, we provide the implementation details to train with \OURS and present the evaluation protocols used in our experiments.
%The rest of the implementation details are in supplementary. 

\paragraph{Vision Transformer.}
We briefly describe the mechanism of the Vision Transformer~(ViT)~\cite{dosovitskiy2020image,vaswani2017attention} and refer to Vaswani~\etal~\cite{vaswani2017attention} for details about Transformers and to Dosovitskiy~\etal~\cite{dosovitskiy2020image} for its adaptation to images.
We follow the implementation used in \texttt{DeiT}~\cite{touvron2020training}.
We summarize the configuration of the different networks used in this paper in Table~\ref{tab:archs}.
The ViT architecture takes as input a grid of non-overlapping contiguous image patches of resolution $N \times N$.
In this paper we typically use $N=16$ (``/16'') or $N=8$ (``/8'').
The patches are then passed through a linear layer to form a set of embeddings.
We add an extra learnable token to the sequence~\cite{devlin2018bert,dosovitskiy2020image}.
The role of this token is to aggregate information from the entire sequence and we attach the projection head $h$ at its output.
We refer to this token as the class token \texttt{[CLS]} for consistency with previous works\cite{devlin2018bert,dosovitskiy2020image,touvron2020training}, even though it is not attached to any label nor supervision in our case.
The set of patch tokens and \texttt{[CLS]} token are fed to a standard Transformer network with a ``pre-norm'' layer normalization~\cite{chen2018best,klein2017opennmt}.
The Transformer is a sequence of self-attention and feed-forward layers, paralleled with skip connections.
The self-attention layers update the token representations by looking at the other token representations with an attention mechanism~\cite{bahdanau2014neural}.
%Overall, ViTs sequentially process a grid of embeddings of the same size as the input grid.


\paragraph{Implementation details.}
We pretrain the models on the ImageNet dataset~\cite{russakovsky2015imagenet} without labels.
We train with the adamw optimizer~\cite{loshchilov2018fixing} and a batch size of $1024$, distributed over $16$ GPUs when using ViT-S/16.
The learning rate is linearly ramped up during the first $10$ epochs to its base value determined with the following linear scaling rule~\cite{goyal2017accurate}: $lr = 0.0005 * \text{batchsize} / 256$.
After this warmup, we decay the learning rate with a cosine schedule~\cite{loshchilov2016sgdr}.
The weight decay also follows a cosine schedule from $0.04$ to $0.4$.
The temperature $\tau_s$ is set to $0.1$ while we use a linear warm-up for $\tau_t$ from $0.04$ to $0.07$ during the first $30$ epochs.
We follow the data augmentations of BYOL~\cite{grill2020bootstrap} (color jittering, Gaussian blur and solarization) and multi-crop~\cite{caron2020unsupervised} with a bicubic interpolation to adapt the position embeddings to the scales~\cite{dosovitskiy2020image, touvron2020training}.
The code and models to reproduce our results is publicly available.

\paragraph{Evaluation protocols.}
Standard protocols for self-supervised learning are to either learn a linear classifier on frozen features~\cite{zhang2016colorful,he2020momentum} or to finetune the features on downstream tasks.
For linear evaluations, we apply random resize crops and horizontal flips augmentation during training, and report accuracy on a central crop.
%We perform global average pooling of the network output before the linear classifier.
%When evaluating ViT, we concatenate the \texttt{[CLS]} output token to the pooled features.
For finetuning evaluations, we initialize networks with the pretrained weights and adapt them during training.
However, both evaluations are sensitive to hyperparameters, and we observe a large variance in accuracy between runs when varying the learning rate for example. 
We thus also evaluate the quality of features with a simple weighted nearest neighbor classifier ($k$-NN) as in~\cite{wu2018unsupervised}. 
We freeze the pretrain model to compute and store the features of the training data of the downstream task.
The nearest neighbor classifier then matches the feature of an image to the $k$ nearest stored features that votes for the label. 
We sweep over different number of nearest neighbors and find that $20$ NN is consistently working the best for most of our runs.
This evaluation protocol does not require any other hyperparameter tuning, nor data augmentation and can be run with only one pass over the downstream dataset, greatly simplifying the feature evaluation.
 
