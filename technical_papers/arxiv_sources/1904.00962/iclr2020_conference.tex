
\documentclass{article} % For LaTeX2e
\usepackage{iclr2020_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{url}

\usepackage{graphicx} % more modern
\usepackage{subfigure} 
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{relsize}
\usepackage{nicefrac}
\usepackage{xspace}
\usepackage{amsmath,amssymb,enumerate}
\usepackage{amsthm,cancel}
\usepackage{natbib}
\usepackage{enumitem}
\usepackage{dsfont}
\usepackage[colorlinks=true,citecolor=blue]{hyperref}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}

\newtheorem*{lemma*}{Lemma}
\newtheorem*{theorem*}{Theorem}
\newtheorem*{assumption*}{Assumption}
\newtheorem*{corollary*}{Corollary}
\newtheorem*{remark*}{Remark}
\newtheorem*{definition*}{Definition}

\newcommand{\resnet}{\textsc{ResNet}}
\newcommand{\bert}{\textsc{Bert}\xspace}


\newcommand{\sag}{\textsc{Sag}}
\newcommand{\sgd}{\textsc{Sgd}\xspace}
\newcommand{\sdca}{\textsc{Sdca}}
\newcommand{\csgd}{\textsc{CSgd}}
\newcommand{\dsgd}{\textsc{DSgd}}
\newcommand{\saga}{\textsc{Saga}}
\newcommand{\stgd}{\textsc{S2gd}}
\newcommand{\svrg}{\textsc{Svrg}\xspace}
\newcommand{\msvrg}{\textsc{Msvrg}}
\newcommand{\gd}{\textsc{GradientDescent}\xspace}
\newcommand{\adagrad}{\textsc{Adagrad}}
\newcommand{\nadam}{\textsc{Nadam}}
\newcommand{\adadelta}{\textsc{Adadelta}}
\newcommand{\adam}{\textsc{Adam}}
\newcommand{\adamnc}{\textsc{AdamNc}}
\newcommand{\adamw}{\textsc{AdamW}}
\newcommand{\rmsprop}{\textsc{RMSprop}}
\newcommand{\amsgrad}{\textsc{AMSGrad}}
\newcommand{\yogi}{\textsc{Yogi}}
\newcommand{\lamb}{\textsc{Lamb}}
\newcommand{\lars}{\textsc{Lars}}
\newcommand{\avsgrad}{\textsc{AVSGrad}}
\newcommand{\cifarnet}{\textsc{Cifarnet}}
\newcommand{\diag}{\text{diag}\xspace}
\newcommand{\sml}[1]{{\small #1}}
\newcommand{\fromto}[3]{\sml{$#1{\;\le\;}#2{\;\le\;}#3$}}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\note}[1]{\textcolor{red}{#1}}
\newcommand{\ip}[2]{\left\langle #1, #2\right\rangle}
\newcommand{\nlsum}{\sum\nolimits}
%\newcommand{\E}{\mathbb{E}}
\newcommand{\Fc}{\mathcal{F}}

\renewcommand{\baselinestretch}{0.97}
\frenchspacing
\sloppy

\usepackage{hyperref}
\usepackage{url}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
%\usepackage{devanagari}
\usepackage{algorithm}
\usepackage{algorithmic}

\title{Large Batch Optimization for Deep Learning: Training BERT in 76 minutes}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{\small Yang You$^{2}$, 
%\footnote{Work was performed when Y.You was a student researcher at Google Brain}
Jing Li$^{1}$, Sashank Reddi$^{1}$, Jonathan Hseu$^{1}$, Sanjiv Kumar$^{1}$, Srinadh Bhojanapalli$^{1}$\\{\bf \small Xiaodan Song}$^{1}$, {\bf \small James Demmel}$^{2}$, {\bf \small Kurt Keutzer}$^{2}$, {\bf \small Cho-Jui Hsieh}$^{1,3}$\\
{\scriptsize Yang You was a student researcher at Google Brain. This project was done when he was at Google Brain.}
\\
\normalsize{Google$^{1}$},
\normalsize{UC Berkeley$^{2}$},
\normalsize{UCLA$^{3}$}\\
\scriptsize{\{youyang, demmel, keutzer\}@cs.berkeley.edu, \{jingli, sashank, jhseu, sanjivk, bsrinadh, xiaodansong, chojui\}@google.com}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Training large deep neural networks on massive datasets is  computationally very challenging. There has been recent surge in interest in using \emph{large batch} stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is $\lars$, which by  employing \emph{layerwise adaptive} learning rates trains $\resnet$ on ImageNet in a few minutes. However, $\lars$ performs poorly for attention models like $\bert$, indicating that its performance gains are \emph{not} consistent across tasks. In this paper, we first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique called $\lamb$; we then provide convergence analysis of $\lamb$ as well as $\lars$, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of $\lamb$ across various tasks such as $\bert$ and $\resnet$-50 training with very little hyperparameter tuning. In particular, for $\bert$ training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance.  By increasing the batch size to the memory limit of a TPUv3 Pod, $\bert$ training time can be reduced from 3 days to just 76 minutes (Table \ref{table:results}). The $\lamb$ implementation is available online\footnote{\url{https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py}}.
\end{abstract}

\iffalse
Training large deep neural networks on massive datasets is  computationally very challenging. There has been recent surge in interest in using \emph{large batch} stochastic optimization methods to tackle this issue. The use of large batch of gradients (which are computed in parallel) in these methods enable them to use much larger learning rates, thereby, drastically reducing the training time. However, current methods require extensive tuning of learning rates and their performance gains are \emph{not} consistent across tasks. In this paper, we study a principled adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layer-wise adaptive large batch optimization technique called $\lamb$. We provide a formal convergence analysis of $\lamb$ as well as the previous published layerwise optimizer $\lars$, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of $\lamb$ across various tasks such as BERT and ResNet-50 training with very little tuning. In particular, for BERT training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance.  By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes (Table \ref{table:results}).
\fi


\iffalse
\begin{abstract}
Training large deep neural networks on massive datasets is  computationally very challenging. One promising approach to tackle this issue is through the use of \emph{large batch} parallel stochastic optimization. However, our understanding of this approach in the context of deep learning is still very limited. Furthermore, the current approaches in this direction are heavily hand-tuned. To this end, we first study a general adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layer-wise adaptive large batch optimization technique called $\lamb$. We also provide a formal convergence analysis of $\lamb$ as well as the previous published layerwise optimizer $\lars$, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of $\lamb$ for BERT and ResNet-50 training.  In particular, for BERT training, our optimizer enables use of very large batches sizes of 32868; thereby, requiring just 8599 iterations to train (as opposed to 1 million iterations in the original paper).  By increasing the batch size to the memory limit of a TPUv3 Pod,  BERT training time can be reduced from 3 days to 76 minutes (Table \ref{table:results}). Finally, we demonstrate that $\lamb$ outperforms previous large-batch training algorithms for ResNet-50 on ImageNet; obtaining state-of-the-art performance in just a few minutes.
%\textcolor{blue}{An implementation of LAMB can be found at https://github.com/xxx.} 
%The training scripts of BERT and ResNet-50 are available upon request.
More results are in the appendix of this paper.
\end{abstract}
\fi

\section{Introduction}
\input{introduction}

\section{Preliminaries}
\input{prelims}

\section{Algorithms}
\input{algorithms}

\section{Experiments}
\label{sec:experiments}
\input{experiments}

%\vspace{-2mm}
\section{Conclusion}
%\vspace*{-3mm}
\input{conclusion}

\vspace{-2mm}
\section{Acknowledgement}
\vspace*{-3mm}
\input{acknowledgement}


\bibliography{iclr2020_conference}
\bibliographystyle{iclr2020_conference}

\appendix
%\section{Appendix}
\input{appendix.tex}

\end{document}
