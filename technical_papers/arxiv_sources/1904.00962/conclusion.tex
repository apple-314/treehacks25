Large batch techniques are critical to speeding up deep neural network training. In this paper, we propose the $\lamb$ optimizer, which supports adaptive elementwise updating and layerwise learning rates. Furthermore, $\lamb$ is a general purpose optimizer that works for both small and large batches. We also provided theoretical analysis for the $\lamb$ optimizer, highlighting the cases where it performs better than standard $\sgd$. $\lamb$ achieves a better performance than existing optimizers for a wide range of applications.  By using $\lamb$, we are able to scale the batch size of $\bert$ pre-training to 64K without losing accuracy, thereby, reducing the $\bert$ training time from 3 days to around 76 minutes. $\lamb$ is also the first large batch adaptive solver that can achieve state-of-the-art accuracy on ImageNet training with $\resnet$-50.