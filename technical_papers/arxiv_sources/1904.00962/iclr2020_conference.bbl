\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Akiba et~al.(2017)Akiba, Suzuki, and Fukuda]{akiba2017extremely}
Takuya Akiba, Shuji Suzuki, and Keisuke Fukuda.
\newblock Extremely large minibatch sgd: Training resnet-50 on imagenet in 15
  minutes.
\newblock \emph{arXiv preprint arXiv:1711.04325}, 2017.

\bibitem[Bengio(2012)]{bengio2012practical}
Yoshua Bengio.
\newblock Practical recommendations for gradient-based training of deep
  architectures.
\newblock In \emph{Neural networks: Tricks of the trade}, pp.\  437--478.
  Springer, 2012.

\bibitem[Bernstein et~al.(2018)Bernstein, Wang, Azizzadenesheli, and
  Anandkumar]{signsgd}
Jeremy Bernstein, Yu{-}Xiang Wang, Kamyar Azizzadenesheli, and Anima
  Anandkumar.
\newblock signsgd: compressed optimisation for non-convex problems.
\newblock \emph{CoRR}, abs/1802.04434, 2018.

\bibitem[Codreanu et~al.(2017)Codreanu, Podareanu, and
  Saletore]{codreanu2017scale}
Valeriu Codreanu, Damian Podareanu, and Vikram Saletore.
\newblock Scale out for large minibatch sgd: Residual network training on
  imagenet-1k with improved accuracy and reduced time to train.
\newblock \emph{arXiv preprint arXiv:1711.04291}, 2017.

\bibitem[Coleman et~al.(2017)Coleman, Narayanan, Kang, Zhao, Zhang, Nardi,
  Bailis, Olukotun, R{\'e}, and Zaharia]{coleman2017dawnbench}
Cody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi
  Nardi, Peter Bailis, Kunle Olukotun, Chris R{\'e}, and Matei Zaharia.
\newblock Dawnbench: An end-to-end deep learning benchmark and competition.
\newblock \emph{Training}, 100\penalty0 (101):\penalty0 102, 2017.

\bibitem[Dean et~al.(2012)Dean, Corrado, Monga, Chen, Devin, Mao, Senior,
  Tucker, Yang, Le, et~al.]{dean2012large}
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
  Andrew Senior, Paul Tucker, Ke~Yang, Quoc~V Le, et~al.
\newblock Large scale distributed deep networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1223--1231, 2012.

\bibitem[Devarakonda et~al.(2017)Devarakonda, Naumov, and
  Garland]{devarakonda2017adabatch}
Aditya Devarakonda, Maxim Naumov, and Michael Garland.
\newblock Adabatch: Adaptive batch sizes for training deep neural networks.
\newblock \emph{arXiv preprint arXiv:1712.02029}, 2017.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dozat(2016)]{dozat2016incorporating}
Timothy Dozat.
\newblock Incorporating nesterov momentum into adam.
\newblock 2016.

\bibitem[Ghadimi \& Lan(2013{\natexlab{a}})Ghadimi and Lan]{Ghadimi13}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first- and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{{SIAM} Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013{\natexlab{a}}.
\newblock \doi{10.1137/120880811}.

\bibitem[Ghadimi \& Lan(2013{\natexlab{b}})Ghadimi and
  Lan]{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013{\natexlab{b}}.

\bibitem[Ghadimi et~al.(2014)Ghadimi, Lan, and Zhang]{Ghadimi14}
Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang.
\newblock Mini-batch stochastic approximation methods for nonconvex stochastic
  composite optimization.
\newblock \emph{Mathematical Programming}, 155\penalty0 (1-2):\penalty0
  267--305, 2014.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Priya Goyal, Piotr Doll{\'a}r, Ross Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Hoffer et~al.(2017)Hoffer, Hubara, and Soudry]{hoffer2017train}
Elad Hoffer, Itay Hubara, and Daniel Soudry.
\newblock Train longer, generalize better: closing the generalization gap in
  large batch training of neural networks.
\newblock \emph{arXiv preprint arXiv:1705.08741}, 2017.

\bibitem[Iandola et~al.(2016)Iandola, Moskewicz, Ashraf, and
  Keutzer]{iandola2016firecaffe}
Forrest~N Iandola, Matthew~W Moskewicz, Khalid Ashraf, and Kurt Keutzer.
\newblock Firecaffe: near-linear acceleration of deep neural network training
  on compute clusters.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  2592--2600, 2016.

\bibitem[Jia et~al.(2018)Jia, Song, He, Wang, Rong, Zhou, Xie, Guo, Yang, Yu,
  et~al.]{jia2018highly}
Xianyan Jia, Shutao Song, Wei He, Yangzihao Wang, Haidong Rong, Feihu Zhou,
  Liqiang Xie, Zhenyu Guo, Yuanzhou Yang, Liwei Yu, et~al.
\newblock Highly scalable deep learning training system with mixed-precision:
  Training imagenet in four minutes.
\newblock \emph{arXiv preprint arXiv:1807.11205}, 2018.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Krizhevsky(2014)]{krizhevsky2014one}
Alex Krizhevsky.
\newblock One weird trick for parallelizing convolutional neural networks.
\newblock \emph{arXiv preprint arXiv:1404.5997}, 2014.

\bibitem[Li(2017)]{li2017scaling}
Mu~Li.
\newblock \emph{Scaling Distributed Machine Learning with System and Algorithm
  Co-design}.
\newblock PhD thesis, Intel, 2017.

\bibitem[Martens \& Grosse(2015)Martens and Grosse]{martens2015optimizing}
James Martens and Roger Grosse.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock In \emph{International conference on machine learning}, pp.\
  2408--2417, 2015.

\bibitem[Mikami et~al.(2018)Mikami, Suganuma, Tanaka, Kageyama,
  et~al.]{mikami2018imagenet}
Hiroaki Mikami, Hisahiro Suganuma, Yoshiki Tanaka, Yuichi Kageyama, et~al.
\newblock Imagenet/resnet-50 training in 224 seconds.
\newblock \emph{arXiv preprint arXiv:1811.05233}, 2018.

\bibitem[Nesterov(1983)]{nesterov1983method}
Yurii~E Nesterov.
\newblock A method for solving the convex programming problem with convergence
  rate o (1/k\^{} 2).
\newblock In \emph{Dokl. akad. nauk Sssr}, volume 269, pp.\  543--547, 1983.

\bibitem[Osawa et~al.(2018)Osawa, Tsuji, Ueno, Naruse, Yokota, and
  Matsuoka]{osawa2018second}
Kazuki Osawa, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Rio Yokota, and Satoshi
  Matsuoka.
\newblock Second-order optimization method for large mini-batch: Training
  resnet-50 on imagenet in 35 epochs.
\newblock \emph{arXiv preprint arXiv:1811.12019}, 2018.

\bibitem[Recht et~al.(2011)Recht, Re, Wright, and Niu]{recht2011hogwild}
Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu.
\newblock Hogwild: A lock-free approach to parallelizing stochastic gradient
  descent.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  693--701, 2011.

\bibitem[Shallue et~al.(2018)Shallue, Lee, Antognini, Sohl-Dickstein, Frostig,
  and Dahl]{shallue2018measuring}
Christopher~J Shallue, Jaehoon Lee, Joe Antognini, Jascha Sohl-Dickstein, Roy
  Frostig, and George~E Dahl.
\newblock Measuring the effects of data parallelism on neural network training.
\newblock \emph{arXiv preprint arXiv:1811.03600}, 2018.

\bibitem[Smith et~al.(2017)Smith, Kindermans, and Le]{smith2017don}
Samuel~L Smith, Pieter-Jan Kindermans, and Quoc~V Le.
\newblock Don't decay the learning rate, increase the batch size.
\newblock \emph{arXiv preprint arXiv:1711.00489}, 2017.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and
  Hinton]{sutskever2013importance}
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In \emph{International conference on machine learning}, pp.\
  1139--1147, 2013.

\bibitem[Yamazaki et~al.(2019)Yamazaki, Kasagi, Tabuchi, Honda, Miwa, Fukumoto,
  Tabaru, Ike, and Nakashima]{yamazaki2019yet}
Masafumi Yamazaki, Akihiko Kasagi, Akihiro Tabuchi, Takumi Honda, Masahiro
  Miwa, Naoto Fukumoto, Tsuguchika Tabaru, Atsushi Ike, and Kohta Nakashima.
\newblock Yet another accelerated sgd: Resnet-50 training on imagenet in 74.7
  seconds.
\newblock \emph{arXiv preprint arXiv:1903.12650}, 2019.

\bibitem[Ying et~al.(2018)Ying, Kumar, Chen, Wang, and Cheng]{ying2018image}
Chris Ying, Sameer Kumar, Dehao Chen, Tao Wang, and Youlong Cheng.
\newblock Image classification at supercomputer scale.
\newblock \emph{arXiv preprint arXiv:1811.06992}, 2018.

\bibitem[You et~al.(2017)You, Gitman, and Ginsburg]{you2017scaling}
Yang You, Igor Gitman, and Boris Ginsburg.
\newblock Scaling sgd batch size to 32k for imagenet training.
\newblock \emph{arXiv preprint arXiv:1708.03888}, 2017.

\bibitem[You et~al.(2018)You, Zhang, Hsieh, Demmel, and
  Keutzer]{you2018imagenet}
Yang You, Zhao Zhang, Cho-Jui Hsieh, James Demmel, and Kurt Keutzer.
\newblock Imagenet training in minutes.
\newblock In \emph{Proceedings of the 47th International Conference on Parallel
  Processing}, pp.\ ~1. ACM, 2018.

\bibitem[You et~al.(2019)You, Hseu, Ying, Demmel, Keutzer, and
  Hsieh]{you2019large}
Yang You, Jonathan Hseu, Chris Ying, James Demmel, Kurt Keutzer, and Cho-Jui
  Hsieh.
\newblock Large-batch training for lstm and beyond.
\newblock \emph{arXiv preprint arXiv:1901.08256}, 2019.

\end{thebibliography}
