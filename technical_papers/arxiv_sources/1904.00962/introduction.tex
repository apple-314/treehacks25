%!TEX root = nips_2018.tex

With the advent of large scale datasets, training large deep neural networks, even using computationally efficient optimization methods like Stochastic gradient descent $(\sgd)$, has become particularly challenging. For instance, training state-of-the-art deep learning models like \bert and ResNet-50 takes 3 days on 16 TPUv3 chips and 29 hours on 8 Tesla P100 gpus respectively \citep{devlin2018bert,he2016deep}. Thus, there is a growing interest to develop optimization solutions to tackle this critical issue. The goal of this paper is to investigate and develop optimization techniques to accelerate training large deep neural networks, mostly focusing on approaches based on variants of \sgd. 

Methods based on $\sgd$ iteratively update the parameters of the model by moving them in a scaled (negative) direction of the gradient calculated on a minibatch.   However, $\sgd$'s scalability is limited by its inherent sequential nature. Owing to this limitation, traditional approaches to improve \sgd training time in the context of deep learning largely resort to distributed asynchronous setup~\citep{dean2012large,recht2011hogwild}. However,  the implicit staleness introduced due to the asynchrony limits the parallelization of the approach, often leading to degraded performance. The feasibility of computing gradient on \emph{large minibatches} in parallel due to recent hardware advances has seen the resurgence of simply using synchronous \sgd with large minibatches as an alternative to asynchronous \sgd. However, na\"ively increasing the batch size typically results in degradation of generalization performance and reduces computational benefits \citep{goyal2017accurate}.

Synchronous $\sgd$ on large minibatches benefits from reduced variance of the stochastic gradients used in $\sgd$. This allows one to use much larger learning rates in $\sgd$, typically of the order square root of the minibatch size. Surprisingly, recent works have demonstrated that up to certain minibatch sizes, linear scaling of the learning rate with minibatch size can be used to further speed up the training \cite{goyal2017accurate}. These works also elucidate two interesting aspects to enable the use of linear scaling in large batch synchronous $\sgd$: (i) linear scaling of learning rate is harmful during the initial phase; thus, a hand-tuned warmup strategy of slowly increasing the learning rate needs to be used initially, and (ii) linear scaling of learning rate can be detrimental beyond a certain batch size. Using these tricks, \cite{goyal2017accurate} was able to drastically reduce the training time of ResNet-50 model from 29 hours to 1 hour using a batch size of 8192. While these works demonstrate the feasibility of this strategy for reducing the wall time for training large deep neural networks, they also highlight the need for an adaptive learning rate mechanism for large batch learning. 

Variants of $\sgd$ using layerwise adaptive learning rates have been recently proposed to address this problem. The most successful in this line of research is the $\lars$ algorithm \citep{you2017scaling}, which was initially proposed for training $\resnet$. Using $\lars$, ResNet-50 can be trained on ImageNet in just a few minutes! However, it has been observed that its performance gains are \emph{not} consistent across tasks. For instance, $\lars$ performs poorly for attention models like $\bert$. Furthermore, theoretical understanding of the adaptation employed in $\lars$ is largely missing. To this end, we study and develop new approaches specially catered to the large batch setting of our interest.

{\bf Contributions.} More specifically, we make the following main contributions in this paper.

\begin{itemize}
\item Inspired by $\lars$, we investigate a general adaptation strategy specially catered to large batch learning and provide intuition for the strategy.
\item Based on the adaptation strategy, we develop a new optimization algorithm (\lamb) for achieving adaptivity of learning rate in $\sgd$. Furthermore, we provide convergence analysis for both $\lars$ and $\lamb$ to achieve a stationary point in nonconvex settings. We highlight the benefits of using these methods for large batch settings.
\item We demonstrate the strong empirical performance of $\lamb$ across several challenging tasks. Using $\lamb$ we scale the batch size in training $\bert$ to more than 32k without degrading the performance; thereby, cutting the time down from 3 days to 76 minutes. Ours is the first work to reduce $\bert$ training wall time to less than couple of hours.
\item We also demonstrate the efficiency of $\lamb$ for training state-of-the-art image classification models like $\resnet$. To the best of our knowledge, ours is first adaptive solver that can achieve state-of-the-art accuracy for $\resnet$-50 as adaptive solvers like Adam fail to obtain the accuracy of $\sgd$ with momentum for these tasks.
\end{itemize}

\subsection{Related Work}

The literature on optimization for machine learning is vast and hence, we restrict our attention to the most relevant works here. Earlier works on large batch optimization for machine learning mostly focused on convex models, benefiting by a factor of square root of batch size using appropriately large learning rate. Similar results can be shown for nonconvex settings wherein using larger minibatches improves the convergence to stationary points; albeit at the cost of extra computation. However, several important concerns were raised with respect to generalization and computational performance in large batch nonconvex settings. It was observed that training with extremely large batch was difficult~\citep{keskar2016large, hoffer2017train}. Thus, several prior works carefully hand-tune training hyper-parameters, like learning rate and momentum, to avoid degradation of generalization performance \citep{goyal2017accurate, li2017scaling, you2018imagenet, shallue2018measuring}. 

\citep{krizhevsky2014one} empirically found that simply scaling the learning rate linearly with respect to batch size works better up to certain batch sizes. To avoid optimization instability due to linear scaling of learning rate, \citet{goyal2017accurate} proposed a highly hand-tuned learning rate which involves a warm-up strategy that gradually increases the LR to a larger value and then switching to the regular LR policy (e.g. exponential or polynomial decay). Using LR warm-up and linear scaling, \citet{goyal2017accurate} managed to train $\resnet$-50 with batch size 8192 without loss in generalization performance. However, empirical study \citep{shallue2018measuring} shows that learning rate scaling heuristics with the batch size do not hold across all problems or across all batch sizes.

More recently, to reduce hand-tuning of hyperparameters, adaptive learning rates for large batch training garnered significant interests. Several recent works successfully scaled the batch size to large values using adaptive learning rates without degrading the performance, thereby, finishing $\resnet$-50 training on ImageNet in a few minutes \citep{you2018imagenet,iandola2016firecaffe,codreanu2017scale,akiba2017extremely,jia2018highly,smith2017don,martens2015optimizing,devarakonda2017adabatch,mikami2018imagenet,osawa2018second,you2019large,yamazaki2019yet}.
To the best of our knowledge, the fastest training result for $\resnet$-50 on ImageNet is due to \cite{ying2018image}, who achieve 76+\% top-1 accuracy. By using the $\lars$ optimizer and scaling the batch size to 32K  on a TPUv3 Pod, \citet{ying2018image} was able to train $\resnet$-50 on ImageNet in 2.2 minutes. However, it was empirically observed that none of these performance gains hold in other tasks such as BERT training (see Section~\ref{sec:experiments}).  


\iffalse
\subsection{Related Work}

The literature on optimization for machine learning is vast and hence, we restrict our attention to the works on large batch settings that are most relevant to our paper. Earlier works on large batch optimization for machine learning mostly focused on convex models. It is known that for general stochastic convex objective functions, the convergence of $\sgd$ with minibatch $b$ is $O(1/\sqrt{bT} + 1/T)$.  If a more complex optimization problem is solved in each iteration, the convergence rate can be improved to $O(1/\sqrt{bT})$, which improves when batch size $b$ is large. Similar results can be shown for nonconvex settings wherein using larger minibatches improves the convergence to stationary points; albeit at the cost of extra computation. However, several important concerns were raised with respect to generalization and computational performance in large batch nonconvex settings. It was observed that training with extremely large batch was difficult~\citep{keskar2016large, hoffer2017train}. The researchers need to carefully tune training hyper-parameters, like learning rate and momentum, to avoid losing test accuracy \citep{goyal2017accurate, li2017scaling, you2018imagenet, shallue2018measuring}. 

\citet{krizhevsky2014one} introduced some practical schemes for training with large batches. One important rule is to increase the LR (learning rate) by $\sqrt{b}$ when batch size is scaled by $b$ since  the variance of the gradient estimation decreases by a factor of $b$. In practice, \citep{krizhevsky2014one} found that  linear scaling works better up to certain batch sizes. To avoid optimization instability due to high learning rate, \citet{goyal2017accurate} proposed to use a highly hand-tuned  learning rate warm-up strategy which starts with a small LR and then gradually increases the LR to a larger value. After warm-up period (usually a few epochs) one switches to the regular LR policy (multi-steps, exponential or polynomial decay etc). Using LR warm-up and linear scaling, \citet{goyal2017accurate} managed to train $\resnet$-50 with batch size 8192 without loss in test accuracy. However, empirical study \citep{shallue2018measuring} shows that learning rate scaling heuristics with the batch size do not hold across all problems or across all batch sizes.

More recently, to reduce hand-tuning of hyperparameters, adaptive learning rates for large batch training garnered significant interests. Several recent works successfully scaled the batch size to large values using adaptive learning rates without degrading the performance, thereby, finishing $\resnet$-50 training on ImageNet in a few minutes \citep{you2018imagenet,iandola2016firecaffe,codreanu2017scale,akiba2017extremely,jia2018highly,smith2017don,martens2015optimizing,devarakonda2017adabatch,mikami2018imagenet,osawa2018second,you2019large,yamazaki2019yet}.
To the best of our knowledge, the fastest training result for $\resnet$-50 on ImageNet is due to \cite{ying2018image}, who achieve 76+\% top-1 accuracy. By using the $\lars$ optimizer and scaling the batch size to 32K  on a TPUv3 Pod, \citet{ying2018image} was able to train $\resnet$-50 on ImageNet in 2.2 minutes. 

\fi