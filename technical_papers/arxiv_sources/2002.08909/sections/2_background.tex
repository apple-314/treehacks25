\section{Background}
\label{sec:background}
\paragraph{Language model pre-training}

The goal of language model pre-training is to learn useful representations of language,
usually from unlabeled text corpora.
The resulting pre-trained model can then be further trained (\emph{fine-tuned}) for a downstream task of primary interest (in our case, \openqa),
often leading to better generalization than training from scratch \cite{dai_finetune,gpt2}.

We focus on the {\em masked language model}\footnote{Strictly speaking, MLM is not a standard language model, since it does not define a distribution over the entire sequence of tokens. In the paper we sometimes abuse the term ``language model'' slightly to make the phrase shorter.}
(MLM) variant of pre-training popularized by BERT~\cite{bert}.
In its basic form, an MLM is trained to predict the missing tokens in an input text passage.
Given an unlabeled pre-training corpus $\mathcal{X}$ (e.g., Wikipedia text),
a training example $(x, y)$ can be generated by randomly masking tokens in a sampled piece of text
(e.g., $x =$ \nl{The \mask is the currency \mask the UK}; $y =$ (\nl{pound}, \nl{of})).
The model uses its representation of the masked input $x$ to predict
the token that should go in each mask.
A good MLM must learn to encode syntactic and semantic information (e.g., to predict \nl{of})
as well as some world knowledge (e.g., to predict \nl{pound}).

\paragraph{Open-domain question answering (\openqa)}
To measure a model's ability to incorporate world knowledge, we need a downstream task where world knowledge is critical.
Perhaps one of the most knowledge-intensive tasks in natural language processing is open-domain question answering (\openqa):
given a question $x$ such as ``\texttt{\small What is the currency of the UK?}'', a model must output the correct answer string $y$, ``\texttt{{\small pound}}''.
The ``open'' part of \openqa refers to the fact that the model does {\em not} receive a pre-identified document that is known to contain the answer, unlike traditional reading comprehension (RC) tasks such as SQuAD \cite{squad, squad2}. While RC models comprehend a single document, \openqa models must retain knowledge from millions of documents, since a question could be about any of them.

We focus on \openqa systems that utilize a \emph{textual knowledge corpus} $\cZ$ as the knowledge source.
Many of these systems employ a \emph{retrieval-based} approach:
given a question $x$, retrieve potentially relevant documents $z$ from the corpus $\cZ$,
and then extract an answer $y$ from the documents~\cite{askmsr,drqa,orqa}.
Our approach, \thename, is inspired by this paradigm and extends it to language model pre-training.
Alternatively, some recent work has proposed \emph{generation-based} systems that
apply a sequence-to-sequence model on $x$ to directly generate $y$ token-by-token
\cite{bart_not_bert,t5}.
We will compare against state-of-the-art systems from both paradigms in our experiments.