[
    {
        "arxiv_id": "1706.03762",
        "title": "Attention Is All You Need",
        "authors": [],
        "abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.",
        "year": "2017",
        "url": "https://arxiv.org/abs/1706.03762"
    },
    {
        "arxiv_id": "2001.08361",
        "title": "Scaling Laws for Neural Language Models",
        "authors": [],
        "abstract": "We study empirical scaling laws for language model performance on the\ncross-entropy loss. The loss scales as a power-law with model size, dataset\nsize, and the amount of compute used for training, with some trends spanning\nmore than seven orders of magnitude. Other architectural details such as\nnetwork width or depth have minimal effects within a wide range. Simple\nequations govern the dependence of overfitting on model/dataset size and the\ndependence of training speed on model size. These relationships allow us to\ndetermine the optimal allocation of a fixed compute budget. Larger models are\nsignificantly more sample-efficient, such that optimally compute-efficient\ntraining involves training very large models on a relatively modest amount of\ndata and stopping significantly before convergence.",
        "year": "2020",
        "url": "https://arxiv.org/abs/2001.08361"
    },
    {
        "arxiv_id": "2303.08774",
        "title": "GPT-4 Technical Report",
        "authors": [],
        "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can\naccept image and text inputs and produce text outputs. While less capable than\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance on\nvarious professional and academic benchmarks, including passing a simulated bar\nexam with a score around the top 10% of test takers. GPT-4 is a\nTransformer-based model pre-trained to predict the next token in a document.\nThe post-training alignment process results in improved performance on measures\nof factuality and adherence to desired behavior. A core component of this\nproject was developing infrastructure and optimization methods that behave\npredictably across a wide range of scales. This allowed us to accurately\npredict some aspects of GPT-4's performance based on models trained with no\nmore than 1/1,000th the compute of GPT-4.",
        "year": "2023",
        "url": "https://arxiv.org/abs/2303.08774"
    },
    {
        "arxiv_id": "1904.00962",
        "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes",
        "authors": [],
        "abstract": "Training large deep neural networks on massive datasets is computationally\nvery challenging. There has been recent surge in interest in using large batch\nstochastic optimization methods to tackle this issue. The most prominent\nalgorithm in this line of research is LARS, which by employing layerwise\nadaptive learning rates trains ResNet on ImageNet in a few minutes. However,\nLARS performs poorly for attention models like BERT, indicating that its\nperformance gains are not consistent across tasks. In this paper, we first\nstudy a principled layerwise adaptation strategy to accelerate training of deep\nneural networks using large mini-batches. Using this strategy, we develop a new\nlayerwise adaptive large batch optimization technique called LAMB; we then\nprovide convergence analysis of LAMB as well as LARS, showing convergence to a\nstationary point in general nonconvex settings. Our empirical results\ndemonstrate the superior performance of LAMB across various tasks such as BERT\nand ResNet-50 training with very little hyperparameter tuning. In particular,\nfor BERT training, our optimizer enables use of very large batch sizes of 32868\nwithout any degradation of performance. By increasing the batch size to the\nmemory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to\njust 76 minutes (Table 1). The LAMB implementation is available at\nhttps://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py",
        "year": "2019",
        "url": "https://arxiv.org/abs/1904.00962"
    },
    {
        "arxiv_id": "1803.03635",
        "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",
        "authors": [],
        "abstract": "Neural network pruning techniques can reduce the parameter counts of trained\nnetworks by over 90%, decreasing storage requirements and improving\ncomputational performance of inference without compromising accuracy. However,\ncontemporary experience is that the sparse architectures produced by pruning\nare difficult to train from the start, which would similarly improve training\nperformance.\n  We find that a standard pruning technique naturally uncovers subnetworks\nwhose initializations made them capable of training effectively. Based on these\nresults, we articulate the \"lottery ticket hypothesis:\" dense,\nrandomly-initialized, feed-forward networks contain subnetworks (\"winning\ntickets\") that - when trained in isolation - reach test accuracy comparable to\nthe original network in a similar number of iterations. The winning tickets we\nfind have won the initialization lottery: their connections have initial\nweights that make training particularly effective.\n  We present an algorithm to identify winning tickets and a series of\nexperiments that support the lottery ticket hypothesis and the importance of\nthese fortuitous initializations. We consistently find winning tickets that are\nless than 10-20% of the size of several fully-connected and convolutional\nfeed-forward architectures for MNIST and CIFAR10. Above this size, the winning\ntickets that we find learn faster than the original network and reach higher\ntest accuracy.",
        "year": "2018",
        "url": "https://arxiv.org/abs/1803.03635"
    },
    {
        "arxiv_id": "2002.08909",
        "title": "REALM: Retrieval-Augmented Language Model Pre-Training",
        "authors": [],
        "abstract": "Language model pre-training has been shown to capture a surprising amount of\nworld knowledge, crucial for NLP tasks such as question answering. However,\nthis knowledge is stored implicitly in the parameters of a neural network,\nrequiring ever-larger networks to cover more facts.\n  To capture knowledge in a more modular and interpretable way, we augment\nlanguage model pre-training with a latent knowledge retriever, which allows the\nmodel to retrieve and attend over documents from a large corpus such as\nWikipedia, used during pre-training, fine-tuning and inference. For the first\ntime, we show how to pre-train such a knowledge retriever in an unsupervised\nmanner, using masked language modeling as the learning signal and\nbackpropagating through a retrieval step that considers millions of documents.\n  We demonstrate the effectiveness of Retrieval-Augmented Language Model\npre-training (REALM) by fine-tuning on the challenging task of Open-domain\nQuestion Answering (Open-QA). We compare against state-of-the-art models for\nboth explicit and implicit knowledge storage on three popular Open-QA\nbenchmarks, and find that we outperform all previous methods by a significant\nmargin (4-16% absolute accuracy), while also providing qualitative benefits\nsuch as interpretability and modularity.",
        "year": "2020",
        "url": "https://arxiv.org/abs/2002.08909"
    },
    {
        "arxiv_id": "2005.11401",
        "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
        "authors": [],
        "abstract": "Large pre-trained language models have been shown to store factual knowledge\nin their parameters, and achieve state-of-the-art results when fine-tuned on\ndownstream NLP tasks. However, their ability to access and precisely manipulate\nknowledge is still limited, and hence on knowledge-intensive tasks, their\nperformance lags behind task-specific architectures. Additionally, providing\nprovenance for their decisions and updating their world knowledge remain open\nresearch problems. Pre-trained models with a differentiable access mechanism to\nexplicit non-parametric memory can overcome this issue, but have so far been\nonly investigated for extractive downstream tasks. We explore a general-purpose\nfine-tuning recipe for retrieval-augmented generation (RAG) -- models which\ncombine pre-trained parametric and non-parametric memory for language\ngeneration. We introduce RAG models where the parametric memory is a\npre-trained seq2seq model and the non-parametric memory is a dense vector index\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\nformulations, one which conditions on the same retrieved passages across the\nwhole generated sequence, the other can use different passages per token. We\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\ntasks and set the state-of-the-art on three open domain QA tasks, outperforming\nparametric seq2seq models and task-specific retrieve-and-extract architectures.\nFor language generation tasks, we find that RAG models generate more specific,\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\nbaseline.",
        "year": "2020",
        "url": "https://arxiv.org/abs/2005.11401"
    },
    {
        "arxiv_id": "2311.06605",
        "title": "Sparsity and integrality gap transference bounds for integer programs",
        "authors": [],
        "abstract": "We obtain new transference bounds that connect two active areas of research:\nproximity and sparsity of solutions to integer programs. Specifically, we study\nthe additive integrality gap of the integer linear programs min{cx: x in P, x\ninteger}, where P={x: Ax=b, x nonnegative} is a polyhedron in the standard form\ndetermined by an integer mxn matrix A and an integer vector b. The main result\nof the paper shows that the integrality gap drops exponentially in the size of\nsupport of the optimal solutions that correspond to the vertices of the integer\nhull of the polyhedron P. Additionally, we obtain a new proximity bound that\nestimates the distance from any point of P to its nearest integer point in P.\nThe proofs make use of the results from the geometry of numbers and convex\ngeometry.",
        "year": "2023",
        "url": "https://arxiv.org/abs/2311.06605"
    },
    {
        "arxiv_id": "2103.00020",
        "title": "Learning Transferable Visual Models From Natural Language Supervision",
        "authors": [],
        "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set\nof predetermined object categories. This restricted form of supervision limits\ntheir generality and usability since additional labeled data is needed to\nspecify any other visual concept. Learning directly from raw text about images\nis a promising alternative which leverages a much broader source of\nsupervision. We demonstrate that the simple pre-training task of predicting\nwhich caption goes with which image is an efficient and scalable way to learn\nSOTA image representations from scratch on a dataset of 400 million (image,\ntext) pairs collected from the internet. After pre-training, natural language\nis used to reference learned visual concepts (or describe new ones) enabling\nzero-shot transfer of the model to downstream tasks. We study the performance\nof this approach by benchmarking on over 30 different existing computer vision\ndatasets, spanning tasks such as OCR, action recognition in videos,\ngeo-localization, and many types of fine-grained object classification. The\nmodel transfers non-trivially to most tasks and is often competitive with a\nfully supervised baseline without the need for any dataset specific training.\nFor instance, we match the accuracy of the original ResNet-50 on ImageNet\nzero-shot without needing to use any of the 1.28 million training examples it\nwas trained on. We release our code and pre-trained model weights at\nhttps://github.com/OpenAI/CLIP.",
        "year": "2021",
        "url": "https://arxiv.org/abs/2103.00020"
    },
    {
        "arxiv_id": "2104.14294",
        "title": "Emerging Properties in Self-Supervised Vision Transformers",
        "authors": [],
        "abstract": "In this paper, we question if self-supervised learning provides new\nproperties to Vision Transformer (ViT) that stand out compared to convolutional\nnetworks (convnets). Beyond the fact that adapting self-supervised methods to\nthis architecture works particularly well, we make the following observations:\nfirst, self-supervised ViT features contain explicit information about the\nsemantic segmentation of an image, which does not emerge as clearly with\nsupervised ViTs, nor with convnets. Second, these features are also excellent\nk-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study\nalso underlines the importance of momentum encoder, multi-crop training, and\nthe use of small patches with ViTs. We implement our findings into a simple\nself-supervised method, called DINO, which we interpret as a form of\nself-distillation with no labels. We show the synergy between DINO and ViTs by\nachieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.",
        "year": "2021",
        "url": "https://arxiv.org/abs/2104.14294"
    },
    {
        "arxiv_id": "2106.04560",
        "title": "Scaling Vision Transformers",
        "authors": [],
        "abstract": "Attention-based neural networks such as the Vision Transformer (ViT) have\nrecently attained state-of-the-art results on many computer vision benchmarks.\nScale is a primary ingredient in attaining excellent results, therefore,\nunderstanding a model's scaling properties is a key to designing future\ngenerations effectively. While the laws for scaling Transformer language models\nhave been studied, it is unknown how Vision Transformers scale. To address\nthis, we scale ViT models and data, both up and down, and characterize the\nrelationships between error rate, data, and compute. Along the way, we refine\nthe architecture and training of ViT, reducing memory consumption and\nincreasing accuracy of the resulting models. As a result, we successfully train\na ViT model with two billion parameters, which attains a new state-of-the-art\non ImageNet of 90.45% top-1 accuracy. The model also performs well for few-shot\ntransfer, for example, reaching 84.86% top-1 accuracy on ImageNet with only 10\nexamples per class.",
        "year": "2021",
        "url": "https://arxiv.org/abs/2106.04560"
    },
    {
        "arxiv_id": "1706.08947",
        "title": "Exploring Generalization in Deep Learning",
        "authors": [],
        "abstract": "With a goal of understanding what drives generalization in deep networks, we\nconsider several recently suggested explanations, including norm-based control,\nsharpness and robustness. We study how these measures can ensure\ngeneralization, highlighting the importance of scale normalization, and making\na connection between sharpness and PAC-Bayes theory. We then investigate how\nwell the measures explain different observed phenomena.",
        "year": "2017",
        "url": "https://arxiv.org/abs/1706.08947"
    },
    {
        "arxiv_id": "2203.02155",
        "title": "Training language models to follow instructions with human feedback",
        "authors": [],
        "abstract": "Making language models bigger does not inherently make them better at\nfollowing a user's intent. For example, large language models can generate\noutputs that are untruthful, toxic, or simply not helpful to the user. In other\nwords, these models are not aligned with their users. In this paper, we show an\navenue for aligning language models with user intent on a wide range of tasks\nby fine-tuning with human feedback. Starting with a set of labeler-written\nprompts and prompts submitted through the OpenAI API, we collect a dataset of\nlabeler demonstrations of the desired model behavior, which we use to fine-tune\nGPT-3 using supervised learning. We then collect a dataset of rankings of model\noutputs, which we use to further fine-tune this supervised model using\nreinforcement learning from human feedback. We call the resulting models\nInstructGPT. In human evaluations on our prompt distribution, outputs from the\n1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,\ndespite having 100x fewer parameters. Moreover, InstructGPT models show\nimprovements in truthfulness and reductions in toxic output generation while\nhaving minimal performance regressions on public NLP datasets. Even though\nInstructGPT still makes simple mistakes, our results show that fine-tuning with\nhuman feedback is a promising direction for aligning language models with human\nintent.",
        "year": "2022",
        "url": "https://arxiv.org/abs/2203.02155"
    },
    {
        "arxiv_id": "2303.12712",
        "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4",
        "authors": [],
        "abstract": "Artificial intelligence (AI) researchers have been developing and refining\nlarge language models (LLMs) that exhibit remarkable capabilities across a\nvariety of domains and tasks, challenging our understanding of learning and\ncognition. The latest model developed by OpenAI, GPT-4, was trained using an\nunprecedented scale of compute and data. In this paper, we report on our\ninvestigation of an early version of GPT-4, when it was still in active\ndevelopment by OpenAI. We contend that (this early version of) GPT-4 is part of\na new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that\nexhibit more general intelligence than previous AI models. We discuss the\nrising capabilities and implications of these models. We demonstrate that,\nbeyond its mastery of language, GPT-4 can solve novel and difficult tasks that\nspan mathematics, coding, vision, medicine, law, psychology and more, without\nneeding any special prompting. Moreover, in all of these tasks, GPT-4's\nperformance is strikingly close to human-level performance, and often vastly\nsurpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's\ncapabilities, we believe that it could reasonably be viewed as an early (yet\nstill incomplete) version of an artificial general intelligence (AGI) system.\nIn our exploration of GPT-4, we put special emphasis on discovering its\nlimitations, and we discuss the challenges ahead for advancing towards deeper\nand more comprehensive versions of AGI, including the possible need for\npursuing a new paradigm that moves beyond next-word prediction. We conclude\nwith reflections on societal influences of the recent technological leap and\nfuture research directions.",
        "year": "2023",
        "url": "https://arxiv.org/abs/2303.12712"
    },
    {
        "arxiv_id": "2309.07864",
        "title": "The Rise and Potential of Large Language Model Based Agents: A Survey",
        "authors": [],
        "abstract": "For a long time, humanity has pursued artificial intelligence (AI) equivalent\nto or surpassing the human level, with AI agents considered a promising vehicle\nfor this pursuit. AI agents are artificial entities that sense their\nenvironment, make decisions, and take actions. Many efforts have been made to\ndevelop intelligent agents, but they mainly focus on advancement in algorithms\nor training strategies to enhance specific capabilities or performance on\nparticular tasks. Actually, what the community lacks is a general and powerful\nmodel to serve as a starting point for designing AI agents that can adapt to\ndiverse scenarios. Due to the versatile capabilities they demonstrate, large\nlanguage models (LLMs) are regarded as potential sparks for Artificial General\nIntelligence (AGI), offering hope for building general AI agents. Many\nresearchers have leveraged LLMs as the foundation to build AI agents and have\nachieved significant progress. In this paper, we perform a comprehensive survey\non LLM-based agents. We start by tracing the concept of agents from its\nphilosophical origins to its development in AI, and explain why LLMs are\nsuitable foundations for agents. Building upon this, we present a general\nframework for LLM-based agents, comprising three main components: brain,\nperception, and action, and the framework can be tailored for different\napplications. Subsequently, we explore the extensive applications of LLM-based\nagents in three aspects: single-agent scenarios, multi-agent scenarios, and\nhuman-agent cooperation. Following this, we delve into agent societies,\nexploring the behavior and personality of LLM-based agents, the social\nphenomena that emerge from an agent society, and the insights they offer for\nhuman society. Finally, we discuss several key topics and open problems within\nthe field. A repository for the related papers at\nhttps://github.com/WooooDyy/LLM-Agent-Paper-List.",
        "year": "2023",
        "url": "https://arxiv.org/abs/2309.07864"
    }
]